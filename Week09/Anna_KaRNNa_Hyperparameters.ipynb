{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 19, 65, 57, 71, 37, 62, 10, 29, 17, 17, 17, 44, 65, 57, 57,  2,\n",
       "       10, 13, 65, 26, 27, 14, 27, 37, 52, 10, 65, 62, 37, 10, 65, 14, 14,\n",
       "       10, 65, 14, 27, 40, 37, 79, 10, 37, 46, 37, 62,  2, 10, 18, 16, 19,\n",
       "       65, 57, 57,  2, 10, 13, 65, 26, 27, 14,  2, 10, 27, 52, 10, 18, 16,\n",
       "       19, 65, 57, 57,  2, 10, 27, 16, 10, 27, 71, 52, 10, 82, 12, 16, 17,\n",
       "       12, 65,  2, 32, 17, 17, 64, 46, 37, 62,  2, 71, 19, 27, 16], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 19, 65, 57, 71, 37, 62, 10, 29, 17],\n",
       "       [49, 16, 58, 10, 19, 37, 10, 26, 82, 46],\n",
       "       [10, 72, 65, 71, 72, 19, 27, 16, 66, 10],\n",
       "       [82, 71, 19, 37, 62, 10, 12, 82, 18, 14],\n",
       "       [10, 71, 19, 37, 10, 14, 65, 16, 58, 20],\n",
       "       [10, 61, 19, 62, 82, 18, 66, 19, 10, 14],\n",
       "       [71, 10, 71, 82, 17, 58, 82, 32, 17, 17],\n",
       "       [82, 10, 19, 37, 62, 52, 37, 14, 13,  4],\n",
       "       [19, 65, 71, 10, 27, 52, 10, 71, 19, 37],\n",
       "       [37, 62, 52, 37, 14, 13, 10, 65, 16, 58]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "#         cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(lstm_size),\n",
    "            output_keep_prob=keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50#100\n",
    "num_steps = 50#100\n",
    "lstm_size = 256#512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, file_writer):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Use the line below to load a checkpoint and resume training\n",
    "        #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "\n",
    "        n_batches = int(train_x.shape[1]/num_steps)\n",
    "        iterations = n_batches * epochs\n",
    "        for e in range(epochs):\n",
    "\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "                iteration = e*n_batches + b\n",
    "                start = time.time()\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: new_state}\n",
    "                summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                              model.final_state, model.optimizer], \n",
    "                                                              feed_dict=feed)\n",
    "                loss += batch_loss\n",
    "                end = time.time()\n",
    "                print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                      'Iteration {}/{}'.format(iteration, iterations),\n",
    "                      'Training loss: {:.4f}'.format(loss/b),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                file_writer.add_summary(summary, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4204 1.0047 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.4085 0.8002 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.3950 0.7837 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.3759 0.8324 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.3371 0.8020 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.2600 0.8325 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 4.1707 0.7997 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 4.0897 0.7947 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 4.0183 0.8000 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.9582 0.8001 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.9033 0.8217 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.8572 0.8033 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.8157 0.7815 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.7804 0.8313 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.7487 0.8236 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.7203 0.7822 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.6935 0.8067 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.6713 0.7804 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.6497 0.9027 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.6281 0.9529 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.6092 0.6734 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.5915 0.7186 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.5749 0.9846 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.5596 0.9259 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.5450 0.8258 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.5322 0.8737 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.5202 0.8725 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.5080 0.7955 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.4969 0.6446 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.4864 0.6507 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.4771 0.6610 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.4675 0.7028 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.4580 0.6347 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.4496 0.5682 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.4411 0.5401 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.4335 0.5762 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.4253 0.5952 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.4178 0.5357 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.4104 0.5641 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.4036 0.5556 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.3969 0.5524 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.3904 0.5593 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.3843 0.5690 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.3782 0.8062 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.3724 0.6281 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.3670 0.5867 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.3620 0.6038 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.3572 0.6971 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.3526 0.6020 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.3479 0.5778 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.3434 0.5879 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.3388 0.6002 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.3346 0.5635 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.3301 0.8370 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.3262 0.6022 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.3219 0.5936 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.3178 0.5627 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.3141 0.5991 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.3102 0.7180 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.3066 0.6420 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.3030 0.8082 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.2998 0.6121 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.2968 0.5575 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.2932 0.5486 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.2897 0.5677 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.2867 0.5573 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.2836 0.5514 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.2799 0.5413 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.2767 0.5401 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.2738 0.5415 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.2707 0.6944 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.2681 0.5989 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.2651 0.5618 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.2623 0.5390 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.2596 0.5446 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.2570 0.5458 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.2542 0.5442 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.2515 0.5376 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.2488 0.5542 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.2458 0.5915 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.2430 0.5493 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.2403 0.5519 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.2378 0.5354 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.2350 0.5324 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.2321 0.5419 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.2293 0.5449 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.2265 0.5371 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.2237 0.5453 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.2211 0.5388 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.2184 0.5385 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.2158 0.5496 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.2131 0.5428 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.2104 0.5501 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.2076 0.5480 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.2048 0.5594 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.2019 0.5631 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.1993 0.5603 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.1965 0.5517 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.1938 0.5399 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.1910 0.5511 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.1883 0.5389 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.1856 0.5400 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.1828 0.5599 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.1799 0.5405 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.1771 0.5375 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.1742 0.5384 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.1712 0.5593 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.1683 0.5448 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.1656 0.5353 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.1624 0.5513 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.1595 0.5432 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.1566 0.5362 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.1536 0.5523 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.1505 0.5388 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.1474 0.5647 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.1443 0.5416 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.1413 0.5351 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.1384 0.5428 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.1356 0.5333 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.1327 0.5629 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.1302 0.5479 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.1273 0.5559 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.1244 0.5418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 124/890 Training loss: 3.1216 0.5313 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.1187 0.5535 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.1156 0.5552 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.1128 0.5510 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.1100 0.5345 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.1071 0.5431 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.1042 0.5486 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.1014 0.5587 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.0985 0.5459 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.0957 0.5496 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.0928 0.5445 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 3.0897 0.5588 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 3.0868 0.5277 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 3.0839 0.5404 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 3.0809 0.5458 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 3.0782 0.5604 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 3.0753 0.5323 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 3.0726 0.5576 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 3.0697 0.5471 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 3.0669 0.5424 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 3.0640 0.5297 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 3.0612 0.5580 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 3.0585 0.5496 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 3.0558 0.5495 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 3.0532 0.5435 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 3.0503 0.5414 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 3.0475 0.5447 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 3.0450 0.5322 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 3.0426 0.5397 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 3.0400 0.5464 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 3.0373 0.5439 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 3.0346 0.5438 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 3.0318 0.5516 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 3.0290 0.5440 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 3.0263 0.5434 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 3.0236 0.5421 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 3.0210 0.5626 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 3.0184 0.5534 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 3.0157 0.5432 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 3.0130 0.5412 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 3.0104 0.5556 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 3.0078 0.5448 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 3.0052 0.5593 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 3.0027 0.5302 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 3.0002 0.5551 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 2.9977 0.5600 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 2.9951 0.6130 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 2.9926 0.5463 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 2.9903 0.5698 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 2.9881 0.5492 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 2.9859 0.5462 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 2.9837 0.5695 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 2.9813 0.5481 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 2.9789 0.5640 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 2.9763 0.5567 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.5935 0.5619 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.5545 0.5868 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.5451 0.5861 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.5416 0.5729 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.5403 0.6020 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.5374 0.6123 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.5361 0.6002 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.5373 0.6494 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.5372 0.5743 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.5358 0.5635 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.5337 0.5582 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.5334 0.5590 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.5324 0.5648 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.5335 0.8362 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.5325 0.7449 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.5319 0.5905 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.5314 0.5656 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.5321 0.5734 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.5313 0.5748 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.5291 0.5955 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.5277 0.5632 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.5279 0.5799 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.5265 0.5856 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.5250 0.5811 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.5235 0.5831 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.5225 0.5931 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.5214 0.6057 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.5201 0.5948 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.5193 0.5915 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.5183 0.5733 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.5181 0.5735 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.5168 0.5864 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.5151 0.5950 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.5143 0.5799 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.5131 0.5937 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.5123 0.5939 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.5110 0.5924 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.5093 0.5969 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.5078 0.5874 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.5065 0.5881 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.5051 0.5909 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.5039 0.5969 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.5023 0.6109 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.5009 0.5814 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.4995 0.5912 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.4979 0.5937 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.4972 0.5930 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.4962 0.5844 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.4951 0.5994 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.4945 0.5905 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.4936 0.5942 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.4928 0.5942 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.4918 0.6084 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.4907 0.6068 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.4895 0.5984 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.4885 0.6142 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.4877 0.6237 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.4865 0.5991 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.4856 0.6056 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.4851 0.6046 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.4841 0.6022 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.4833 0.6142 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.4827 0.6053 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.4818 0.6147 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.4807 0.6007 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.4801 0.6134 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 245/890 Training loss: 2.4793 0.6168 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.4781 0.6006 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.4770 0.5987 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.4762 0.6310 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.4755 0.6137 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.4749 0.6175 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.4741 0.6075 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.4733 0.6201 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.4725 0.6165 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.4722 0.6188 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.4714 0.6247 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.4709 0.6216 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.4699 0.6247 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.4691 0.6323 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.4683 0.6112 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.4676 0.6358 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.4667 0.6135 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.4658 0.7285 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.4647 0.6133 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.4638 0.6110 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.4631 0.6175 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.4623 0.6386 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.4614 0.6241 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.4608 0.6364 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.4600 0.6285 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.4593 0.6152 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.4586 0.6295 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.4576 0.6396 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.4567 0.6291 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.4558 0.6238 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.4551 0.6230 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.4542 0.6244 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.4535 0.6208 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.4528 0.6346 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.4522 0.6234 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.4515 0.6215 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.4506 0.6272 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.4498 0.6295 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.4491 0.6348 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.4484 0.6343 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.4476 0.6398 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.4472 0.6431 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.4466 0.6286 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.4458 0.6311 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.4452 0.6299 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.4448 0.6249 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.4440 0.6380 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.4434 0.6273 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.4427 0.6407 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.4417 0.6354 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.4411 0.6417 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.4405 0.6362 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.4401 0.6323 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.4395 0.6333 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.4390 0.6385 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.4384 0.6368 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.4378 0.6353 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.4373 0.6394 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.4366 0.6360 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.4359 0.6272 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.4353 0.6344 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.4348 0.6353 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.4342 0.6379 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.4336 0.6211 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.4331 0.6281 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.4324 0.6493 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.4318 0.6409 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.4314 0.6410 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.4307 0.6404 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.4302 0.6247 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.4296 0.6534 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.4291 0.6406 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.4288 0.6419 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.4282 0.6434 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.4279 0.6366 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.4272 0.6387 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.4267 0.6429 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.4261 0.6487 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.4256 0.6501 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.4252 0.6380 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.4248 0.6304 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.4244 0.6383 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.4239 0.6568 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.4233 0.6335 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.4230 0.6600 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.4227 0.6432 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.4223 0.6446 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.4219 0.6364 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.4213 0.6372 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.4207 0.6551 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.4202 0.6407 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.4197 0.6411 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.4190 0.6388 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.4187 0.6559 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.4183 0.6451 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.4177 0.6460 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.4171 0.6447 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.4165 0.6429 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.4161 0.6439 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.4156 0.6393 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.4151 0.6437 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.4147 0.6627 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.4143 0.6708 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.4137 0.6546 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.4133 0.6704 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.4131 0.6711 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.4128 0.7891 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.4126 0.7037 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.4124 0.6902 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.4120 0.7506 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.4115 0.7280 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.4109 0.7117 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.3751 0.6914 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.3306 0.7301 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.3201 0.8199 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.3176 0.8246 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.3165 0.7849 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.3150 0.7713 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.3163 0.8603 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.3165 0.7957 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.3189 0.6922 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 366/890 Training loss: 2.3177 0.7386 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.3162 0.8582 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.3159 0.7455 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.3163 0.7260 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.3186 0.9335 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.3180 0.9681 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.3178 0.7371 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.3174 0.6946 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.3194 0.7255 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.3194 0.7861 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.3183 0.6923 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.3178 0.6775 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.3191 0.7043 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.3185 0.7012 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.3175 0.6724 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.3166 0.7330 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.3159 0.7096 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.3152 0.6814 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.3151 0.7041 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.3157 0.6919 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.3154 0.7019 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.3156 0.6926 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.3148 0.6806 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.3144 0.6915 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.3147 0.7200 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.3143 0.6892 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.3140 0.6682 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.3134 0.6862 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.3120 0.7215 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.3111 0.7100 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.3101 0.6837 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.3093 0.6928 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.3086 0.6867 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.3078 0.6820 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.3068 0.6856 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 2.3060 0.7067 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.3048 0.6996 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.3048 0.6820 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 2.3042 0.6930 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.3036 0.7077 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.3039 0.7092 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.3031 0.7297 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.3033 0.7378 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.3028 0.7008 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.3022 0.7025 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.3017 0.6753 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.3015 0.7127 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.3011 0.6855 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.3006 0.6920 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.2999 0.7611 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.3001 0.7046 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.2997 0.7300 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.2995 0.6958 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.2994 0.6746 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.2989 0.6977 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.2983 0.7312 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.2981 0.7209 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.2978 0.6910 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.2971 0.6856 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.2965 0.6772 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.2962 0.6980 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.2961 0.6838 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.2958 0.6908 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.2957 0.6839 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.2953 0.7023 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.2949 0.6980 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.2951 0.6714 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.2948 0.6898 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.2947 0.6875 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.2941 0.6761 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.2937 0.7088 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.2931 0.6795 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.2929 0.6904 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 2.2924 0.6849 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.2919 0.6780 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.2910 0.6763 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 2.2905 0.6886 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.2901 0.6795 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.2896 0.6898 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 2.2891 0.6780 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.2890 0.6835 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.2886 0.6876 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.2882 0.6858 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.2877 0.6762 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.2872 0.6923 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.2865 0.6642 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.2861 0.6728 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.2858 0.6828 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.2853 0.6905 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.2848 0.6828 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.2843 0.6795 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.2841 0.6901 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.2837 0.6804 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.2831 0.6820 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.2827 0.6776 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.2823 0.6862 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.2819 0.6883 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.2816 0.6931 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.2815 0.6839 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.2813 0.6778 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.2808 0.6886 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 2.2805 0.6967 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 2.2803 0.6761 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 2.2799 0.6814 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 2.2795 0.6895 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 2.2791 0.6712 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 2.2784 0.6930 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 2.2781 0.6860 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 2.2778 0.6965 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 2.2777 0.6862 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 2.2772 0.6773 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 2.2770 0.6831 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 2.2766 0.6880 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 2.2763 0.6774 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 2.2761 0.6786 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 2.2757 0.6758 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 2.2753 0.6796 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 2.2751 0.6767 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 2.2749 0.6762 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 2.2746 0.6843 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 2.2744 0.6947 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 487/890 Training loss: 2.2742 0.7067 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 2.2736 0.7040 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 2.2734 0.6859 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 2.2732 0.7013 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 2.2729 0.6851 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 2.2727 0.6835 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 2.2724 0.6915 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 2.2723 0.6928 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 2.2723 0.6677 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 2.2719 0.7124 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 2.2718 0.6644 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 2.2716 0.6869 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 2.2714 0.6917 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 2.2711 0.6743 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 2.2707 0.6883 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 2.2706 0.6810 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 2.2704 0.6961 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 2.2703 0.6783 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 2.2700 0.6890 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 2.2696 0.6969 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 2.2695 0.6892 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 2.2695 0.6894 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 2.2693 0.6932 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 2.2691 0.7751 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 2.2688 0.8502 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 2.2685 0.7095 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 2.2682 0.7016 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 2.2679 0.7094 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 2.2675 0.7130 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 2.2675 0.6810 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 2.2673 0.6889 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 2.2669 0.6853 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 2.2667 0.6987 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 2.2664 0.7688 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 2.2661 0.6933 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 2.2659 0.7002 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 2.2657 0.6913 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 2.2656 0.6948 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 2.2653 0.6972 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 2.2650 0.6951 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 2.2647 0.7034 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 2.2645 0.7031 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 2.2645 0.7843 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 2.2644 0.6919 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 2.2644 0.7114 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 2.2645 0.6998 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 2.2646 0.6922 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 2.2646 0.6978 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 2.2715 0.7091 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 2.2267 0.7039 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 2.2148 0.7032 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 2.2129 0.7005 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 2.2118 0.6734 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 2.2087 0.6879 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 2.2093 0.6823 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 2.2102 0.6964 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 2.2125 0.6949 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 2.2120 0.7127 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 2.2102 0.6838 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 2.2090 0.6957 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 2.2092 0.6920 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 2.2114 0.6959 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 2.2105 0.6883 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 2.2097 0.7040 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 2.2092 0.7031 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 2.2111 0.6999 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 2.2109 0.6831 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 2.2099 0.6956 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 2.2094 0.6859 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 2.2108 0.7108 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 2.2103 0.7005 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 2.2095 0.6891 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 2.2090 0.7112 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 2.2081 0.7050 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 2.2074 0.6875 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 2.2074 0.7006 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 2.2080 0.6982 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 2.2082 0.7004 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 2.2083 0.6975 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 2.2075 0.6969 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 2.2072 0.7301 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 2.2075 0.6929 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 2.2072 0.6803 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 2.2070 0.6870 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 2.2067 0.6862 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 2.2055 0.6859 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 2.2045 0.6939 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 2.2037 0.6882 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 2.2031 0.7107 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 2.2028 0.7046 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 2.2021 0.6774 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 2.2013 0.6839 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 2.2010 0.7113 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 2.1996 0.6957 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 2.1995 0.6888 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 2.1988 0.6856 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 2.1984 0.6970 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 2.1987 0.6975 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 2.1982 0.6884 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 2.1985 0.7030 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 2.1980 0.6975 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 2.1976 0.7089 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 2.1971 0.6882 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 2.1971 0.6992 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 2.1970 0.6930 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 2.1965 0.7118 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 2.1960 0.6959 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 2.1960 0.7086 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 2.1957 0.7071 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 2.1958 0.7045 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 2.1958 0.6928 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 2.1955 0.6898 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 2.1951 0.6882 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 2.1951 0.7006 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 2.1949 0.7156 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 2.1942 0.6976 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 2.1937 0.7010 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 2.1935 0.6925 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 2.1933 0.6962 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 2.1932 0.7949 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 2.1932 0.7144 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 608/890 Training loss: 2.1929 0.7016 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 2.1927 0.7072 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 2.1928 0.6985 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 2.1925 0.6875 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 2.1925 0.6945 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 2.1919 0.6873 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 2.1916 0.6883 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 2.1910 0.6868 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 2.1910 0.7089 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 2.1905 0.6815 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 2.1900 0.6874 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 2.1893 0.6962 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 2.1888 0.6990 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 2.1886 0.7015 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 2.1880 0.7182 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 2.1875 0.6775 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 2.1874 0.6926 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 2.1870 0.7012 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 2.1867 0.7233 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 2.1861 0.6963 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 2.1855 0.6935 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 2.1851 0.6861 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 2.1847 0.7033 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 2.1845 0.6774 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 2.1840 0.6881 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 2.1836 0.6956 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 2.1831 0.7021 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 2.1830 0.6864 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 2.1827 0.6899 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 2.1822 0.7022 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 2.1820 0.7043 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 2.1816 0.6928 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 2.1812 0.6957 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 2.1810 0.6991 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 2.1808 0.6946 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 2.1807 0.6844 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 2.1804 0.6950 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 2.1802 0.6872 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 2.1799 0.6952 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 2.1796 0.6862 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 2.1792 0.6975 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 2.1788 0.6936 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 2.1783 0.7092 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 2.1780 0.6845 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 2.1778 0.7042 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 2.1776 0.6910 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 2.1774 0.6889 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 2.1773 0.6835 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 2.1769 0.6921 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 2.1766 0.6958 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 2.1764 0.7058 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 2.1762 0.6992 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 2.1757 0.7045 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 2.1756 0.6914 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 2.1754 0.6943 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 2.1753 0.6815 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 2.1751 0.6949 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 2.1749 0.7217 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 2.1744 0.7195 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 2.1742 0.7122 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 2.1742 0.6971 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 2.1739 0.7050 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 2.1738 0.6986 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 2.1737 0.7228 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 2.1735 0.7059 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 2.1735 0.6897 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 2.1731 0.6995 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 2.1731 0.6947 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 2.1728 0.7039 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 2.1726 0.6962 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 2.1724 0.6809 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 2.1721 0.7006 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 2.1721 0.6926 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 2.1719 0.6962 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 2.1718 0.6949 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 2.1715 0.6887 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 2.1712 0.6901 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 2.1710 0.7001 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 2.1711 0.7032 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 2.1710 0.6898 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 2.1709 0.6788 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 2.1706 0.7197 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 2.1703 0.6967 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 2.1701 0.7170 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 2.1698 0.6876 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 2.1694 0.6890 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 2.1694 0.6885 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 2.1693 0.7077 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 2.1691 0.6991 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 2.1689 0.7036 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 2.1686 0.6969 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 2.1685 0.7789 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 2.1683 0.7033 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 2.1680 0.7017 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 2.1680 0.7279 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 2.1678 0.7021 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 2.1676 0.7055 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 2.1673 0.7196 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 2.1670 0.7168 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 2.1669 0.6912 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 2.1668 0.6992 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 2.1667 0.7017 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 2.1665 0.7021 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 2.1662 0.7057 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 2.1659 0.7049 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 2.1860 0.6968 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 2.1440 0.6866 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 2.1279 0.6870 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 2.1224 0.7119 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 2.1191 0.6982 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 2.1160 0.7012 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 2.1175 0.6963 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 2.1179 0.6982 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 2.1202 0.7000 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 2.1200 0.7108 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 2.1184 0.7017 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 2.1173 0.7087 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 2.1187 0.6877 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 2.1211 0.6896 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 2.1210 0.7007 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 2.1204 0.6965 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 729/890 Training loss: 2.1196 0.7053 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 2.1217 0.7109 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 2.1217 0.6907 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 2.1212 0.7080 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 2.1204 0.7004 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 2.1217 0.6907 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 2.1214 0.7077 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 2.1208 0.7099 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 2.1204 0.6978 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 2.1199 0.7074 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 2.1189 0.7035 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 2.1191 0.6913 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 2.1199 0.7035 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 2.1200 0.7006 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 2.1200 0.7051 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 2.1194 0.6985 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 2.1190 0.7008 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 2.1196 0.7064 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 2.1195 0.6973 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 2.1191 0.6972 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 2.1188 0.7153 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 2.1173 0.7120 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 2.1164 0.7062 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 2.1160 0.7244 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 2.1155 0.6930 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 2.1152 0.6873 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 2.1144 0.6918 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 2.1136 0.7120 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 2.1133 0.7026 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 2.1121 0.7224 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 2.1120 0.8345 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 2.1113 0.7789 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 2.1109 0.9517 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 2.1113 1.0713 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 2.1108 1.0525 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 2.1113 1.0416 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 2.1110 1.0215 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 2.1106 1.1849 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 2.1099 1.0687 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 2.1099 1.0768 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 2.1097 1.1268 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 2.1093 1.0768 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 2.1088 0.9220 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 2.1090 0.7750 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 2.1087 0.7267 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 2.1091 0.6940 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 2.1092 0.6903 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 2.1091 0.6947 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 2.1087 0.7023 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 2.1089 0.7117 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 2.1088 0.7009 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 2.1082 0.6917 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 2.1080 0.7184 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 2.1077 0.7055 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 2.1079 0.6858 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 2.1079 0.7889 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 2.1081 0.6913 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 2.1077 0.6940 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 2.1075 0.7235 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 2.1078 0.7055 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 2.1075 0.7011 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 2.1076 0.6846 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 2.1072 0.8298 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 2.1068 0.7680 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 2.1062 0.8358 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 2.1062 0.8676 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 2.1057 0.8937 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 2.1053 0.7548 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 2.1046 0.7062 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 2.1042 0.7150 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 2.1039 0.7354 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 2.1035 0.7940 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 2.1031 1.0128 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 2.1030 1.2568 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 2.1026 1.4859 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 2.1022 1.3257 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 2.1017 1.4594 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 2.1013 1.2456 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 2.1008 1.1415 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 2.1006 1.0658 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 2.1004 1.0911 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 2.1000 1.1023 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 2.0995 1.2012 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 2.0991 1.1904 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 2.0990 1.1498 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 2.0988 1.1264 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 2.0983 1.1176 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 2.0980 1.1051 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 2.0977 1.1234 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 2.0975 1.0665 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 2.0972 1.0281 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 2.0970 1.0334 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 2.0969 1.0622 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 2.0967 1.1135 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 2.0964 1.0662 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 2.0961 1.0664 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 2.0959 1.0402 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 2.0956 1.0473 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 2.0952 1.0412 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 2.0947 1.0719 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 2.0945 1.0507 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 2.0943 1.0314 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 2.0941 1.0347 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 2.0939 1.0788 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 2.0938 1.0883 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 2.0935 1.0538 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 2.0932 1.0484 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 2.0932 1.0344 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 2.0931 1.0647 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 2.0926 1.0959 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 2.0926 1.1087 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 2.0924 1.0751 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 2.0923 1.0571 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 2.0922 1.0578 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 2.0919 1.0239 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 2.0915 1.0108 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 2.0914 1.0721 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 2.0913 1.0695 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 2.0911 1.0717 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 2.0909 1.0758 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 2.0908 1.0878 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 850/890 Training loss: 2.0907 1.3415 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 2.0908 1.0857 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 2.0904 1.0772 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 2.0905 1.1534 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 2.0902 1.2031 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 2.0901 1.1379 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 2.0899 1.1590 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 2.0896 1.0541 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 2.0896 1.0463 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 2.0894 1.0146 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 2.0895 1.0601 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 2.0892 1.0397 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 2.0890 1.0104 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 2.0888 1.0383 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 2.0889 1.0152 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 2.0887 1.0319 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 2.0887 1.0359 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 2.0884 1.1035 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 2.0882 1.0439 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 2.0880 1.0418 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 2.0878 1.0509 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 2.0875 0.9880 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 2.0875 1.0044 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 2.0875 1.0369 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 2.0872 1.0478 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 2.0870 1.0488 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 2.0869 1.0316 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 2.0867 1.0426 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 2.0866 1.0223 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 2.0864 1.0280 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 2.0866 1.0384 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 2.0864 1.0697 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 2.0863 1.0438 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 2.0860 1.0468 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 2.0858 1.0045 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 2.0858 1.0355 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 2.0858 1.0554 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 2.0858 1.0247 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 2.0856 1.0123 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 2.0854 1.0115 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 2.0853 1.0611 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4199 1.1080 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.4139 1.0424 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.4078 1.0226 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.4009 1.0114 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.3927 1.0138 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.3820 1.0291 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 4.3657 1.0365 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 4.3391 1.0368 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 4.2995 1.0541 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 4.2543 1.0359 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 4.2055 1.0689 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 4.1589 1.0546 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 4.1143 1.0480 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 4.0747 1.0588 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 4.0371 1.0648 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 4.0026 1.0569 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.9689 1.0547 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.9396 1.0410 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.9113 1.0576 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.8839 1.0588 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.8594 1.0647 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.8360 1.0205 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.8142 1.0325 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.7940 1.0300 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.7745 1.0613 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.7569 1.0436 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.7401 1.0536 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.7234 1.0499 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.7081 1.0156 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.6937 0.9888 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.6807 1.0622 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.6672 1.0328 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.6542 1.0550 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.6425 1.0496 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.6307 1.0883 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.6199 1.0402 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.6090 1.0339 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.5987 1.0575 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.5885 1.0112 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.5790 1.0943 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.5700 1.0717 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.5613 0.9941 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.5528 1.0468 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.5447 1.0312 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.5367 1.0443 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.5294 1.3599 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.5226 1.5256 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.5156 1.5172 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.5092 1.1071 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.5030 1.0449 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.4969 1.0602 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.4907 1.0457 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.4851 1.0990 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.4791 1.0403 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.4739 1.0205 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.4683 1.0229 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.4630 1.0413 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.4580 1.0196 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.4529 1.1592 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.4482 1.0173 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.4437 1.0404 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.4395 1.0506 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.4354 1.0657 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.4308 1.0136 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.4264 1.0223 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.4225 1.0562 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.4186 1.0281 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.4142 1.0381 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.4102 1.0214 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.4066 1.0770 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.4029 1.0330 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.3996 1.0603 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.3961 1.0472 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.3927 0.9961 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.3895 0.9869 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.3865 1.0257 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.3835 1.0186 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.3804 1.0359 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.3773 1.0305 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.3742 1.0177 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.3711 1.0562 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 82/890 Training loss: 3.3684 1.0291 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.3657 1.0027 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.3629 1.0075 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.3600 1.0071 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.3573 1.0266 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.3546 0.9831 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.3520 1.0160 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.3496 1.0265 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.3472 1.0355 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.3448 1.0157 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.3423 1.0420 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.3399 1.0662 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.3377 1.0521 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.3353 1.0452 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.3331 1.0849 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.3309 1.0552 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.3286 1.0306 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.3265 1.0444 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.3244 1.0553 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.3224 1.0514 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.3203 1.0039 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.3183 1.0407 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.3163 1.0277 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.3143 1.0517 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.3124 1.0568 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.3102 1.0641 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.3082 1.0219 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.3063 1.0210 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.3041 1.0529 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.3022 1.0736 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.3004 1.0603 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.2984 1.0621 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.2965 1.0615 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.2945 1.0178 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.2926 1.0645 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.2908 1.0462 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.2891 1.0315 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.2874 1.0555 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.2856 1.0602 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.2840 1.0450 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.2824 1.0332 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.2807 1.0246 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.2792 1.0530 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.2775 1.0573 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.2757 1.0382 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.2740 1.0531 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.2724 1.0444 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.2707 1.0602 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.2691 1.0411 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.2676 1.0662 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.2659 1.0488 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.2644 1.0331 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.2628 1.0672 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 3.2609 1.0185 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 3.2591 0.9946 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 3.2575 1.0464 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 3.2558 1.0660 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 3.2544 1.0789 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 3.2527 1.0300 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 3.2512 1.0262 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 3.2494 1.0161 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 3.2478 1.0174 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 3.2462 1.0077 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 3.2446 1.0172 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 3.2430 1.0437 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 3.2415 1.0137 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 3.2402 1.0248 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 3.2385 1.0623 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 3.2370 1.0474 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 3.2355 1.0110 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 3.2342 1.0180 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 3.2327 1.0618 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 3.2312 1.0490 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 3.2295 1.0566 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 3.2280 1.0444 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 3.2264 1.0642 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 3.2248 1.0535 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 3.2231 1.0570 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 3.2215 1.0861 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 3.2200 1.0508 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 3.2183 1.0216 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 3.2165 1.0224 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 3.2150 1.0360 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 3.2133 1.0450 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 3.2117 1.0697 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 3.2102 1.0336 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 3.2086 1.0159 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 3.2070 1.0152 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 3.2053 1.0592 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 3.2037 1.0444 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 3.2024 1.0773 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 3.2011 1.0546 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 3.1998 1.1046 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 3.1984 1.0758 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 3.1969 1.0491 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 3.1952 1.0754 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 3.1935 1.0487 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.9698 1.0697 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.9282 1.0316 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.9127 1.0256 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.9048 1.0141 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.9020 1.0371 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.8996 1.0793 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.8976 1.0139 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.8973 1.0491 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.8951 1.0184 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.8925 0.9915 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.8887 1.0394 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.8864 1.0375 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.8841 1.0793 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.8835 1.0373 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.8823 1.0245 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.8811 1.0164 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.8792 1.0428 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.8790 1.0509 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.8775 1.0220 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.8745 1.0490 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.8725 1.0285 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.8714 1.0383 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.8691 1.0491 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.8672 1.0415 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 203/890 Training loss: 2.8651 1.0258 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.8636 1.0292 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.8621 1.0387 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.8600 1.0357 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.8582 1.0712 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.8566 1.0653 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.8555 1.0098 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.8535 1.0278 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.8514 1.0305 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.8496 1.0528 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.8474 1.0634 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.8459 1.0659 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.8438 1.0519 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.8415 1.0597 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.8393 1.0320 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.8374 1.0609 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.8353 1.0711 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.8332 1.0546 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.8310 1.0520 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.8288 1.0346 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.8267 1.0376 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.8246 1.0716 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.8230 1.0487 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.8213 0.9928 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.8194 1.0509 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.8179 1.0433 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.8161 1.0005 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.8143 1.0114 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.8125 0.9885 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.8106 1.0379 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.8088 1.0232 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.8069 1.0335 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.8052 1.0465 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.8033 1.0422 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.8014 1.0780 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.7999 1.0344 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.7981 1.0666 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.7966 1.0989 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.7952 1.0272 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.7934 1.0353 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.7916 1.0362 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.7902 1.0399 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.7887 1.0160 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.7867 1.0237 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.7849 1.0253 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.7835 1.0752 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.7820 1.0843 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.7806 1.0671 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.7791 1.0529 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.7774 1.0755 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.7759 1.0936 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.7749 0.9976 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.7733 1.0361 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.7720 1.1380 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.7704 1.0861 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.7689 1.0352 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.7674 1.0162 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.7661 1.0114 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.7646 1.0205 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.7630 1.0264 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.7613 1.0729 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.7597 1.0513 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.7583 1.0488 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.7568 1.0378 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.7554 1.0537 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.7541 1.0464 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.7527 1.0658 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.7513 1.0689 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.7499 1.0966 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.7484 1.0499 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.7467 1.0322 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.7453 1.0204 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.7440 1.0388 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.7425 1.0621 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.7413 1.0424 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.7399 1.0384 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.7387 1.0428 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.7374 1.0374 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.7360 1.0581 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.7347 1.0363 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.7334 1.0668 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.7322 1.0535 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.7307 1.0573 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.7296 1.0373 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.7285 1.0928 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.7270 1.1307 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.7258 1.0558 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.7247 1.0945 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.7235 1.0751 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.7221 1.0433 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.7209 1.0563 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.7194 1.0451 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.7182 1.0287 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.7170 1.0528 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.7161 1.0452 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.7149 1.0748 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.7139 1.0176 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.7128 1.0611 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.7116 1.0817 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.7105 1.0960 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.7094 1.0695 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.7081 1.0927 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.7070 1.0718 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.7060 1.0516 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.7048 1.0147 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.7037 1.0047 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.7027 1.0436 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.7015 1.0426 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.7005 1.0286 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.6995 1.0351 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.6983 1.0107 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.6972 1.0532 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.6961 1.0549 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.6950 1.0428 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.6942 1.0518 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.6931 1.0337 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.6922 1.0299 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.6911 1.0543 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.6901 1.0348 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.6890 1.0056 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.6880 1.0363 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 324/890 Training loss: 2.6871 1.0352 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.6861 1.0612 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.6854 1.0541 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.6843 1.0333 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.6833 1.0391 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.6825 1.0525 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.6819 1.0427 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.6811 1.0526 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.6802 1.0386 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.6792 1.0983 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.6782 1.0642 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.6772 1.0525 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.6762 1.0674 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.6751 1.0804 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.6743 1.0879 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.6735 1.0554 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.6724 1.0800 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.6713 1.0697 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.6703 1.0646 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.6695 1.0307 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.6686 0.9944 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.6677 1.0608 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.6668 1.0295 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.6660 1.0676 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.6650 1.0639 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.6641 1.0216 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.6634 1.0688 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.6628 1.0719 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.6622 1.0510 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.6615 1.0795 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.6607 1.0582 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.6598 1.0179 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.6588 1.0444 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.5750 1.0686 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.5208 1.0776 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.5064 1.0612 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.5021 1.0241 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.5009 1.0066 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.4978 1.0442 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.4978 1.0556 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.4993 1.0261 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.5005 1.0269 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.4998 1.0219 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.4979 1.0733 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.4977 1.0893 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.4969 1.0755 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.4983 1.0388 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.4982 1.0863 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.4978 1.0896 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.4978 1.0432 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.4992 1.0478 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.4990 1.0526 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.4970 1.0689 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.4963 1.0157 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.4971 1.0336 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.4965 1.0407 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.4956 1.0768 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.4947 1.0464 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.4941 1.0529 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.4935 1.0586 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.4926 1.0255 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.4928 1.0701 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.4924 1.0149 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.4924 1.0412 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.4914 1.0853 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.4905 1.0512 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.4902 1.0581 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.4895 1.0365 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.4891 1.0229 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.4884 1.0521 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.4871 1.0455 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.4860 1.0270 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.4853 1.0317 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.4845 1.0116 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.4836 1.0401 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.4827 1.0250 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.4818 1.0641 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 2.4809 1.1172 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.4797 1.0229 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.4795 1.0764 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 2.4789 1.0239 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.4782 1.0756 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.4780 1.0580 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.4772 1.0535 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.4769 1.0316 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.4762 1.0745 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.4755 1.0426 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.4750 1.0529 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.4744 1.0465 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.4739 1.0368 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.4730 1.0011 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.4723 1.0571 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.4720 1.0240 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.4714 1.0509 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.4712 1.0341 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.4710 1.0531 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.4704 1.0775 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.4697 1.0364 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.4695 1.0319 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.4692 1.0322 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.4683 1.0621 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.4675 1.0356 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.4673 1.0453 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.4669 1.0915 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.4667 1.0624 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.4664 1.0059 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.4658 1.0551 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.4654 1.0844 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.4654 1.0628 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.4649 1.0598 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.4648 1.0462 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.4642 1.0587 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.4637 1.0443 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.4633 1.0914 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.4630 1.0883 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 2.4626 1.0231 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.4620 1.0260 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.4612 1.0557 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 2.4606 1.0795 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.4602 1.0647 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.4598 1.0542 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 445/890 Training loss: 2.4592 1.0252 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.4589 1.0268 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.4583 1.0463 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.4580 1.0428 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.4575 1.0361 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.4569 1.0544 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.4562 1.0528 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.4557 1.0686 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.4552 1.0905 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.4547 1.0175 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.4542 1.0580 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.4537 1.0618 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.4535 1.0709 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.4531 1.0567 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.4525 1.0798 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.4520 1.0625 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.4514 1.0665 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.4510 1.0262 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.4505 1.0756 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.4503 1.0669 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.4501 0.9963 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.4495 1.0518 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 2.4492 1.0676 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 2.4489 1.0797 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 2.4485 1.0879 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 2.4479 1.0947 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 2.4475 1.0688 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 2.4469 1.0701 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 2.4464 1.0956 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 2.4462 1.0244 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 2.4459 1.0512 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 2.4455 1.0377 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 2.4453 1.0487 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 2.4450 1.0304 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 2.4446 1.0644 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 2.4443 1.0161 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 2.4440 1.0473 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 2.4435 1.0762 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 2.4432 1.0331 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 2.4430 1.0815 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 2.4427 1.0700 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 2.4424 1.0785 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 2.4421 1.1138 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 2.4416 1.0452 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 2.4413 1.0629 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 2.4410 1.0332 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 2.4406 1.0188 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 2.4403 1.0502 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 2.4399 1.0468 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 2.4397 1.0220 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 2.4395 1.0719 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 2.4392 1.0597 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 2.4390 1.0442 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 2.4385 1.0555 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 2.4382 1.0544 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 2.4378 1.0550 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 2.4374 1.0908 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 2.4373 1.0563 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 2.4370 1.0836 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 2.4369 1.0892 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 2.4365 1.0679 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 2.4361 1.0461 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 2.4360 1.0503 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 2.4360 1.0442 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 2.4357 1.0619 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 2.4355 1.0659 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 2.4351 1.0524 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 2.4348 1.0709 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 2.4344 1.0889 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 2.4340 1.1444 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 2.4336 1.0139 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 2.4334 1.1000 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 2.4332 1.0816 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 2.4327 1.0300 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 2.4323 1.0473 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 2.4319 1.0710 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 2.4317 1.0679 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 2.4315 1.0338 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 2.4311 1.0338 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 2.4308 1.0566 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 2.4306 1.1027 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 2.4303 1.0455 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 2.4299 1.0791 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 2.4297 1.0710 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 2.4296 1.0882 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 2.4294 1.0454 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 2.4293 1.0376 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 2.4293 1.0515 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 2.4293 1.0544 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 2.4291 1.0203 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 2.4446 1.0191 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 2.3926 1.0296 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 2.3783 1.0520 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 2.3749 0.9948 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 2.3716 1.0508 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 2.3698 1.0468 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 2.3699 1.0298 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 2.3713 0.9906 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 2.3726 1.0781 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 2.3727 1.0897 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 2.3706 1.0567 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 2.3701 1.0966 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 2.3697 1.0687 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 2.3716 1.0640 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 2.3715 1.0433 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 2.3711 1.0347 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 2.3710 1.0535 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 2.3726 1.0623 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 2.3723 1.0572 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 2.3708 1.0263 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 2.3703 1.0685 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 2.3720 1.0480 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 2.3717 1.0222 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 2.3711 1.0129 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 2.3700 1.0708 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 2.3693 1.0519 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 2.3688 1.0516 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 2.3686 1.0823 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 2.3690 1.0199 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 2.3691 1.0839 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 2.3690 1.0378 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 566/890 Training loss: 2.3682 1.0384 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 2.3675 1.0594 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 2.3675 1.0369 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 2.3673 1.0330 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 2.3670 1.0560 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 2.3666 1.0523 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 2.3656 1.0632 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 2.3646 1.0283 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 2.3640 1.0604 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 2.3634 1.0484 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 2.3627 1.0460 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 2.3619 1.0482 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 2.3612 1.0624 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 2.3607 1.0629 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 2.3595 1.0585 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 2.3594 1.0493 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 2.3589 1.0557 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 2.3585 1.0537 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 2.3584 1.0170 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 2.3577 1.0368 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 2.3577 1.0574 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 2.3571 1.0576 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 2.3565 1.0373 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 2.3560 1.0586 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 2.3558 1.0650 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 2.3555 1.0326 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 2.3549 1.0249 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 2.3545 1.0470 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 2.3544 1.0688 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 2.3541 1.0469 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 2.3540 1.0949 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 2.3540 1.0703 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 2.3536 1.0629 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 2.3530 1.0603 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 2.3529 1.0833 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 2.3527 1.0157 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 2.3520 1.0430 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 2.3514 1.0346 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 2.3513 1.0197 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 2.3511 1.0503 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 2.3510 1.0584 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 2.3508 1.0314 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 2.3504 1.0494 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 2.3500 1.0618 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 2.3503 1.0366 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 2.3499 1.0673 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 2.3499 1.0467 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 2.3495 1.0955 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 2.3491 1.0784 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 2.3485 1.0806 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 2.3485 1.0563 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 2.3480 1.0836 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 2.3475 1.0472 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 2.3468 1.0594 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 2.3463 1.0573 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 2.3460 1.0596 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 2.3456 1.0431 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 2.3451 1.0558 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 2.3450 1.0509 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 2.3446 1.0908 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 2.3443 1.1784 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 2.3438 1.1103 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 2.3434 1.0495 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 2.3430 1.0664 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 2.3426 1.0524 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 2.3423 1.0761 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 2.3419 1.0401 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 2.3417 1.0519 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 2.3412 1.0737 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 2.3411 1.1220 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 2.3409 1.1129 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 2.3403 1.0587 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 2.3399 1.0743 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 2.3396 1.0469 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 2.3394 1.0524 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 2.3391 1.0657 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 2.3391 1.0682 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 2.3389 1.0655 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 2.3384 1.0611 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 2.3382 1.0672 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 2.3380 1.0665 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 2.3377 1.0581 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 2.3374 1.0545 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 2.3371 1.0870 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 2.3366 1.0323 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 2.3364 1.0493 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 2.3362 1.0884 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 2.3362 1.0782 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 2.3360 1.0606 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 2.3359 1.0944 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 2.3356 1.0334 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 2.3352 1.0737 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 2.3351 1.0677 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 2.3348 1.0514 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 2.3345 1.0304 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 2.3343 1.0130 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 2.3342 1.0657 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 2.3340 1.0386 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 2.3338 1.0808 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 2.3335 1.0841 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 2.3332 1.0815 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 2.3329 1.0942 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 2.3328 1.0663 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 2.3325 1.0847 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 2.3324 1.0466 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 2.3322 1.0491 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 2.3320 1.0508 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 2.3320 1.0696 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 2.3318 1.0437 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 2.3318 1.0369 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 2.3315 1.0545 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 2.3313 1.0438 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 2.3310 1.0300 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 2.3308 1.0609 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 2.3307 1.0822 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 2.3306 1.0579 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 2.3305 1.1086 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 2.3303 1.0726 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 2.3300 1.0695 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 2.3300 1.0664 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 2.3300 1.0362 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 687/890 Training loss: 2.3299 1.0304 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 2.3298 1.0522 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 2.3296 1.0766 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 2.3295 1.0327 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 2.3292 1.0758 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 2.3289 1.0767 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 2.3286 1.0840 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 2.3286 1.1029 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 2.3285 1.0600 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 2.3281 1.1029 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 2.3279 1.0669 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 2.3277 1.0579 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 2.3275 1.0536 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 2.3274 1.0361 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 2.3272 1.0515 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 2.3271 1.0475 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 2.3269 1.0541 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 2.3266 1.0426 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 2.3265 1.0358 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 2.3262 1.0852 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 2.3261 1.0422 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 2.3260 1.0143 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 2.3259 1.0534 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 2.3258 1.0417 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 2.3256 1.0503 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 2.3254 1.0547 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 2.3513 1.0630 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 2.2993 1.0554 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 2.2858 1.0379 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 2.2834 1.0613 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 2.2816 1.0752 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 2.2792 1.0725 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 2.2805 1.0778 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 2.2817 0.9977 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 2.2838 1.0177 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 2.2830 1.0369 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 2.2818 1.0320 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 2.2811 1.0216 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 2.2815 1.0277 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 2.2838 1.0160 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 2.2837 1.0384 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 2.2839 1.0357 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 2.2840 0.9897 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 2.2862 1.0377 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 2.2864 1.0455 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 2.2850 1.0143 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 2.2843 1.0583 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 2.2861 1.0070 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 2.2862 1.0397 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 2.2853 1.0683 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 2.2846 1.0453 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 2.2843 1.0338 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 2.2839 1.0780 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 2.2840 1.0565 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 2.2846 1.0455 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 2.2845 1.0806 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 2.2849 1.0729 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 2.2843 1.0427 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 2.2839 1.0709 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 2.2843 1.0314 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 2.2839 1.0907 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 2.2837 1.0408 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 2.2835 1.0653 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 2.2825 1.0390 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 2.2816 1.0708 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 2.2810 1.0442 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 2.2805 1.0253 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 2.2801 1.0428 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 2.2795 1.0346 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 2.2787 1.0526 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 2.2783 1.0463 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 2.2769 1.0647 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 2.2769 1.0352 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 2.2762 1.0605 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 2.2759 1.0505 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 2.2762 1.0610 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 2.2758 1.0448 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 2.2761 1.0577 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 2.2754 1.0324 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 2.2750 1.0817 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 2.2746 1.0691 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 2.2747 1.0800 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 2.2745 1.0565 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 2.2739 1.0303 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 2.2736 1.0505 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 2.2735 1.0138 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 2.2733 1.0111 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 2.2734 1.0756 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 2.2735 1.0680 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 2.2733 1.0450 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 2.2730 1.0326 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 2.2730 1.0668 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 2.2729 1.0515 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 2.2723 1.0487 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 2.2718 1.0695 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 2.2718 1.0696 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 2.2718 1.0545 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 2.2717 1.0689 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 2.2718 1.0642 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 2.2713 1.0354 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 2.2710 1.0462 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 2.2712 1.0834 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 2.2709 1.0627 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 2.2708 1.0884 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 2.2703 1.0424 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 2.2699 1.0337 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 2.2695 1.1020 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 2.2694 1.0591 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 2.2688 1.0562 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 2.2684 1.0553 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 2.2676 1.0492 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 2.2672 1.0926 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 2.2669 1.0629 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 2.2666 1.0212 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 2.2662 1.0726 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 2.2661 1.0823 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 2.2658 1.0318 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 2.2656 1.1080 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 2.2651 0.9909 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 2.2647 1.0764 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 2.2643 1.0457 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 808/890 Training loss: 2.2640 1.0552 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 2.2637 1.0897 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 2.2634 1.0310 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 2.2630 1.0538 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 2.2625 1.0612 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 2.2624 1.0491 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 2.2622 1.0809 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 2.2617 1.0550 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 2.2615 1.0462 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 2.2611 1.0658 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 2.2608 1.0644 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 2.2606 1.0924 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 2.2605 1.0354 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 2.2604 1.0270 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 2.2601 1.0611 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 2.2600 1.0825 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 2.2599 1.0962 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 2.2596 1.0548 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 2.2593 1.0676 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 2.2591 1.0493 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 2.2586 1.0842 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 2.2584 1.0154 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 2.2583 1.0815 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 2.2582 1.0174 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 2.2581 1.0481 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 2.2579 1.0698 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 2.2576 1.1068 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 2.2574 1.0735 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 2.2573 1.1058 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 2.2571 1.0846 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 2.2567 1.0790 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 2.2565 1.0266 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 2.2564 1.0743 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 2.2562 1.0925 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 2.2561 1.0322 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 2.2559 1.0182 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 2.2555 1.0103 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 2.2554 1.0392 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 2.2553 1.0703 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 2.2551 1.0566 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 2.2549 1.0664 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 2.2548 1.0402 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 2.2548 1.0149 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 2.2548 1.0891 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 2.2545 1.1438 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 2.2545 1.0793 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 2.2544 1.1022 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 2.2542 1.0588 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 2.2540 1.0409 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 2.2536 1.0530 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 2.2536 1.0333 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 2.2535 1.0200 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 2.2535 1.0585 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 2.2533 1.0258 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 2.2530 1.0294 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 2.2530 1.0572 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 2.2532 1.0553 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 2.2532 1.0854 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 2.2531 1.0371 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 2.2529 1.0604 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 2.2528 1.0938 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 2.2526 1.1014 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 2.2523 1.0828 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 2.2520 1.0660 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 2.2521 1.0515 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 2.2520 1.0513 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 2.2518 1.0419 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 2.2516 1.0268 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 2.2515 1.0258 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 2.2514 1.0347 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 2.2513 1.0550 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 2.2511 1.0247 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 2.2510 1.0429 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 2.2509 1.0912 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 2.2507 1.0819 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 2.2504 1.0580 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 2.2503 1.0792 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 2.2502 1.0849 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 2.2502 1.0843 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 2.2501 1.0603 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 2.2500 1.0637 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 2.2498 1.0614 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 2.2495 1.0957 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4210 1.6990 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.4082 1.7125 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.3901 1.7179 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.3494 1.6799 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.2647 1.6795 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.1689 1.7145 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 4.0868 1.6352 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 4.0170 1.6025 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 3.9544 1.6783 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.9000 1.6844 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.8502 1.6560 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.8079 1.6932 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.7709 1.6172 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.7387 1.6134 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.7095 1.7289 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.6837 1.6457 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.6591 1.6895 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.6392 1.7474 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.6191 1.6699 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.5989 1.6405 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.5812 1.7159 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.5649 1.6576 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.5500 1.6003 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.5363 1.7329 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.5230 1.6211 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.5113 1.6630 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.5007 1.6449 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.4897 1.6825 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.4797 1.6477 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.4703 1.7110 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.4622 1.6853 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.4534 1.6711 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.4451 1.6502 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.4379 1.7084 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.4301 1.7256 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.4233 1.6402 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.4162 1.7153 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.4095 1.6775 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.4030 1.6449 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 40/890 Training loss: 3.3971 1.7391 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.3911 1.7027 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.3855 1.6288 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.3801 1.7507 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.3749 1.7371 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.3698 1.7018 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.3652 1.8059 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.3608 1.7106 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.3568 1.6831 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.3529 1.6859 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.3491 1.6516 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.3452 1.7240 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.3414 1.6836 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.3379 1.6990 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.3342 1.6874 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.3309 1.7140 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.3273 1.6707 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.3240 1.6994 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.3210 1.6967 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.3177 1.7054 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.3148 1.6769 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.3120 1.7430 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.3097 1.7092 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.3074 1.6430 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.3045 1.6652 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.3017 1.6919 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.2994 1.6745 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.2972 1.7269 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.2943 1.6336 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.2919 1.6820 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.2898 1.7134 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.2875 1.6758 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.2856 1.6461 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.2835 1.6831 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.2815 1.7321 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.2797 1.6499 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.2780 1.6501 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.2761 1.7491 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.2743 1.6592 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.2724 1.6055 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.2705 1.7734 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.2686 1.7313 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.2670 1.6412 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.2655 1.6701 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.2638 1.6813 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.2619 1.6047 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.2603 1.7202 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.2585 1.6847 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.2569 1.6699 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.2555 1.6510 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.2541 1.7533 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.2527 1.6765 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.2512 1.6726 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.2497 1.7552 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.2483 1.6218 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.2468 1.7004 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.2454 1.6847 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.2441 1.6598 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.2427 1.6032 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.2414 1.7397 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.2400 1.6440 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.2387 1.7032 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.2374 1.6656 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.2361 1.6960 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.2348 1.6637 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.2336 1.6670 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.2322 1.7045 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.2308 1.6386 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.2293 1.6825 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.2280 1.7018 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.2264 1.7155 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.2251 1.6968 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.2237 1.7251 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.2222 1.6803 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.2206 1.7191 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.2191 1.6673 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.2176 1.8556 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.2161 1.5804 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.2147 1.6980 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.2133 1.7265 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.2117 1.7062 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.2104 1.6368 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.2089 1.6882 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.2074 1.7018 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.2059 1.7088 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.2042 1.6676 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.2024 1.7181 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.2007 1.6748 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.1991 1.6603 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.1973 1.7996 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.1956 1.6650 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.1939 1.6811 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.1920 1.6936 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.1902 1.7162 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.1883 1.6626 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 3.1862 1.6788 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 3.1841 1.7194 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 3.1820 1.6675 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 3.1800 1.6954 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 3.1781 1.7781 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 3.1759 1.6753 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 3.1739 1.6886 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 3.1717 1.6415 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 3.1696 1.7047 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 3.1674 1.6646 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 3.1653 1.6784 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 3.1632 1.7235 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 3.1611 1.6799 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 3.1591 1.6154 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 3.1568 1.7214 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 3.1546 1.7149 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 3.1528 1.6448 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 3.1508 1.7231 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 3.1487 1.6791 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 3.1465 1.6998 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 3.1443 1.6835 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 3.1420 1.7450 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 3.1397 1.6281 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 3.1375 1.6470 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 3.1352 1.7214 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 3.1329 1.7202 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 3.1307 1.6882 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 162/890 Training loss: 3.1283 1.6791 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 3.1259 1.6523 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 3.1237 1.6729 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 3.1214 1.6879 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 3.1191 1.7036 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 3.1169 1.6440 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 3.1146 1.6998 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 3.1124 1.7037 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 3.1101 1.6501 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 3.1080 1.6787 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 3.1060 1.6796 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 3.1039 1.6788 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 3.1020 1.6757 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 3.1000 1.7509 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 3.0978 1.6448 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 3.0956 1.6754 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 3.0932 1.7021 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.7233 1.6969 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.6906 1.6936 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.6830 1.7055 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.6805 1.6923 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.6793 1.6530 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.6763 1.6072 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.6758 1.7230 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.6761 1.6993 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.6757 1.7501 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.6716 1.6670 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.6686 1.7477 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.6678 1.7340 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.6655 1.6224 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.6660 1.7388 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.6653 1.6860 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.6640 1.7085 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.6624 1.7384 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.6625 1.7269 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.6614 1.6628 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.6590 1.6656 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.6569 1.6925 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.6563 1.6894 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.6549 1.7134 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.6532 1.7234 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.6514 1.7007 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.6507 1.6830 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.6489 1.6749 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.6476 1.6895 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.6462 1.7482 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.6454 1.6834 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.6446 1.7147 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.6434 1.6763 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.6416 1.6913 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.6405 1.7210 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.6388 1.7526 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.6377 1.6281 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.6360 1.6627 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.6340 1.7236 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.6322 1.6849 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.6303 1.6961 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.6286 1.7367 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.6270 1.7231 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.6252 1.6643 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.6236 1.6834 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.6221 1.7159 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.6201 1.6625 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.6190 1.6714 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.6178 1.7254 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.6165 1.7202 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.6157 1.7388 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.6145 1.7062 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.6131 1.7278 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.6118 1.6509 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.6104 1.7000 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.6090 1.7508 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.6078 1.6326 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.6067 1.6493 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.6054 1.7160 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.6042 1.6548 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.6031 1.6292 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.6019 1.7162 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.6009 1.6747 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.6001 1.7231 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.5988 1.7018 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.5975 1.7073 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.5967 1.6831 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.5956 1.6857 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.5940 1.7378 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.5927 1.7299 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.5920 1.7203 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.5910 1.6499 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.5901 1.7493 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.5890 1.6763 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.5879 1.7144 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.5869 1.7112 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.5863 1.7129 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.5852 1.7029 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.5843 1.6719 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.5831 1.7918 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.5820 1.7050 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.5809 1.6915 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.5799 1.7357 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.5789 1.7837 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.5778 1.7328 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.5764 1.6724 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.5753 1.6882 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.5743 1.6748 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.5732 1.6863 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.5722 1.6912 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.5714 1.7269 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.5704 1.7001 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.5696 1.6825 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.5686 1.7518 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.5674 1.7268 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.5662 1.7103 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.5652 1.7313 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.5643 1.7683 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.5633 1.7673 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.5624 1.7117 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.5614 1.6880 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.5606 1.7502 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.5599 1.7415 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.5588 1.7018 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.5578 1.6346 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 283/890 Training loss: 2.5567 1.7828 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.5558 1.6818 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.5548 1.6695 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.5540 1.7772 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.5532 1.7325 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.5521 1.7047 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.5513 1.7511 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.5506 1.7851 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.5497 1.6931 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.5489 1.6718 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.5480 1.6795 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.5469 1.7811 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.5460 1.7063 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.5453 1.7295 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.5446 1.6972 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.5439 1.7402 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.5434 1.7352 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.5426 1.6763 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.5418 1.7379 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.5411 1.6698 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.5403 1.7199 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.5393 1.7029 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.5386 1.7413 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.5380 1.7313 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.5372 1.6304 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.5365 1.7465 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.5357 1.7065 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.5349 1.6690 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.5342 1.6662 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.5336 1.7168 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.5326 1.6521 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.5318 1.7150 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.5310 1.7197 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.5303 1.6835 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.5297 1.6857 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.5289 1.7110 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.5282 1.7266 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.5274 1.7008 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.5267 1.7114 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.5259 1.7582 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.5251 1.6546 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.5246 1.7155 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.5239 1.7133 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.5233 1.6721 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.5225 1.8115 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.5217 1.6883 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.5212 1.7500 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.5208 1.7315 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.5201 1.6700 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.5194 1.7038 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.5186 1.7705 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.5179 1.6597 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.5172 1.6983 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.5164 1.6989 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.5156 1.7642 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.5149 1.7007 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.5143 1.6449 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.5134 1.7369 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.5127 1.7588 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.5120 1.6659 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.5114 1.7016 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.5107 1.7918 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.5100 1.7122 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.5094 1.7285 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.5088 1.7042 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.5080 1.7648 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.5075 1.6812 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.5070 1.7214 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.5066 1.7087 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.5063 1.6805 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.5059 1.6846 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.5053 1.6950 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.5046 1.7293 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.5038 1.6831 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.4267 1.6760 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.3884 1.7151 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.3790 1.6760 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.3785 1.7459 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.3761 1.6805 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.3744 1.7348 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.3763 1.7420 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.3776 1.7201 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.3797 1.7237 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.3790 2.0080 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.3773 2.0030 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.3763 1.9414 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.3764 1.7463 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.3783 1.8970 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.3778 1.9892 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.3775 1.8458 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.3769 1.8268 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.3787 1.9186 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.3788 1.9630 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.3770 2.3322 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.3758 1.9844 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.3762 2.6341 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.3756 3.1861 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.3748 3.0014 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.3739 2.1342 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.3735 1.7756 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.3729 1.8505 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.3723 2.3052 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.3723 1.9275 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.3718 1.6715 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.3721 1.6350 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.3710 1.9076 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.3702 2.2968 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.3704 2.0396 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.3698 2.0054 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.3698 1.2692 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.3692 1.2674 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.3679 1.3363 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.3667 1.3026 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.3658 1.3049 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.3649 1.4070 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.3640 2.0262 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.3629 1.3865 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.3622 1.2531 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 2.3612 1.2151 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.3598 1.2120 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.3597 1.2563 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 404/890 Training loss: 2.3591 1.7761 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.3585 2.2746 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.3586 1.6119 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.3579 1.5992 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.3577 1.8076 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.3572 1.3795 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.3566 1.4312 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.3560 1.2909 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.3557 1.2542 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.3553 1.2175 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.3547 1.3650 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.3540 1.2705 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.3539 1.3411 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.3534 1.2033 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.3531 1.2354 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.3530 1.3201 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.3524 1.3086 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.3516 2.0785 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.3514 1.9950 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.3509 2.0960 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.3500 2.1516 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.3494 2.0340 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.3492 2.1702 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.3489 2.1685 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.3488 2.1499 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.3485 1.9777 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.3480 1.7379 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.3475 1.2627 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.3476 1.3466 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.3470 1.5820 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.3467 1.2090 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.3462 1.3006 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.3457 1.4531 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.3451 1.6966 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.3448 2.1978 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 2.3442 2.2312 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.3435 1.2242 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.3426 1.2768 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 2.3420 1.8871 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.3416 1.8123 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.3411 1.2598 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 2.3405 1.8163 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.3401 1.4526 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.3397 1.4944 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.3393 1.2193 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.3389 1.2632 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.3382 1.1896 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.3375 1.2533 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.3371 1.2469 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.3367 1.5767 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.3363 1.6218 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.3358 1.2595 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.3353 1.1637 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.3351 1.2346 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.3348 1.3794 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.3342 1.1907 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.3337 1.1784 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.3333 1.2158 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.3329 1.1922 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.3325 1.1685 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.3322 1.1815 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.3319 1.1824 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.3313 1.1875 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 2.3309 1.1756 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 2.3307 1.2054 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 2.3302 1.1855 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 2.3297 1.1898 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 2.3294 1.2086 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 2.3288 1.3098 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 2.3284 1.1820 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 2.3281 1.2041 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 2.3279 1.1602 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 2.3276 1.1991 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 2.3274 1.4635 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 2.3270 1.3875 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 2.3266 1.3497 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 2.3263 1.3890 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 2.3259 1.1951 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 2.3254 1.2318 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 2.3250 1.2051 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 2.3248 1.1798 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 2.3244 1.2308 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 2.3241 1.1772 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 2.3237 1.2061 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 2.3231 1.1813 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 2.3228 1.2718 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 2.3226 1.2572 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 2.3221 1.2088 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 2.3219 1.1759 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 2.3215 1.2091 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 2.3212 1.6843 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 2.3210 1.3056 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 2.3205 1.7528 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 2.3203 1.3568 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 2.3199 1.2091 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 2.3196 1.2292 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 2.3192 1.1805 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 2.3189 1.2043 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 2.3188 1.1740 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 2.3185 1.1646 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 2.3183 1.2100 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 2.3180 1.2142 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 2.3176 1.1916 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 2.3174 1.1811 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 2.3173 1.1819 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 2.3170 1.8860 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 2.3168 1.2130 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 2.3164 1.2133 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 2.3160 1.2033 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 2.3156 1.1905 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 2.3153 1.1870 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 2.3148 1.1788 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 2.3146 1.2291 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 2.3143 1.5514 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 2.3138 1.3756 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 2.3135 1.3990 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 2.3132 1.3211 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 2.3129 1.8788 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 2.3126 1.3774 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 2.3123 1.4330 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 2.3120 1.5012 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 525/890 Training loss: 2.3117 1.6547 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 2.3113 1.5334 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 2.3111 1.5695 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 2.3109 1.4013 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 2.3109 1.6765 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 2.3109 1.5985 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 2.3108 1.5367 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 2.3105 1.8375 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 2.3101 1.7065 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 2.3096 1.4881 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 2.2944 1.2141 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 2.2614 1.2586 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 2.2501 1.5904 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 2.2478 1.6913 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 2.2475 1.3758 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 2.2450 1.4569 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 2.2466 1.4351 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 2.2489 1.4383 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 2.2498 1.2365 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 2.2484 1.2464 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 2.2477 1.3059 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 2.2471 1.2284 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 2.2468 1.3168 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 2.2494 1.4310 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 2.2498 1.7735 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 2.2493 1.9512 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 2.2483 1.9995 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 2.2502 2.2182 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 2.2501 2.2097 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 2.2488 1.8775 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 2.2481 2.0358 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 2.2489 1.5379 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 2.2485 1.5640 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 2.2476 1.2218 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 2.2471 1.3898 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 2.2469 1.4789 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 2.2463 1.3301 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 2.2460 1.5095 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 2.2465 1.3244 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 2.2465 1.4823 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 2.2465 1.9551 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 2.2459 2.1552 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 2.2454 1.6379 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 2.2456 1.5234 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 2.2451 1.5256 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 2.2449 1.3076 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 2.2445 1.2985 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 2.2432 1.2717 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 2.2425 1.3430 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 2.2415 1.2463 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 2.2405 1.5347 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 2.2397 1.7899 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 2.2386 1.3929 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 2.2378 1.3502 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 2.2375 1.2393 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 2.2363 2.1233 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 2.2364 1.4604 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 2.2360 1.2682 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 2.2355 1.6194 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 2.2358 1.2010 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 2.2352 1.1985 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 2.2352 1.2160 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 2.2345 1.2060 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 2.2340 1.4918 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 2.2335 1.6820 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 2.2333 1.2310 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 2.2331 1.1806 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 2.2326 1.2097 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 2.2321 1.1987 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 2.2320 1.2182 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 2.2316 1.2069 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 2.2316 1.1733 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 2.2317 1.1868 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 2.2313 1.1984 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 2.2306 1.2044 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 2.2306 1.1847 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 2.2304 1.1895 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 2.2296 1.1959 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 2.2291 1.1748 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 2.2289 1.2062 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 2.2289 1.2091 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 2.2288 1.2086 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 2.2290 1.2586 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 2.2285 1.1831 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 2.2282 1.2441 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 2.2286 1.1809 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 2.2280 1.1691 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 2.2280 1.3651 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 2.2276 1.3267 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 2.2272 1.2224 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 2.2267 1.4902 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 2.2267 1.4257 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 2.2263 1.8308 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 2.2259 1.6760 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 2.2251 1.3561 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 2.2247 1.4270 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 2.2243 2.0790 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 2.2239 2.0777 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 2.2234 1.3681 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 2.2233 1.2113 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 2.2230 1.2029 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 2.2228 1.1998 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 2.2224 1.2169 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 2.2218 1.3284 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 2.2213 1.3174 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 2.2208 1.2150 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 2.2205 1.2284 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 2.2201 1.2198 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 2.2197 1.3273 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 2.2192 1.2989 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 2.2191 1.2073 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 2.2189 1.3262 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 2.2184 1.3296 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 2.2180 1.3139 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 2.2176 1.3994 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 2.2173 1.3470 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 2.2170 1.2420 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 2.2169 1.2247 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 2.2167 1.2329 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 2.2163 1.3484 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 2.2162 1.9221 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 646/890 Training loss: 2.2159 1.3136 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 2.2156 1.3445 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 2.2153 1.2132 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 2.2149 1.1755 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 2.2144 1.1955 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 2.2141 1.1868 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 2.2139 1.1738 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 2.2138 1.1888 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 2.2135 1.1800 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 2.2135 1.1859 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 2.2132 1.1782 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 2.2129 1.1751 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 2.2128 1.1841 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 2.2125 1.1931 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 2.2121 1.1632 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 2.2118 1.2071 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 2.2117 1.1720 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 2.2116 1.1824 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 2.2114 1.1921 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 2.2112 1.1844 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 2.2108 1.1904 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 2.2107 1.1756 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 2.2105 1.1702 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 2.2102 1.1730 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 2.2100 1.1793 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 2.2098 1.1724 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 2.2096 1.1924 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 2.2096 1.1829 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 2.2093 1.1917 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 2.2093 1.1843 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 2.2090 1.2229 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 2.2088 1.2368 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 2.2086 1.1704 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 2.2084 1.1495 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 2.2083 1.1891 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 2.2082 1.2014 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 2.2081 1.1844 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 2.2079 1.1956 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 2.2077 1.2008 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 2.2075 1.1848 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 2.2076 1.1616 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 2.2074 1.1623 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 2.2072 1.1781 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 2.2070 1.1819 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 2.2068 1.1732 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 2.2065 1.1766 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 2.2063 1.1904 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 2.2059 1.1874 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 2.2057 1.1690 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 2.2056 1.1790 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 2.2052 1.1758 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 2.2050 1.1791 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 2.2047 1.1856 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 2.2046 1.1858 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 2.2043 1.1914 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 2.2041 1.1954 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 2.2040 1.1770 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 2.2039 1.1842 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 2.2036 1.1805 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 2.2034 1.2076 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 2.2033 1.1696 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 2.2032 1.1650 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 2.2032 1.1706 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 2.2032 1.1843 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 2.2030 1.1764 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 2.2027 1.1682 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 2.2024 1.1667 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 2.2202 1.1787 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 2.1765 1.1673 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 2.1645 1.1715 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 2.1579 1.1824 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 2.1586 1.1880 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 2.1568 1.1877 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 2.1577 1.1646 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 2.1593 1.2013 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 2.1607 1.1654 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 2.1608 1.1831 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 2.1591 1.1834 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 2.1582 1.2010 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 2.1584 1.1828 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 2.1606 1.1867 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 2.1605 1.1898 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 2.1600 1.1651 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 2.1594 1.1847 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 2.1614 1.1798 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 2.1610 1.1867 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 2.1603 1.1680 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 2.1599 1.1629 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 2.1607 1.1705 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 2.1602 1.1952 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 2.1591 1.1751 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 2.1583 1.2157 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 2.1576 1.1850 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 2.1570 1.2017 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 2.1572 1.1678 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 2.1579 1.1726 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 2.1580 1.1696 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 2.1586 1.1641 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 2.1580 1.2489 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 2.1579 1.1706 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 2.1585 1.1778 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 2.1581 1.1821 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 2.1578 1.1710 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 2.1573 1.1808 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 2.1562 1.1828 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 2.1551 1.1625 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 2.1544 1.1666 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 2.1536 1.1666 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 2.1531 1.2062 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 2.1524 1.1530 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 2.1519 1.1576 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 2.1516 1.1750 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 2.1503 1.1703 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 2.1503 1.1877 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 2.1500 1.1984 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 2.1498 1.1700 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 2.1500 1.1605 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 2.1495 1.1559 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 2.1497 1.1814 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 2.1492 1.1781 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 2.1486 1.1803 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 767/890 Training loss: 2.1481 1.1745 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 2.1478 1.1784 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 2.1479 1.1774 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 2.1475 1.2027 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 2.1471 1.1653 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 2.1469 1.1755 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 2.1466 1.1677 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 2.1469 1.1794 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 2.1470 1.1924 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 2.1468 1.1897 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 2.1464 1.1824 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 2.1465 1.1626 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 2.1464 1.1565 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 2.1456 1.1674 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 2.1453 1.1624 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 2.1451 1.1811 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 2.1451 1.1714 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 2.1451 1.1769 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 2.1451 1.1878 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 2.1447 1.1688 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 2.1444 1.1793 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 2.1446 1.1974 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 2.1443 1.1953 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 2.1442 1.2114 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 2.1437 1.1780 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 2.1434 1.1766 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 2.1429 1.1971 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 2.1429 1.1551 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 2.1425 1.1877 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 2.1420 1.1764 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 2.1414 1.1797 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 2.1410 1.1787 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 2.1406 1.1565 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 2.1402 1.1834 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 2.1397 1.1822 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 2.1396 1.1699 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 2.1393 1.1718 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 2.1390 1.2055 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 2.1385 1.1858 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 2.1380 1.1610 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 2.1376 1.1818 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 2.1374 1.1698 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 2.1370 1.1841 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 2.1367 1.1985 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 2.1363 1.1916 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 2.1358 1.1797 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 2.1356 1.1791 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 2.1354 1.1767 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 2.1350 1.1833 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 2.1348 1.1818 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 2.1343 1.1933 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 2.1342 1.1821 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 2.1340 1.1960 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 2.1338 1.1906 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 2.1337 1.1670 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 2.1334 1.1660 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 2.1333 1.1857 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 2.1330 1.1944 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 2.1329 1.1970 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 2.1326 1.1622 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 2.1322 1.1747 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 2.1318 1.1948 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 2.1316 1.1936 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 2.1315 1.1771 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 2.1315 1.1805 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 2.1312 1.1875 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 2.1312 1.1784 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 2.1309 1.1475 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 2.1306 1.1666 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 2.1306 1.1771 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 2.1304 1.1930 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 2.1299 1.2028 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 2.1298 1.1811 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 2.1297 1.2016 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 2.1296 1.1666 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 2.1294 1.1876 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 2.1292 1.1609 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 2.1288 1.1960 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 2.1287 1.2682 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 2.1286 1.1783 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 2.1283 1.1885 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 2.1282 1.1630 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 2.1280 1.1966 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 2.1279 1.1912 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 2.1278 1.1759 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 2.1275 1.1869 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 2.1276 1.1778 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 2.1273 1.2063 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 2.1271 1.1918 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 2.1269 1.1737 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 2.1267 1.1853 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 2.1267 1.1991 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 2.1266 1.1769 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 2.1266 1.1977 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 2.1264 1.2010 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 2.1262 1.1750 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 2.1261 1.1715 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 2.1262 1.1812 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 2.1261 1.1763 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 2.1260 1.1696 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 2.1257 1.1876 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 2.1255 1.1775 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 2.1254 1.1745 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 2.1252 1.1704 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 2.1249 1.1632 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 2.1249 1.1740 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 2.1248 1.1643 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 2.1245 1.1780 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 2.1244 1.1689 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 2.1242 1.1808 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 2.1241 1.1829 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 2.1240 1.1895 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 2.1238 1.1738 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 2.1238 1.1761 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 2.1236 1.1594 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 2.1234 1.1705 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 2.1232 1.1806 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 2.1230 1.1675 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 2.1229 1.1787 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 2.1229 1.1798 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 2.1229 1.1564 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 888/890 Training loss: 2.1227 1.1874 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 2.1224 1.1690 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 2.1222 1.1794 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4191 1.2571 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.4120 1.1711 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.4038 1.1851 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.3931 1.2008 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.3781 1.1650 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.3548 1.1660 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 4.3162 1.1593 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 4.2600 1.1825 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 4.1992 1.1747 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 4.1443 1.3713 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 4.0948 1.2405 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 4.0507 1.1935 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 4.0087 1.1878 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.9715 1.1634 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.9370 1.1769 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.9050 1.1625 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.8750 1.1622 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.8492 1.1966 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.8239 1.1623 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.7992 1.1975 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.7774 1.1763 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.7567 1.2013 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.7375 1.1888 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.7199 1.2022 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.7030 1.1899 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.6879 1.1862 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.6736 1.1585 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.6594 1.1672 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.6464 1.1751 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.6339 1.1601 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.6228 1.1958 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.6116 1.1686 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.6006 1.1720 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.5908 1.1720 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.5808 1.1757 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.5718 1.1811 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.5624 1.1580 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.5538 1.1723 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.5454 1.2341 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.5373 1.1861 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.5293 1.1646 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.5219 1.1859 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.5146 1.1624 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.5076 1.1868 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.5008 1.1650 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.4945 1.1692 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.4886 1.1766 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.4829 1.1741 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.4774 1.1770 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.4723 1.1587 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.4671 1.1812 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.4620 1.1928 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.4572 1.1869 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.4523 1.2999 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.4477 1.1956 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.4429 1.2076 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.4386 1.1520 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.4345 1.1676 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.4301 1.1818 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.4261 1.1876 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.4223 1.1793 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.4188 1.1656 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.4155 1.1823 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.4117 1.1916 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.4080 1.1982 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.4048 1.1832 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.4017 1.1782 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.3979 1.1838 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.3946 1.1796 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.3916 1.1957 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.3886 1.1722 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.3859 1.1658 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.3830 1.1970 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.3803 1.1563 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.3777 1.1760 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.3752 1.1767 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.3726 1.1879 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.3700 1.1754 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.3675 1.1544 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.3650 1.1668 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.3625 1.1652 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.3603 1.1728 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.3581 1.1853 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.3559 1.1653 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.3535 1.1764 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.3512 1.1462 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.3489 1.1774 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.3468 1.2531 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.3448 1.2160 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.3429 1.1990 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.3411 1.1965 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.3392 1.2073 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.3373 1.2061 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.3355 1.2335 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.3336 1.1790 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.3318 1.1848 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.3301 1.1924 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.3284 1.1878 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.3266 1.1649 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.3249 1.1740 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.3233 1.1922 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.3217 1.1815 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.3202 1.2499 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.3186 1.1866 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.3171 1.1835 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.3156 1.1767 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.3140 1.1859 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.3123 1.1559 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.3109 1.1678 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.3092 1.1875 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.3078 1.1887 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.3064 1.1697 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.3050 1.1740 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.3035 1.1846 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.3020 1.2010 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.3006 1.1771 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.2992 1.1733 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.2980 1.1686 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.2968 1.1693 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.2954 1.1799 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 121/890 Training loss: 3.2943 1.1758 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.2931 1.1736 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.2919 1.1842 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.2908 1.1808 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.2895 1.1952 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.2881 1.1738 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.2870 1.1890 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.2859 1.1833 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.2847 1.1969 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.2835 1.1857 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.2825 1.1790 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.2813 1.1603 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.2802 1.1937 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.2790 1.1857 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 3.2776 1.1875 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 3.2763 1.1650 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 3.2752 1.1800 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 3.2740 1.1794 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 3.2730 1.1957 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 3.2719 1.1819 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 3.2708 1.1732 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 3.2696 1.1825 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 3.2684 1.2010 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 3.2673 1.1878 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 3.2662 1.1867 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 3.2652 1.1842 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 3.2643 1.1804 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 3.2634 1.1854 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 3.2623 1.1620 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 3.2612 1.1758 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 3.2603 1.1970 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 3.2595 1.1944 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 3.2584 1.1974 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 3.2575 1.2144 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 3.2563 1.2535 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 3.2553 1.1831 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 3.2542 1.1819 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 3.2531 1.1802 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 3.2519 1.1969 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 3.2508 1.1975 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 3.2498 1.1772 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 3.2486 1.1771 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 3.2474 1.1865 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 3.2463 1.1852 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 3.2452 1.1686 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 3.2441 1.1804 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 3.2429 1.1613 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 3.2419 1.1906 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 3.2408 1.1816 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 3.2396 1.1653 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 3.2385 1.1688 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 3.2376 1.1547 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 3.2367 1.1628 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 3.2358 1.1836 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 3.2348 1.1787 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 3.2337 1.1678 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 3.2326 1.1794 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 3.2313 1.1658 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 3.0668 1.1691 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 3.0311 1.1820 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 3.0174 1.2189 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 3.0136 1.1939 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 3.0119 1.1606 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 3.0105 1.1762 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 3.0097 1.1919 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 3.0087 1.1784 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 3.0067 1.1821 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 3.0046 1.1551 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 3.0006 1.1908 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.9988 1.1881 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.9960 1.2002 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.9947 1.1678 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.9928 1.1773 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.9913 1.1733 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.9886 1.1783 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.9885 1.2039 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.9865 1.1800 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.9826 1.2036 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.9801 1.1906 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.9782 1.1771 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.9753 1.1722 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.9730 1.1801 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.9698 1.1733 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.9677 1.1698 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.9659 1.1818 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.9633 1.1796 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.9609 1.1883 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.9586 1.1751 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.9572 1.1992 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.9547 1.1999 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.9519 1.1938 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.9496 1.1911 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.9467 1.1782 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.9447 1.1580 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.9418 1.1747 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.9390 1.1689 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.9359 1.1991 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.9331 1.1717 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.9302 1.1748 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.9275 1.1813 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.9247 1.1879 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.9220 1.1720 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.9192 1.1786 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.9169 1.1963 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.9147 1.1797 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.9126 1.1795 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.9106 1.2024 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.9085 1.1898 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.9061 1.1739 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.9036 1.1863 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.9014 1.1722 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.8989 1.1821 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.8967 1.2008 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.8942 1.2079 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.8920 1.1793 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.8898 1.2160 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.8872 1.1899 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.8850 1.1963 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.8827 1.1756 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.8806 1.1805 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.8789 1.1615 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 242/890 Training loss: 2.8767 1.1774 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.8743 1.1782 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.8726 1.1786 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.8704 1.1892 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.8677 1.1846 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.8652 1.1574 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.8633 1.1990 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.8613 1.1658 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.8596 1.1782 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.8576 1.1812 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.8557 1.1786 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.8538 1.1788 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.8523 1.1635 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.8504 1.2069 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.8486 1.2277 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.8467 1.1985 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.8447 1.1909 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.8428 1.1798 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.8412 1.1826 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.8396 1.2030 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.8376 1.1858 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.8355 1.2052 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.8336 1.1665 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.8319 1.1674 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.8300 1.1910 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.8283 1.1872 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.8267 1.1713 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.8251 1.1847 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.8236 1.1707 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.8219 1.1916 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.8203 1.2152 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.8185 1.1906 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.8167 1.1858 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.8152 1.2008 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.8135 1.1800 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.8120 1.1751 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.8104 1.1868 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.8089 1.2035 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.8074 1.1782 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.8057 1.1920 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.8041 1.1573 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.8025 1.2044 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.8010 1.1862 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.7992 1.1776 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.7977 1.1980 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.7964 1.1755 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.7946 1.1959 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.7932 1.1821 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.7918 1.2068 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.7903 1.1916 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.7887 1.1838 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.7872 1.1872 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.7855 1.1846 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.7840 1.2013 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.7827 1.1806 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.7815 1.2064 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.7801 1.1911 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.7791 1.1774 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.7779 1.1957 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.7766 1.1720 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.7754 1.2057 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.7741 1.1901 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.7726 1.1768 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.7715 1.1733 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.7703 1.1885 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.7690 1.2013 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.7678 1.1640 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.7666 1.1768 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.7652 1.1904 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.7640 1.1849 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.7629 1.1725 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.7614 1.1733 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.7601 1.1881 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.7589 1.1930 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.7577 1.1926 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.7566 1.1969 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.7553 1.1926 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.7542 1.1821 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.7529 1.2006 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.7518 1.1844 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.7505 1.2047 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.7494 1.1797 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.7484 1.1906 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.7473 1.1866 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.7464 1.1685 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.7452 1.1762 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.7440 1.1657 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.7431 1.1769 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.7423 1.2118 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.7413 1.1719 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.7403 1.2101 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.7391 1.1759 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.7381 1.2026 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.7369 1.1812 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.7359 1.1766 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.7347 1.1866 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.7337 1.1724 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.7327 1.1749 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.7315 1.1639 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.7303 1.1674 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.7293 1.1724 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.7283 1.1722 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.7272 1.1668 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.7263 1.1938 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.7253 1.1840 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.7242 1.1651 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.7231 1.1932 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.7221 1.1892 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.7214 1.1811 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.7206 1.1746 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.7199 1.1934 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.7191 1.1962 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.7182 1.1677 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.7172 1.1683 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.7161 1.2877 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.6071 1.1958 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.5576 1.2065 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.5430 1.1858 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.5416 1.1997 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.5401 1.1832 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.5365 1.1763 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 363/890 Training loss: 2.5367 1.1745 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.5370 1.1989 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.5374 1.2198 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.5359 1.2062 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.5343 1.1882 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.5339 1.2007 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.5332 1.1746 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.5344 1.2217 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.5337 1.2070 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.5337 1.1848 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.5328 1.1794 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.5344 1.1830 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.5339 1.1978 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.5322 1.2028 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.5314 1.2050 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.5322 1.1764 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.5316 1.1704 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.5309 1.1934 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.5297 1.1964 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.5294 1.1659 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.5288 1.1911 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.5281 1.2091 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.5278 1.2045 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.5275 1.2048 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.5276 1.1934 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.5270 1.1479 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.5263 1.1823 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.5260 1.1765 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.5252 1.1941 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.5251 1.2017 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.5241 1.1767 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.5231 1.1989 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.5222 1.1701 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.5214 1.1750 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.5203 1.2071 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.5193 1.1718 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.5184 1.1946 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.5177 1.2087 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 2.5167 1.1990 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.5154 1.1714 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.5151 1.1938 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 2.5145 1.1704 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.5140 1.1628 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.5139 1.1734 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.5133 1.1904 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.5128 1.1667 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.5122 1.1723 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.5114 1.1705 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.5107 1.1698 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.5102 1.1801 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.5098 1.1676 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.5092 1.1711 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.5085 1.1673 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.5082 1.1915 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.5075 1.1765 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.5073 1.2000 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.5071 1.1783 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.5064 1.1961 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.5058 1.1774 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.5055 1.1650 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.5051 1.1749 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.5041 1.1734 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.5034 1.1862 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.5031 1.1786 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.5028 1.1653 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.5025 1.1702 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.5021 1.1624 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.5016 1.1982 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.5011 1.1781 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.5011 1.1783 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.5007 1.1785 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.5006 1.1911 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.5000 1.1647 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.4994 1.1804 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.4988 1.1799 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.4985 1.1865 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 2.4980 1.1836 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.4975 1.1606 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.4966 1.1704 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 2.4960 1.1645 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.4955 1.1757 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.4950 1.1701 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 2.4946 1.1880 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.4942 1.1704 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.4938 1.1795 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.4935 1.1995 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.4931 1.1957 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.4925 1.1764 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.4919 1.1691 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.4913 1.1879 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.4909 1.1911 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.4906 1.1892 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.4901 1.1805 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.4897 1.1986 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.4895 1.2908 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.4892 1.1962 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.4886 1.1759 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.4881 1.1872 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.4876 1.1884 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.4872 1.1871 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.4867 1.1947 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.4864 1.1955 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.4861 1.1920 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.4855 1.1804 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 2.4852 1.1561 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 2.4850 1.1872 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 2.4845 1.1873 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 2.4840 1.1770 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 2.4835 1.1869 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 2.4828 1.1774 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 2.4825 1.1615 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 2.4821 1.1902 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 2.4819 1.1716 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 2.4816 1.1620 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 2.4814 1.1806 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 2.4811 1.1816 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 2.4806 1.1853 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 2.4804 1.1701 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 2.4802 1.1911 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 2.4797 1.1972 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 2.4794 1.1901 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 484/890 Training loss: 2.4792 1.1547 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 2.4789 1.1757 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 2.4787 1.1882 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 2.4783 1.1899 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 2.4779 1.1781 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 2.4776 1.1796 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 2.4775 1.1769 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 2.4769 1.1956 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 2.4766 1.1935 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 2.4763 1.1759 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 2.4760 1.1719 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 2.4757 1.1815 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 2.4754 1.1854 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 2.4752 1.1898 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 2.4748 1.1723 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 2.4744 1.1617 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 2.4740 1.1856 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 2.4737 1.1839 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 2.4736 1.2033 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 2.4733 1.1666 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 2.4732 1.1796 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 2.4728 1.1686 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 2.4724 1.1914 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 2.4723 1.1852 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 2.4722 1.1769 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 2.4719 1.1835 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 2.4717 1.1648 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 2.4714 1.1643 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 2.4710 1.1872 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 2.4706 1.1694 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 2.4703 1.1835 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 2.4699 1.1914 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 2.4697 1.1701 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 2.4694 1.1721 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 2.4689 1.1809 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 2.4686 1.1956 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 2.4683 1.1916 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 2.4681 1.2064 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 2.4677 1.1979 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 2.4675 1.1737 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 2.4672 1.1854 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 2.4669 1.1844 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 2.4665 1.1867 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 2.4662 1.2003 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 2.4660 1.1863 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 2.4660 1.1958 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 2.4660 1.2012 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 2.4660 1.1688 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 2.4657 1.1870 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 2.4654 1.1942 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 2.4649 1.2026 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 2.4749 1.1704 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 2.4275 1.1639 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 2.4103 1.1756 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 2.4078 1.2058 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 2.4069 1.1795 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 2.4055 1.2058 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 2.4063 1.1816 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 2.4093 1.2208 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 2.4112 1.1757 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 2.4099 1.1694 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 2.4087 1.1820 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 2.4077 1.2058 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 2.4068 1.1695 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 2.4086 1.1781 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 2.4081 1.1723 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 2.4087 1.1964 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 2.4079 1.1727 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 2.4102 1.1900 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 2.4106 1.1888 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 2.4089 1.1528 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 2.4081 1.1778 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 2.4085 1.1855 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 2.4081 1.2726 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 2.4068 1.1980 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 2.4055 1.1755 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 2.4055 1.1836 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 2.4045 1.1692 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 2.4043 1.1667 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 2.4049 1.1755 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 2.4048 1.1976 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 2.4053 1.1753 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 2.4046 1.1722 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 2.4041 1.1846 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 2.4040 1.1543 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 2.4033 1.1627 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 2.4033 1.1815 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 2.4028 1.1962 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 2.4018 1.1774 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 2.4009 1.1917 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 2.4000 1.1815 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 2.3992 1.1873 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 2.3983 1.1844 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 2.3975 1.1871 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 2.3969 1.1617 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 2.3963 1.1867 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 2.3952 1.1942 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 2.3951 1.2065 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 2.3947 1.1840 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 2.3945 1.1805 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 2.3946 1.1926 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 2.3940 1.1787 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 2.3938 1.1798 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 2.3932 1.1784 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 2.3927 1.1977 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 2.3922 1.1719 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 2.3920 1.1703 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 2.3914 1.1654 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 2.3908 1.1748 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 2.3903 1.1907 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 2.3903 1.1614 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 2.3899 1.1868 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 2.3899 1.1914 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 2.3900 1.1854 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 2.3895 1.1977 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 2.3889 1.1960 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 2.3888 1.1790 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 2.3885 1.1811 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 2.3877 1.1728 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 2.3870 1.1823 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 2.3869 1.1697 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 605/890 Training loss: 2.3867 1.1615 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 2.3866 1.2116 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 2.3865 1.2325 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 2.3860 1.2076 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 2.3856 1.2141 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 2.3858 1.1880 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 2.3854 1.1975 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 2.3852 1.2030 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 2.3847 1.1760 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 2.3842 1.1759 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 2.3837 1.1706 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 2.3836 1.1992 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 2.3832 1.1863 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 2.3827 1.1981 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 2.3819 1.1897 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 2.3814 1.1932 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 2.3811 1.1918 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 2.3807 1.2026 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 2.3803 1.1759 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 2.3801 1.2019 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 2.3797 1.1877 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 2.3795 1.2020 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 2.3791 1.1903 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 2.3787 1.1933 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 2.3781 1.2088 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 2.3777 1.2105 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 2.3774 1.1832 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 2.3772 1.1849 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 2.3769 1.1967 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 2.3765 1.1645 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 2.3765 1.1889 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 2.3764 1.1867 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 2.3758 1.1767 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 2.3754 1.1898 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 2.3750 1.1676 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 2.3747 1.1734 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 2.3742 1.1715 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 2.3741 1.1936 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 2.3739 1.1764 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 2.3734 1.2093 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 2.3732 1.2011 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 2.3731 1.2020 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 2.3728 1.1793 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 2.3725 1.1799 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 2.3721 1.1942 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 2.3717 1.1757 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 2.3713 1.2012 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 2.3711 1.1684 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 2.3710 1.1801 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 2.3708 1.1788 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 2.3707 1.1812 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 2.3705 1.1848 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 2.3701 1.1848 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 2.3700 1.2676 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 2.3697 1.1751 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 2.3693 1.2008 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 2.3690 1.1750 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 2.3689 1.1781 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 2.3687 1.1802 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 2.3685 1.2069 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 2.3683 1.1731 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 2.3679 1.1991 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 2.3677 1.1995 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 2.3676 1.1675 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 2.3672 1.1941 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 2.3670 1.1802 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 2.3667 1.1722 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 2.3665 1.1809 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 2.3664 1.1650 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 2.3661 1.2107 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 2.3659 1.1826 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 2.3656 1.1808 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 2.3653 1.1986 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 2.3650 1.2034 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 2.3647 1.1829 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 2.3647 1.2095 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 2.3645 1.2077 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 2.3644 1.1931 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 2.3641 1.2058 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 2.3637 1.1736 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 2.3637 1.1809 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 2.3637 1.2126 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 2.3635 1.1960 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 2.3634 1.1996 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 2.3631 1.1844 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 2.3630 1.1756 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 2.3627 1.2165 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 2.3624 1.1787 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 2.3620 1.1791 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 2.3618 1.1853 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 2.3617 1.1995 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 2.3613 1.1998 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 2.3611 1.1946 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 2.3609 1.1982 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 2.3608 1.1678 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 2.3605 1.1676 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 2.3604 1.1760 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 2.3602 1.1930 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 2.3600 1.1855 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 2.3596 1.1901 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 2.3594 1.1995 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 2.3593 1.1823 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 2.3593 1.1919 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 2.3593 1.1872 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 2.3593 1.1795 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 2.3592 1.1824 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 2.3589 1.1928 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 2.3585 1.1858 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 2.3977 1.1559 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 2.3408 1.2077 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 2.3252 1.1834 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 2.3193 1.1893 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 2.3162 1.1885 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 2.3153 1.1915 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 2.3154 1.2104 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 2.3169 1.2020 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 2.3184 1.1826 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 2.3170 1.1786 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 2.3152 1.1734 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 2.3143 1.2101 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 2.3143 1.2073 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 726/890 Training loss: 2.3159 1.2050 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 2.3161 1.1840 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 2.3161 1.1656 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 2.3161 1.1655 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 2.3174 1.1807 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 2.3175 1.1827 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 2.3165 1.1801 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 2.3158 1.1849 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 2.3172 1.2107 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 2.3170 1.1906 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 2.3164 1.1583 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 2.3156 1.1783 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 2.3154 1.1767 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 2.3149 1.1732 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 2.3145 1.1764 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 2.3150 1.1850 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 2.3151 1.1817 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 2.3153 1.1849 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 2.3146 1.1948 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 2.3142 1.1706 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 2.3145 1.1975 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 2.3139 1.1817 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 2.3136 1.1793 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 2.3132 1.1994 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 2.3121 1.1851 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 2.3113 1.2007 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 2.3104 1.1891 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 2.3097 1.1721 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 2.3091 1.1693 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 2.3082 1.1850 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 2.3076 1.1828 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 2.3071 1.1977 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 2.3058 1.2610 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 2.3057 1.2019 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 2.3053 1.2111 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 2.3049 1.1827 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 2.3052 1.1599 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 2.3048 1.2010 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 2.3049 1.2145 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 2.3045 1.1869 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 2.3041 1.1909 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 2.3036 1.2098 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 2.3035 1.1990 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 2.3034 1.1642 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 2.3030 1.1783 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 2.3027 1.2030 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 2.3027 1.2172 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 2.3023 1.1949 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 2.3023 1.2090 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 2.3023 1.2048 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 2.3020 1.1911 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 2.3016 1.1852 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 2.3016 1.1813 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 2.3015 1.1920 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 2.3009 1.1862 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 2.3004 1.1741 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 2.3003 1.1853 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 2.3004 1.1819 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 2.3004 1.1939 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 2.3004 1.1984 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 2.3000 1.1931 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 2.2997 1.1731 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 2.2999 1.1979 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 2.2997 1.1651 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 2.2997 1.1780 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 2.2992 1.1652 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 2.2988 1.1697 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 2.2984 1.1981 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 2.2983 1.1803 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 2.2980 1.1803 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 2.2975 1.1935 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 2.2970 1.1997 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 2.2967 1.1875 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 2.2965 1.1864 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 2.2962 1.1967 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 2.2957 1.1967 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 2.2957 1.1851 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 2.2954 1.1919 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 2.2953 1.1872 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 2.2949 1.1819 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 2.2945 1.1900 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 2.2940 1.1692 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 2.2937 1.1967 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 2.2935 1.1718 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 2.2933 1.1576 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 2.2931 1.1861 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 2.2927 1.2020 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 2.2926 1.2031 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 2.2925 1.1755 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 2.2921 1.1721 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 2.2919 1.2124 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 2.2917 1.1917 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 2.2915 1.1722 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 2.2913 1.2063 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 2.2912 1.1757 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 2.2911 1.1868 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 2.2906 1.1728 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 2.2905 1.1767 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 2.2904 1.1800 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 2.2901 1.2054 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 2.2899 1.1892 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 2.2897 1.1747 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 2.2892 1.1641 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 2.2891 1.1768 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 2.2889 1.1877 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 2.2888 1.1789 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 2.2886 1.1937 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 2.2886 1.1641 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 2.2885 1.1885 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 2.2882 1.2110 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 2.2881 1.1818 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 2.2880 1.1925 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 2.2876 1.1785 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 2.2875 1.1933 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 2.2874 1.1876 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 2.2872 1.1899 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 2.2872 1.1879 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 2.2870 1.1960 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 2.2867 1.1928 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 2.2866 1.1838 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 2.2866 1.1941 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 847/890 Training loss: 2.2863 1.1772 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 2.2862 1.1927 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 2.2860 1.1781 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 2.2859 1.1897 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 2.2859 1.1931 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 2.2855 1.1776 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 2.2855 1.2030 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 2.2852 1.1791 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 2.2851 1.1687 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 2.2849 1.1762 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 2.2847 1.1811 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 2.2847 1.2843 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 2.2846 1.2265 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 2.2846 1.2192 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 2.2845 1.1996 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 2.2842 1.1862 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 2.2842 1.1790 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 2.2844 1.1852 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 2.2842 1.2464 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 2.2841 1.4277 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 2.2839 1.3478 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 2.2838 1.2584 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 2.2835 1.4772 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 2.2833 1.9313 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 2.2830 1.8258 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 2.2829 1.7371 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 2.2828 1.6183 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 2.2826 1.4947 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 2.2824 1.8957 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 2.2823 1.4604 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 2.2822 1.2699 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 2.2821 1.1847 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 2.2819 1.2547 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 2.2818 1.1962 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 2.2817 1.2156 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 2.2814 1.2084 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 2.2813 1.1878 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 2.2811 1.1755 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 2.2811 1.1895 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 2.2811 1.1614 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 2.2811 1.1690 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 2.2810 1.1847 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 2.2808 1.1936 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 2.2805 1.1761 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4186 1.6709 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.3961 1.5902 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.3559 1.5829 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.2140 1.6267 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.0842 1.6277 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 3.9860 1.6264 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 3.9053 1.6543 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 3.8383 1.6339 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 3.7813 1.6109 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.7344 1.6232 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.6938 1.5967 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.6594 1.6178 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.6282 1.6296 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.6012 1.5980 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.5772 1.5918 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.5554 1.5984 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.5349 1.5916 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.5179 1.6352 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.5012 1.6054 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.4844 1.6253 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.4697 1.5780 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.4562 1.6016 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.4434 1.6092 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.4316 1.6176 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.4203 1.5818 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.4104 1.6196 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.4013 1.6142 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.3915 1.6176 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.3828 1.6136 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.3748 1.6084 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.3681 1.5804 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.3604 1.6118 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.3531 1.6122 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.3465 1.5999 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.3397 1.5886 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.3337 1.5887 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.3272 1.6255 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.3210 1.5909 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.3149 1.6051 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.3093 1.6029 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.3038 1.6103 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.2984 1.6051 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.2932 1.6564 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.2880 1.6114 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.2830 1.6180 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.2783 1.6307 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.2740 1.7304 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.2698 1.6251 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.2658 1.6255 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.2618 1.6265 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.2577 1.5985 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.2536 1.6008 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.2496 1.5961 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.2456 1.6293 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.2418 1.6101 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.2377 1.6147 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.2339 1.5852 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.2301 1.5884 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.2262 1.6025 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.2225 1.6092 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.2189 1.6255 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.2156 1.6067 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.2124 1.6119 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.2085 1.6007 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.2047 1.5999 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.2013 1.6252 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.1977 1.6027 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.1935 1.6274 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.1895 1.5753 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.1860 1.5999 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.1822 1.5980 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.1787 1.6055 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.1749 1.6041 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.1710 1.6210 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.1673 1.6425 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.1637 1.6033 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.1598 1.5965 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.1559 1.6163 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 79/890 Training loss: 3.1518 1.6160 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.1476 1.6090 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.1435 1.5959 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.1396 1.6056 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.1355 1.6156 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.1313 1.6072 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.1268 1.6384 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.1224 1.5832 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.1181 1.6050 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.1137 1.6296 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.1093 1.6046 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.1051 1.5699 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.1007 1.5856 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.0964 1.6047 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.0922 1.6162 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.0878 1.6261 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.0835 1.6304 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.0790 1.5919 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.0747 1.6071 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.0704 1.6112 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.0660 1.5866 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.0617 1.5981 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.0575 1.5964 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.0532 1.6329 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.0488 1.5897 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.0444 1.5977 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.0401 1.6157 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.0358 1.6159 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.0314 1.6339 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.0272 1.6154 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.0231 1.6161 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.0186 1.6701 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.0143 1.5770 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.0103 1.7387 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.0060 1.6024 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.0017 1.6329 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 2.9974 1.6067 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 2.9930 1.7001 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 2.9889 1.6113 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 2.9848 1.6238 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 2.9809 1.6013 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 2.9769 1.6310 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 2.9731 1.7365 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 2.9692 1.6361 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 2.9652 1.6218 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 2.9614 1.6084 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 2.9575 1.6115 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 2.9535 1.6226 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 2.9498 1.6155 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 2.9461 1.6320 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 2.9423 1.6119 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 2.9386 1.6045 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 2.9349 1.6245 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 2.9311 1.6180 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 2.9274 1.6525 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 2.9239 1.6056 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 2.9201 1.6076 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 2.9165 1.6261 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 2.9129 1.6307 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 2.9094 1.6035 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 2.9060 1.6009 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 2.9025 1.5920 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 2.8992 1.6326 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 2.8957 1.6068 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 2.8923 1.6146 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 2.8889 1.6176 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 2.8855 1.6135 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 2.8823 1.6405 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 2.8791 1.6196 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 2.8759 1.5846 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 2.8726 1.5913 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 2.8693 1.5883 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 2.8663 1.6446 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 2.8634 1.6321 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 2.8604 1.6564 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 2.8573 1.6816 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 2.8542 1.6288 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 2.8511 1.6298 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 2.8480 1.6320 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 2.8449 1.6477 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 2.8418 1.6151 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 2.8389 1.6241 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 2.8360 1.6262 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 2.8329 1.6300 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 2.8299 1.6167 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 2.8270 1.6321 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 2.8242 1.6304 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 2.8214 1.6134 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 2.8186 1.6269 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 2.8158 1.6543 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 2.8131 1.6272 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 2.8103 1.6155 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 2.8076 1.6464 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 2.8050 1.6377 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 2.8025 1.6390 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 2.8001 1.6336 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 2.7976 1.6346 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 2.7950 1.6245 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 2.7923 1.6274 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 2.7895 1.7554 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.3712 1.6287 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.3285 1.6398 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.3139 1.6310 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.3116 1.6150 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.3091 1.6360 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.3070 1.6198 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.3072 1.6359 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.3083 1.5990 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.3093 1.6078 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.3083 1.6163 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.3064 1.6230 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.3056 1.6330 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.3050 1.6116 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.3071 1.6319 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.3060 1.6174 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.3052 1.7499 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.3045 1.6492 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.3060 1.6395 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.3053 1.6123 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.3036 1.6315 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.3025 1.6343 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 200/890 Training loss: 2.3032 1.6254 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.3017 1.6342 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.3000 1.6116 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.2986 1.6076 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.2973 1.6358 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.2958 1.6512 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.2949 1.6147 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.2947 1.6538 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.2941 1.6049 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.2933 1.6189 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.2919 1.6105 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.2906 1.6351 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.2899 1.6126 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.2885 1.6252 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.2875 1.6326 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.2865 1.6282 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.2848 1.6117 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.2838 1.6188 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.2825 1.6182 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.2812 1.6202 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.2800 1.6245 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.2788 1.6029 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.2773 1.5938 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.2762 1.5916 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.2743 1.6300 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.2737 1.6273 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.2726 1.6396 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.2716 1.6071 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.2711 1.6067 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.2698 1.6176 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.2694 1.6094 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.2683 1.6171 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.2671 1.6023 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.2662 1.6273 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.2654 1.5981 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.2646 1.6013 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.2635 1.6152 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.2626 1.6423 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.2621 1.6389 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.2623 1.6371 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.2630 1.6349 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.2639 1.6165 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.2643 1.6263 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.2644 1.5941 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.2647 1.6156 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.2646 1.6367 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.2640 1.6022 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.2637 1.6128 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.2634 1.6106 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.2632 1.6074 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.2624 1.6230 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.2617 1.6375 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.2604 1.6142 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.2595 1.6243 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.2591 1.6357 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.2580 1.6160 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.2572 1.6272 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.2560 1.6140 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.2549 1.6055 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.2538 1.9120 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.2530 2.4296 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.2518 1.6119 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.2507 1.6378 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.2493 1.5847 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.2481 1.6072 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.2473 1.6226 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.2464 1.6163 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.2452 1.7231 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.2445 1.6142 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.2435 1.6525 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.2427 1.6027 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.2415 1.6062 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.2405 1.6100 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.2394 1.5852 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.2385 1.6005 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.2376 1.6329 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.2366 1.6212 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.2354 1.6331 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.2343 1.6414 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.2335 1.6416 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.2327 1.6106 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.2316 1.6207 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.2307 1.6185 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.2297 1.6351 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.2288 1.6242 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.2280 1.6266 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.2274 1.6163 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.2268 1.6066 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.2258 1.6265 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.2250 1.6304 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.2243 1.6226 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.2234 1.6027 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.2225 1.6146 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.2215 1.6242 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.2203 1.6054 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.2194 1.6282 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.2187 1.6258 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.2180 1.6167 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.2174 1.5953 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.2167 1.5872 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.2158 1.6113 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.2148 1.6169 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.2143 1.6302 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.2134 1.6353 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.2125 1.6201 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.2119 1.6285 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.2111 1.6238 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.2104 1.6033 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.2097 1.6112 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.2089 1.5960 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.2079 1.6311 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.2072 1.6136 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.2066 1.6191 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.2058 1.6129 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.2052 1.6226 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.2045 1.6304 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.2039 1.6247 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.2035 1.6243 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.2027 1.6232 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.2021 1.5964 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.2014 1.5925 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 321/890 Training loss: 2.2006 1.6287 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.1999 1.6147 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.1992 1.6220 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.1987 1.6260 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.1981 1.6171 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.1976 1.6269 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.1968 1.6412 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.1960 1.6146 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.1952 1.6363 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.1948 1.6085 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.1941 1.5878 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.1935 1.6056 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.1928 1.6191 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.1921 1.5960 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.1913 1.6116 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.1906 1.6351 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.1898 1.6198 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.1893 1.6117 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.1888 1.6410 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.1881 1.6078 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.1874 1.7306 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.1868 1.6483 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.1862 1.6446 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.1855 1.6301 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.1850 1.6360 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.1845 1.6263 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.1839 1.6110 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.1832 1.6407 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.1825 1.6140 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.1818 1.5996 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.1814 1.6113 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.1808 1.6071 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.1803 1.6414 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.1797 1.5983 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.1789 1.6027 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.1783 1.5927 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.1239 1.6372 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.0787 1.6265 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.0685 1.6394 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.0618 1.6491 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.0604 1.6320 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.0547 1.6152 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.0548 1.6410 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.0553 1.6023 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.0574 1.6189 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.0561 1.5929 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.0533 1.6216 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.0516 1.6441 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.0517 1.6387 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.0539 1.6439 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.0522 1.6110 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.0503 1.6283 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.0492 1.6119 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.0512 1.6246 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.0512 1.6251 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.0512 1.6265 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.0503 1.6148 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.0511 1.6043 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.0502 1.6195 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.0490 1.6268 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.0486 1.6359 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.0468 1.6034 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.0455 1.5974 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.0453 1.6238 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.0458 1.6152 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.0456 1.6144 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.0450 1.6019 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.0439 1.6272 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.0433 1.6054 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.0438 1.6166 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.0429 1.6294 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.0422 1.6057 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.0414 1.6275 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.0398 1.6167 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.0384 1.6381 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.0372 1.6334 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.0364 1.6117 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.0359 1.6142 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.0351 1.6313 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.0339 1.6271 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 2.0336 1.6321 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.0318 1.6101 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.0315 1.6156 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 2.0307 1.6502 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.0302 1.6284 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.0304 1.6292 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.0295 1.6411 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.0297 1.6358 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.0289 1.6454 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.0284 1.6377 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.0277 1.6198 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.0274 1.6185 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.0273 1.6163 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.0266 1.6233 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.0258 1.7274 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.0258 1.6126 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.0253 1.6299 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.0254 1.6377 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.0255 1.6242 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.0253 1.6094 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.0247 1.6262 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.0245 1.6064 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.0242 1.6260 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.0233 1.6312 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.0227 1.6062 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.0222 1.6268 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.0222 1.6090 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.0219 1.6390 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.0216 1.6073 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.0209 1.6248 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.0204 1.6444 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.0202 1.6334 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.0197 1.6095 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.0195 1.6342 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.0187 1.6360 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.0181 1.6454 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.0173 1.6461 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.0170 1.6358 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 2.0161 1.6448 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.0155 1.6291 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.0146 1.6040 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 442/890 Training loss: 2.0138 1.6224 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.0132 1.6301 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.0125 1.6164 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 2.0117 1.6197 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.0114 1.6085 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.0107 1.6120 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.0102 1.6089 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.0093 1.6321 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.0086 1.6262 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.0078 1.6487 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.0073 1.6368 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.0067 1.6183 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.0060 1.6157 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.0053 1.5901 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.0045 1.6315 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.0041 1.5952 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.0038 1.6024 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.0031 1.6283 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.0026 1.6127 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.0019 1.6017 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.0015 1.6059 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.0011 1.6246 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.0007 1.5970 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.0004 1.6137 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.0000 1.6104 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 1.9995 1.6154 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 1.9990 1.6284 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 1.9986 1.5981 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 1.9980 1.6098 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 1.9973 1.6156 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 1.9966 1.6125 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 1.9961 1.6135 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 1.9956 1.5971 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 1.9952 1.5953 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 1.9948 1.6390 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 1.9944 1.6908 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 1.9937 1.6206 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 1.9931 1.6147 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 1.9928 1.6206 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 1.9923 1.6347 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 1.9916 1.6285 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 1.9913 1.6276 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 1.9909 1.6082 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 1.9904 1.6231 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 1.9900 1.6348 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 1.9893 1.6009 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 1.9887 1.7292 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 1.9883 1.6158 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 1.9880 1.6281 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 1.9876 1.6063 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 1.9873 1.5961 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 1.9869 1.6219 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 1.9866 1.6131 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 1.9865 1.6247 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 1.9860 1.6150 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 1.9857 1.6019 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 1.9852 1.6135 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 1.9848 1.6235 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 1.9844 1.6260 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 1.9839 1.6443 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 1.9836 1.6099 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 1.9832 1.5916 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 1.9830 1.6043 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 1.9826 1.6094 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 1.9821 1.6166 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 1.9815 1.6149 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 1.9813 1.6251 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 1.9810 1.6118 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 1.9807 1.6222 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 1.9803 1.6025 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 1.9799 1.6084 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 1.9794 1.6002 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 1.9790 1.6088 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 1.9784 1.6216 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 1.9783 1.6260 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 1.9780 1.6251 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 1.9776 1.6094 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 1.9773 1.6112 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 1.9769 1.6271 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 1.9765 1.6233 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 1.9761 1.5996 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 1.9758 1.5919 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 1.9757 1.5940 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 1.9754 1.6182 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 1.9750 1.6407 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 1.9745 1.6063 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 1.9741 1.6046 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 1.9739 1.6130 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 1.9736 1.6367 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 1.9732 1.6247 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 1.9728 1.6239 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 1.9723 1.6307 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 1.9720 1.6100 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 1.9757 1.6414 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 1.9346 1.6540 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 1.9225 1.6084 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 1.9141 1.6273 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 1.9096 1.6237 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 1.9007 1.6060 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 1.8999 1.6080 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 1.8984 1.6290 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 1.9005 1.5841 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 1.8996 1.6111 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 1.8960 1.6057 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 1.8933 1.6309 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 1.8935 1.6421 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 1.8951 1.6137 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 1.8942 1.5997 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 1.8927 1.6081 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 1.8920 1.6286 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 1.8940 1.6024 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 1.8942 1.6461 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 1.8944 1.6373 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 1.8939 1.6290 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 1.8947 1.6242 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 1.8940 1.5965 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 1.8931 1.6330 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 1.8929 1.6280 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 1.8917 1.6088 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 1.8904 1.5993 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 1.8904 1.7308 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 563/890 Training loss: 1.8913 1.6383 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 1.8916 1.6434 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 1.8912 1.6117 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 1.8901 1.6321 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 1.8899 1.6200 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 1.8904 1.6006 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 1.8899 1.6314 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 1.8894 1.6386 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 1.8886 1.6226 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 1.8870 1.6320 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 1.8856 1.6209 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 1.8847 1.6265 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 1.8841 1.6357 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 1.8842 1.6380 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 1.8835 1.6147 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 1.8826 1.5997 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 1.8824 1.5936 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 1.8807 1.6135 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 1.8805 1.6186 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 1.8797 1.6344 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 1.8792 1.6077 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 1.8797 1.6164 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 1.8789 1.5987 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 1.8794 1.6275 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 1.8790 1.6097 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 1.8786 1.6113 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 1.8778 1.6256 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 1.8776 1.5829 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 1.8776 1.6065 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 1.8770 1.6206 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 1.8762 1.5942 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 1.8765 1.6074 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 1.8761 1.6040 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 1.8765 1.6103 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 1.8767 1.6033 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 1.8768 1.6236 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 1.8763 1.6112 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 1.8764 1.6121 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 1.8763 1.6269 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 1.8755 1.6253 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 1.8752 1.6307 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 1.8749 1.6159 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 1.8752 1.5979 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 1.8750 1.5840 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 1.8749 1.6308 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 1.8742 1.6355 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 1.8739 1.6035 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 1.8741 1.6269 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 1.8739 1.6190 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 1.8737 1.6235 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 1.8731 1.6300 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 1.8727 1.6197 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 1.8720 1.6114 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 1.8720 1.6179 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 1.8712 1.6393 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 1.8709 1.6019 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 1.8701 1.6210 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 1.8695 1.6508 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 1.8691 1.6366 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 1.8686 1.6288 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 1.8679 1.5912 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 1.8678 1.6015 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 1.8673 1.6101 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 1.8668 1.6139 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 1.8660 1.6398 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 1.8654 1.6475 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 1.8648 1.6231 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 1.8645 1.6231 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 1.8640 1.6285 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 1.8634 1.6019 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 1.8627 1.6594 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 1.8620 1.6093 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 1.8617 1.6258 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 1.8613 1.7193 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 1.8608 1.6204 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 1.8604 1.6432 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 1.8599 1.6295 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 1.8594 1.6406 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 1.8591 1.6256 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 1.8587 1.6104 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 1.8586 1.6295 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 1.8582 1.6180 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 1.8579 1.6141 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 1.8575 1.6152 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 1.8570 1.6155 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 1.8567 1.6153 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 1.8563 1.6379 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 1.8557 1.5973 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 1.8554 1.6093 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 1.8552 1.6426 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 1.8547 1.6324 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 1.8543 1.6250 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 1.8541 1.6280 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 1.8535 1.6203 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 1.8529 1.6198 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 1.8528 1.6416 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 1.8524 1.6189 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 1.8518 1.6529 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 1.8516 1.6264 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 1.8514 1.6186 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 1.8510 1.6341 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 1.8506 1.6461 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 1.8502 1.6154 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 1.8497 1.6440 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 1.8494 1.6172 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 1.8492 1.6044 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 1.8489 1.6463 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 1.8487 1.6050 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 1.8486 1.6161 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 1.8484 1.6293 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 1.8483 1.6261 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 1.8479 1.6312 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 1.8479 1.6350 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 1.8476 1.6144 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 1.8473 1.6142 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 1.8471 1.6244 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 1.8467 1.6245 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 1.8465 1.6048 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 1.8463 1.6180 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 1.8463 1.6216 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 1.8460 1.5955 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 684/890 Training loss: 1.8456 1.6169 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 1.8451 1.6164 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 1.8450 1.6251 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 1.8448 1.6196 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 1.8446 1.6060 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 1.8444 1.6087 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 1.8442 1.5952 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 1.8440 1.6369 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 1.8436 1.6175 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 1.8431 1.6114 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 1.8431 1.6326 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 1.8430 1.6283 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 1.8427 1.6087 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 1.8425 1.6356 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 1.8423 1.6383 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 1.8420 1.6420 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 1.8417 1.6138 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 1.8416 1.6393 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 1.8416 1.6003 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 1.8413 1.6321 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 1.8411 1.6362 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 1.8407 1.6286 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 1.8404 1.6297 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 1.8403 1.6263 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 1.8400 1.6249 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 1.8399 1.7213 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 1.8396 1.6222 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 1.8392 1.6017 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 1.8391 1.6299 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 1.8601 1.5971 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 1.8243 1.5818 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 1.8158 1.6086 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 1.8067 1.5788 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 1.8039 1.6035 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 1.7938 1.6100 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 1.7943 1.5865 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 1.7911 1.5892 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 1.7937 1.5943 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 1.7918 1.5968 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 1.7893 1.6084 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 1.7875 1.6305 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 1.7876 1.5891 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 1.7900 1.6004 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 1.7887 1.5990 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 1.7862 1.6188 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 1.7859 1.6225 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 1.7883 1.5742 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 1.7886 1.6008 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 1.7891 1.6262 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 1.7885 1.6203 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 1.7893 1.6193 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 1.7887 1.6001 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 1.7881 1.5969 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 1.7879 1.6083 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 1.7865 1.5988 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 1.7855 1.6231 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 1.7859 1.6062 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 1.7864 1.6229 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 1.7866 1.6104 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 1.7862 1.5975 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 1.7852 1.6066 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 1.7850 1.5999 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 1.7852 1.5944 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 1.7845 1.6172 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 1.7842 1.6073 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 1.7835 1.6082 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 1.7819 1.6088 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 1.7806 1.5917 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 1.7799 1.5928 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 1.7791 1.6049 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 1.7795 1.6115 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 1.7792 1.6062 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 1.7783 1.5806 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 1.7786 1.6171 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 1.7773 1.6040 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 1.7772 1.5837 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 1.7764 1.6260 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 1.7762 1.6215 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 1.7770 1.6050 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 1.7765 1.6025 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 1.7772 1.6253 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 1.7768 1.5948 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 1.7766 1.6289 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 1.7761 1.6210 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 1.7760 1.5799 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 1.7761 1.6070 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 1.7757 1.5857 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 1.7749 1.5966 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 1.7754 1.6033 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 1.7750 1.6248 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 1.7755 1.6134 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 1.7758 1.6156 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 1.7759 1.6257 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 1.7756 1.6391 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 1.7757 1.6200 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 1.7755 1.5877 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 1.7749 1.5868 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 1.7748 1.5844 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 1.7745 1.6202 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 1.7747 1.7139 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 1.7748 1.6268 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 1.7750 1.6241 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 1.7746 1.6111 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 1.7742 1.6181 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 1.7742 1.6139 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 1.7740 1.6302 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 1.7738 1.6254 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 1.7732 1.5957 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 1.7728 1.6108 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 1.7722 1.6109 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 1.7721 1.6225 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 1.7714 1.6216 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 1.7711 1.6273 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 1.7705 1.5913 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 1.7700 1.6254 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 1.7696 1.5787 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 1.7693 1.6066 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 1.7685 1.6151 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 1.7685 1.6068 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 1.7681 1.6074 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 1.7677 1.5964 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 805/890 Training loss: 1.7670 1.6010 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 1.7665 1.6112 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 1.7659 1.6008 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 1.7656 1.6139 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 1.7653 1.6029 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 1.7647 1.6037 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 1.7643 1.6329 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 1.7636 1.6281 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 1.7634 1.6194 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 1.7631 1.6306 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 1.7627 1.6239 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 1.7624 1.5963 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 1.7620 1.6148 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 1.7617 1.6283 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 1.7615 1.6151 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 1.7612 1.6138 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 1.7610 1.6266 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 1.7609 1.6309 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 1.7606 1.6342 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 1.7602 1.6535 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 1.7600 1.6109 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 1.7596 1.6376 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 1.7593 1.6273 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 1.7587 1.6308 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 1.7585 1.6280 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 1.7583 1.6165 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 1.7580 1.6295 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 1.7578 1.6364 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 1.7577 1.6174 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 1.7572 1.6187 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 1.7567 1.5924 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 1.7567 1.5996 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 1.7565 1.6092 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 1.7560 1.6142 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 1.7560 1.6210 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 1.7559 1.6245 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 1.7557 1.6200 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 1.7554 1.6040 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 1.7550 1.5895 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 1.7546 1.5980 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 1.7545 1.5922 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 1.7544 1.6609 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 1.7542 1.6039 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 1.7540 1.6277 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 1.7539 1.6393 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 1.7539 1.6451 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 1.7538 1.6083 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 1.7536 1.6178 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 1.7537 1.6039 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 1.7535 1.6181 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 1.7533 1.6279 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 1.7532 1.6181 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 1.7528 1.7208 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 1.7527 1.6092 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 1.7526 1.6101 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 1.7527 1.5988 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 1.7525 1.6076 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 1.7522 1.6273 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 1.7518 1.6264 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 1.7518 1.6311 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 1.7517 1.6260 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 1.7516 1.6162 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 1.7514 1.6037 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 1.7512 1.6290 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 1.7510 1.5878 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 1.7508 1.6015 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 1.7504 1.5974 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 1.7504 1.6128 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 1.7504 1.5925 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 1.7502 1.5965 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 1.7501 1.6013 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 1.7499 1.5955 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 1.7497 1.6127 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 1.7495 1.6007 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 1.7495 1.6196 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 1.7497 1.6624 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 1.7494 1.6262 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 1.7493 1.6067 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 1.7490 1.5987 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 1.7487 1.6219 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 1.7486 1.6210 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 1.7485 1.6124 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 1.7485 1.6039 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 1.7482 1.6260 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 1.7479 1.6130 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 1.7478 1.6058 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4163 1.6575 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.4037 1.6008 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.3897 1.5909 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.3694 1.5971 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.3273 1.6131 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.2341 1.6002 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 4.1703 1.6177 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 4.1018 1.6177 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 4.0349 1.6157 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.9773 1.5974 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.9271 1.6130 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.8854 1.6086 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.8477 1.6066 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.8134 1.6004 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.7822 1.6229 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.7534 1.5972 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.7258 1.6241 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.7035 1.6098 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.6821 1.6290 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.6608 1.6086 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.6425 1.6198 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.6252 1.6349 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.6086 1.6325 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.5933 1.6317 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.5790 1.6038 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.5660 1.6992 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.5537 1.6597 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.5413 1.6135 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.5299 1.6160 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.5193 1.6017 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.5098 1.6085 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.5002 1.6173 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.4906 1.6168 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.4823 1.6071 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.4737 1.6001 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.4659 1.6324 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 37/890 Training loss: 3.4578 1.6258 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.4501 1.6277 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.4427 1.6065 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.4359 1.7375 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.4291 1.6346 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.4226 1.5984 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.4164 1.6222 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.4104 1.6233 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.4044 1.6243 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.3991 1.5948 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.3940 1.6232 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.3891 1.6313 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.3845 1.6063 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.3799 1.5994 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.3752 1.6182 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.3705 1.6130 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.3661 1.6193 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.3616 1.6043 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.3575 1.6164 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.3532 1.6107 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.3491 1.6068 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.3454 1.6230 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.3414 1.6181 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.3379 1.6175 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.3343 1.6048 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.3312 1.5991 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.3281 1.6266 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.3245 1.6154 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.3211 1.6299 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.3182 1.6297 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.3152 1.6108 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.3117 1.6106 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.3085 1.6265 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.3057 1.6093 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.3029 1.6216 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.3003 1.6125 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.2975 1.6047 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.2948 1.6235 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.2923 1.6288 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.2899 1.6131 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.2874 1.6116 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.2851 1.6278 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.2825 1.6326 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.2799 1.6289 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.2774 1.6193 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.2752 1.6428 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.2730 1.6150 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.2706 1.6047 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.2682 1.6033 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.2658 1.6133 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.2635 1.6227 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.2612 1.6240 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.2592 1.6164 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.2572 1.6244 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.2551 1.6098 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.2530 1.6312 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.2509 1.6072 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.2489 1.6136 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.2467 1.5906 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.2446 1.6059 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.2428 1.6107 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.2407 1.6215 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.2388 1.6154 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.2368 1.6320 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.2349 1.6002 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.2331 1.6197 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.2312 1.5905 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.2293 1.6300 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.2273 1.6336 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.2254 1.6390 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.2233 1.6133 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.2212 1.5969 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.2194 1.6113 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.2171 1.6166 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.2152 1.6130 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.2133 1.6246 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.2113 1.6104 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.2092 1.7293 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.2071 1.6241 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.2050 1.6352 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.2030 1.6090 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.2011 1.6181 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.1992 1.6017 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.1972 1.6185 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.1954 1.6023 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.1935 1.5933 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.1916 1.6104 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.1896 1.6182 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.1876 1.6062 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.1853 1.5939 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.1833 1.6010 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.1813 1.6184 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.1792 1.5903 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.1771 1.6267 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.1751 1.6076 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.1729 1.5855 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.1708 1.6017 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.1687 1.5984 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 3.1663 1.6174 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 3.1640 1.5960 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 3.1617 1.6346 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 3.1594 1.6257 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 3.1572 1.6142 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 3.1550 1.6183 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 3.1528 1.6461 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 3.1504 1.6383 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 3.1481 1.5987 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 3.1457 1.6065 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 3.1434 1.6110 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 3.1411 1.6267 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 3.1388 1.6263 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 3.1367 1.6132 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 3.1342 1.6157 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 3.1318 1.6033 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 3.1296 1.6125 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 3.1275 1.6049 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 3.1252 1.6137 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 3.1229 1.6049 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 3.1204 1.6137 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 3.1179 1.6102 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 3.1154 1.6134 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 3.1129 1.6164 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 159/890 Training loss: 3.1104 1.6289 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 3.1079 1.6201 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 3.1055 1.6133 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 3.1030 1.6080 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 3.1003 1.6330 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 3.0979 1.6101 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 3.0954 1.6035 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 3.0930 1.6213 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 3.0905 1.6468 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 3.0881 1.6118 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 3.0856 1.6062 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 3.0831 1.6015 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 3.0807 1.6021 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 3.0784 1.6136 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 3.0762 1.6017 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 3.0739 1.5831 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 3.0716 1.5976 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 3.0692 1.6241 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 3.0667 1.6248 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 3.0641 1.6250 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.7013 1.6217 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.6418 1.6090 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.6257 1.6142 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.6186 1.6053 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.6131 1.5927 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.6090 1.6176 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.6074 1.5959 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.6056 1.5914 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.6054 1.6030 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.6031 1.7072 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.5994 1.6175 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.5978 1.6111 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.5957 1.6118 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.5964 1.6135 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.5945 1.6251 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.5932 1.6196 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.5919 1.6168 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.5919 1.6218 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.5900 1.6120 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.5873 1.6306 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.5854 1.6157 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.5846 1.6063 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.5829 1.6108 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.5806 1.6007 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.5783 1.6155 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.5767 1.6019 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.5749 1.6190 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.5727 1.6226 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.5712 1.6192 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.5695 1.6162 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.5685 1.6207 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.5666 1.6100 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.5645 1.6391 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.5629 1.6026 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.5609 1.6186 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.5596 1.6218 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.5577 1.6204 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.5554 1.6192 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.5533 1.6115 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.5513 1.6146 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.5492 1.6047 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.5474 1.6043 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.5454 1.6082 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.5434 1.6227 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.5416 1.6074 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.5394 1.6139 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.5381 1.6198 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.5365 1.6028 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.5349 1.6107 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.5337 1.6135 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.5320 1.6134 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.5305 1.6071 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.5288 1.5958 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.5271 1.6140 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.5254 1.6133 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.5239 1.6085 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.5224 1.6153 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.5207 1.6288 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.5192 1.6250 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.5180 1.6044 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.5165 1.6276 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.5153 1.6004 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.5143 1.6192 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.5128 1.6132 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.5113 1.6082 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.5103 1.6010 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.5091 1.6288 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.5074 1.6220 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.5059 1.6186 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.5049 1.6260 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.5037 1.6194 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.5025 1.5964 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.5013 1.6432 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.4999 1.6003 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.4986 1.6227 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.4979 1.6066 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.4966 1.5998 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.4957 1.6178 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.4943 1.6324 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.4930 1.6340 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.4916 1.6190 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.4907 1.5932 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.4894 1.5960 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.4880 1.7117 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.4864 1.6349 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.4850 1.6379 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.4839 1.5992 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.4827 1.6247 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.4814 1.6066 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.4804 1.6178 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.4792 1.6273 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.4781 1.6095 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.4769 1.6248 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.4756 1.6321 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.4743 1.6395 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.4731 1.6360 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.4720 1.6421 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.4709 1.6092 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.4697 1.6092 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.4686 1.6204 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.4676 1.6117 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 280/890 Training loss: 2.4665 1.6027 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.4653 1.5973 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.4641 1.6136 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.4630 1.6473 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.4620 1.6321 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.4609 1.6329 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.4601 1.6294 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.4591 1.6353 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.4579 1.6143 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.4569 1.6327 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.4560 1.6023 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.4550 1.6014 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.4539 1.6195 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.4529 1.5939 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.4515 1.6286 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.4506 1.6068 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.4496 1.6120 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.4489 1.6085 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.4480 1.6150 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.4473 1.6163 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.4463 1.6161 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.4453 1.6165 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.4445 1.6247 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.4435 1.6256 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.4425 1.6320 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.4416 1.6005 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.4408 1.6113 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.4399 1.6329 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.4391 1.5929 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.4382 1.6109 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.4371 1.6204 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.4363 1.5953 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.4356 1.6046 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.4346 1.5919 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.4338 1.6445 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.4330 1.6053 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.4321 1.5992 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.4315 1.6346 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.4307 1.5871 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.4306 1.6257 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.4301 1.6171 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.4298 1.6323 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.4294 1.6154 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.4289 1.6010 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.4286 1.6037 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.4281 1.6722 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.4275 1.6061 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.4266 1.6015 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.4258 1.5983 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.4252 1.6189 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.4247 1.6010 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.4241 1.6348 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.4234 1.6102 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.4226 1.6102 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.4218 1.5978 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.4211 1.6171 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.4203 1.7375 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.4194 1.6137 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.4188 1.6064 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.4181 1.5923 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.4173 1.6121 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.4164 1.6022 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.4156 1.5961 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.4150 1.6079 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.4142 1.6132 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.4135 1.5944 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.4128 1.6092 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.4121 1.6105 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.4113 1.6068 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.4105 1.6094 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.4098 1.6094 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.4092 1.6028 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.4086 1.6264 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.4079 1.6144 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.4073 1.6138 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.4064 1.6216 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.4057 1.6220 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.3566 1.6101 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.2969 1.6339 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.2807 1.6114 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.2746 1.6305 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.2703 1.5957 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.2665 1.6126 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.2672 1.6456 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.2675 1.6201 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.2686 1.6410 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.2684 1.6500 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.2662 1.6252 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.2649 1.6229 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.2645 1.6085 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.2664 1.6276 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.2658 1.6146 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.2651 1.6056 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.2646 1.6122 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.2660 1.6011 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.2658 1.6215 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.2646 1.6424 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.2641 1.6410 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.2654 1.6109 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.2643 1.6378 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.2633 1.5899 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.2626 1.6299 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.2620 1.6285 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.2614 1.6398 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.2612 1.6225 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.2617 1.6211 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.2616 1.6296 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.2614 1.6288 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.2606 1.6158 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.2598 1.6206 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.2597 1.6157 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.2588 1.5995 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.2584 1.6146 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.2577 1.6308 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.2563 1.6112 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.2555 1.6360 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.2545 1.6427 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.2536 1.5997 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.2530 1.6194 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.2522 1.6191 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.2513 1.6181 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 401/890 Training loss: 2.2508 1.6177 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.2491 1.6311 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.2490 1.6084 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 2.2482 1.6172 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.2476 1.5978 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.2476 1.6154 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.2467 1.6130 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.2467 1.6260 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.2462 1.6982 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.2455 1.6616 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.2449 1.6236 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.2446 1.6286 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.2442 1.6308 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.2436 1.6504 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.2428 1.6276 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.2427 1.5955 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.2420 1.6267 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.2419 1.6312 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.2418 1.6037 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.2414 1.6162 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.2409 1.6454 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.2407 1.6212 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.2403 1.6148 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.2395 1.6083 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.2388 1.5940 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.2385 1.6011 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.2383 1.6009 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.2381 1.5968 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.2379 1.6247 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.2373 1.6043 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.2369 1.6035 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.2371 1.6230 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.2365 1.6282 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.2362 1.6516 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.2355 1.6266 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.2349 1.6365 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.2343 1.6299 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.2340 1.6429 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 2.2334 1.6259 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.2328 1.6047 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.2318 1.6309 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 2.2313 1.6214 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.2309 1.6167 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.2304 1.6238 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 2.2297 1.5861 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.2294 1.6211 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.2288 1.5975 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.2284 1.6266 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.2277 1.5846 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.2271 1.6194 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.2265 1.6371 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.2259 1.6144 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.2254 1.6343 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.2249 1.6247 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.2242 1.5994 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.2236 1.6071 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.2233 1.5993 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.2229 1.6183 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.2223 1.6225 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.2217 1.6163 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.2212 1.6123 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.2207 1.6142 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.2203 1.6188 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.2200 1.6066 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.2197 1.6230 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.2192 1.6164 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 2.2189 1.6205 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 2.2186 1.6268 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 2.2181 1.6215 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 2.2176 1.6064 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 2.2172 1.6132 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 2.2164 1.6000 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 2.2160 1.6202 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 2.2156 1.6072 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 2.2154 1.6342 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 2.2150 1.6284 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 2.2147 1.6145 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 2.2142 1.6080 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 2.2137 1.6251 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 2.2136 1.6294 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 2.2131 1.6211 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 2.2125 1.6206 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 2.2123 1.7305 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 2.2120 1.6169 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 2.2116 1.6365 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 2.2113 1.6177 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 2.2109 1.6149 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 2.2102 1.6219 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 2.2099 1.6063 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 2.2097 1.6178 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 2.2093 1.6348 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 2.2091 1.6263 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 2.2088 1.6215 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 2.2085 1.6516 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 2.2085 1.6111 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 2.2082 1.6029 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 2.2080 1.5975 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 2.2076 1.6309 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 2.2073 1.6334 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 2.2069 1.6382 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 2.2066 1.6294 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 2.2064 1.6186 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 2.2061 1.6049 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 2.2059 1.6282 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 2.2056 1.6171 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 2.2051 1.6305 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 2.2048 1.6194 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 2.2048 1.6194 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 2.2045 1.6223 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 2.2043 1.6157 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 2.2039 1.6080 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 2.2035 1.6078 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 2.2031 1.6076 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 2.2028 1.6281 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 2.2023 1.6239 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 2.2022 1.6216 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 2.2020 1.6295 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 2.2016 1.6256 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 2.2013 1.6438 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 2.2010 1.6143 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 2.2007 1.6237 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 522/890 Training loss: 2.2004 1.6025 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 2.2001 1.6165 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 2.1999 1.5991 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 2.1997 1.6021 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 2.1993 1.6010 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 2.1989 1.6401 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 2.1985 1.6093 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 2.1984 1.5874 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 2.1981 1.5815 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 2.1979 1.6107 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 2.1976 1.5987 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 2.1971 1.6084 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 2.1968 1.6266 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 2.2161 1.6266 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 2.1626 1.6087 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 2.1487 1.6242 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 2.1426 1.6107 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 2.1404 1.6100 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 2.1342 1.6326 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 2.1344 1.6051 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 2.1350 1.6055 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 2.1367 1.6047 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 2.1356 1.6037 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 2.1329 1.6257 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 2.1313 2.6579 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 2.1311 1.6392 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 2.1332 1.6095 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 2.1326 1.6313 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 2.1317 1.6208 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 2.1311 1.6042 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 2.1332 1.5897 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 2.1333 1.6121 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 2.1333 1.6008 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 2.1325 1.6030 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 2.1340 1.7090 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 2.1334 1.6326 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 2.1326 1.6097 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 2.1322 1.6421 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 2.1310 1.6012 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 2.1301 1.6104 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 2.1301 1.6173 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 2.1309 1.6035 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 2.1305 1.6330 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 2.1302 1.6076 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 2.1295 1.6139 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 2.1290 1.6205 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 2.1295 1.6215 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 2.1288 1.6235 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 2.1284 1.6139 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 2.1279 1.6118 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 2.1264 1.6032 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 2.1254 1.6168 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 2.1246 1.6354 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 2.1239 1.6274 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 2.1236 1.6131 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 2.1230 1.6034 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 2.1222 1.6126 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 2.1219 1.6226 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 2.1206 1.5963 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 2.1205 1.6263 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 2.1198 1.6172 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 2.1195 1.6112 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 2.1198 1.6013 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 2.1191 1.6459 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 2.1195 1.6162 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 2.1191 1.6231 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 2.1186 1.6078 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 2.1181 1.6184 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 2.1181 1.5942 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 2.1180 1.5995 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 2.1173 1.6097 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 2.1167 1.6166 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 2.1170 1.6266 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 2.1168 1.6035 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 2.1169 1.6205 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 2.1170 1.6371 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 2.1169 1.6138 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 2.1164 1.6186 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 2.1165 1.6435 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 2.1162 1.6058 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 2.1156 1.6327 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 2.1151 1.6148 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 2.1148 1.6241 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 2.1150 1.6047 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 2.1148 1.6143 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 2.1147 1.6154 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 2.1142 1.6145 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 2.1139 1.5927 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 2.1142 1.6101 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 2.1138 1.6107 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 2.1137 1.6092 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 2.1131 1.5996 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 2.1127 1.6079 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 2.1121 1.6390 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 2.1120 1.6235 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 2.1114 1.6180 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 2.1110 1.6192 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 2.1101 1.6180 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 2.1096 1.6351 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 2.1094 1.6294 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 2.1089 1.6283 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 2.1084 1.6196 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 2.1082 1.6346 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 2.1078 1.6332 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 2.1076 1.6107 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 2.1069 1.6372 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 2.1064 1.6142 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 2.1059 1.6208 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 2.1055 1.7410 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 2.1051 1.6117 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 2.1046 1.6061 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 2.1039 1.6207 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 2.1033 1.6272 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 2.1031 1.5983 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 2.1028 1.6167 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 2.1023 1.6146 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 2.1019 1.6086 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 2.1014 1.6346 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 2.1010 1.6104 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 2.1007 1.6183 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 2.1005 1.6245 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 643/890 Training loss: 2.1003 1.6414 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 2.1000 1.6084 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 2.0997 1.6190 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 2.0995 1.6487 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 2.0991 1.6196 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 2.0987 1.6256 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 2.0983 1.6213 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 2.0976 1.6333 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 2.0974 1.6183 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 2.0971 1.6189 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 2.0970 1.6306 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 2.0966 1.6246 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 2.0965 1.6257 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 2.0961 1.6320 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 2.0957 1.6367 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 2.0956 1.6113 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 2.0953 1.6248 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 2.0947 1.6212 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 2.0946 1.6208 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 2.0944 1.6416 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 2.0942 1.6493 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 2.0940 1.6197 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 2.0935 1.6371 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 2.0930 1.6408 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 2.0928 1.6257 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 2.0926 1.6388 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 2.0923 1.6107 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 2.0921 1.6179 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 2.0919 1.6209 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 2.0918 1.6192 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 2.0918 1.6187 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 2.0914 1.6380 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 2.0913 1.5934 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 2.0910 1.6045 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 2.0908 1.6119 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 2.0906 1.6349 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 2.0903 1.6289 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 2.0902 1.6234 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 2.0900 1.6213 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 2.0900 1.6189 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 2.0897 1.6183 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 2.0894 1.6336 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 2.0891 1.6085 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 2.0890 1.6085 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 2.0889 1.6238 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 2.0887 1.6602 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 2.0884 1.6199 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 2.0881 1.6237 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 2.0879 1.6186 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 2.0876 1.6194 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 2.0872 1.6638 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 2.0872 1.5926 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 2.0871 1.6021 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 2.0869 1.6115 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 2.0867 1.6020 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 2.0865 1.6194 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 2.0863 1.6189 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 2.0860 1.6046 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 2.0859 1.6391 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 2.0859 1.5836 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 2.0856 1.6060 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 2.0853 1.6981 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 2.0850 1.6124 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 2.0848 1.6191 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 2.0846 1.6186 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 2.0845 1.6081 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 2.0843 1.6006 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 2.0841 1.6102 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 2.0837 1.6109 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 2.0835 1.6130 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 2.1139 1.6178 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 2.0646 1.6143 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 2.0519 1.6105 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 2.0446 1.5975 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 2.0411 1.6080 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 2.0347 1.6081 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 2.0356 1.5961 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 2.0352 1.6179 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 2.0374 1.5930 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 2.0374 1.6410 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 2.0351 1.6282 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 2.0333 1.5962 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 2.0336 1.6083 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 2.0356 1.6051 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 2.0347 1.5925 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 2.0328 1.5863 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 2.0322 1.6033 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 2.0340 1.6223 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 2.0339 1.6031 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 2.0334 1.6388 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 2.0329 1.6010 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 2.0340 1.6117 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 2.0332 1.5988 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 2.0324 1.6239 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 2.0318 1.6081 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 2.0306 1.6130 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 2.0296 1.6379 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 2.0296 1.6085 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 2.0301 1.6316 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 2.0300 1.6242 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 2.0296 1.6223 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 2.0287 1.6194 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 2.0286 1.6119 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 2.0293 1.6277 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 2.0289 1.6012 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 2.0285 1.6263 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 2.0281 1.6099 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 2.0267 1.6181 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 2.0254 1.6170 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 2.0246 1.6113 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 2.0241 1.6157 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 2.0240 1.6233 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 2.0234 1.6134 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 2.0226 1.6137 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 2.0225 1.6157 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 2.0210 1.6192 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 2.0209 1.5785 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 2.0203 1.6003 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 2.0201 1.6072 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 2.0206 1.6183 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 2.0200 1.6015 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 764/890 Training loss: 2.0204 1.6041 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 2.0201 1.6265 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 2.0198 1.6051 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 2.0194 1.6321 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 2.0196 1.5916 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 2.0195 1.6427 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 2.0191 1.6142 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 2.0185 1.6153 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 2.0190 1.6079 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 2.0185 1.6334 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 2.0190 1.6332 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 2.0193 1.6461 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 2.0193 1.5978 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 2.0190 1.6037 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 2.0192 1.7384 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 2.0190 1.6367 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 2.0184 1.6437 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 2.0181 1.6143 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 2.0179 1.6075 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 2.0183 1.6179 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 2.0181 1.6156 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 2.0183 1.6019 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 2.0175 1.6316 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 2.0172 1.6143 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 2.0174 1.6377 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 2.0172 1.6168 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 2.0171 1.6081 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 2.0165 1.6157 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 2.0162 1.6051 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 2.0157 1.6069 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 2.0155 1.6396 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 2.0148 1.6030 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 2.0145 1.6086 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 2.0139 1.6037 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 2.0134 1.6176 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 2.0132 1.6199 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 2.0127 1.6482 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 2.0121 1.6190 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 2.0120 1.6134 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 2.0116 1.6317 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 2.0113 1.6070 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 2.0107 1.6067 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 2.0104 1.6069 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 2.0099 1.6200 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 2.0096 1.6243 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 2.0092 1.6302 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 2.0087 1.6255 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 2.0081 1.6399 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 2.0074 1.6302 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 2.0073 1.6123 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 2.0071 1.6158 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 2.0066 1.6161 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 2.0063 1.6089 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 2.0059 1.6094 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 2.0056 1.6010 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 2.0053 1.6285 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 2.0051 1.6003 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 2.0049 1.6275 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 2.0047 1.6114 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 2.0045 1.6147 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 2.0043 1.6064 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 2.0040 1.6140 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 2.0036 1.6124 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 2.0032 1.6035 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 2.0027 1.6276 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 2.0025 1.6418 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 2.0022 1.6192 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 2.0021 1.6038 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 2.0018 1.6067 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 2.0017 1.6037 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 2.0012 1.5992 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 2.0008 1.6360 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 2.0007 1.6091 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 2.0005 1.6343 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 2.0000 1.6099 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 1.9999 1.6030 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 1.9998 1.6226 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 1.9996 1.6392 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 1.9994 1.6053 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 1.9990 1.6104 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 1.9986 1.5994 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 1.9985 1.6115 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 1.9984 1.6270 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 1.9982 1.6307 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 1.9981 1.6200 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 1.9980 1.6377 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 1.9979 1.6533 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 1.9979 1.7495 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 1.9976 1.6362 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 1.9976 1.6301 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 1.9974 1.6281 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 1.9973 1.6479 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 1.9972 1.6308 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 1.9969 1.6093 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 1.9969 1.6214 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 1.9968 1.6102 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 1.9969 1.6215 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 1.9968 1.6142 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 1.9965 1.6159 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 1.9962 1.6163 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 1.9962 1.6442 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 1.9962 1.6219 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 1.9961 1.6259 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 1.9959 1.6150 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 1.9957 1.6345 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 1.9955 1.6168 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 1.9953 1.6158 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 1.9950 1.5973 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 1.9951 1.5940 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 1.9950 1.6159 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 1.9948 1.6151 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 1.9946 1.6120 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 1.9944 1.6126 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 1.9943 1.6218 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 1.9940 1.6041 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 1.9939 1.6337 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 1.9940 1.6018 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 1.9938 1.5847 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 1.9936 1.5960 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 1.9933 1.6132 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 1.9930 1.6167 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 885/890 Training loss: 1.9929 1.6105 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 1.9928 1.6202 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 1.9927 1.6264 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 1.9925 1.6123 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 1.9922 1.6513 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 1.9921 1.6163 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4199 3.5055 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.3884 3.4141 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.2491 3.4756 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.1641 3.4606 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.0390 3.4696 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 3.9446 3.4700 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 3.8771 3.5003 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 3.8205 3.5014 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 3.7699 3.4503 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.7257 3.4102 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.6859 3.4231 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.6515 3.4413 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.6215 3.4731 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.5971 3.4507 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.5741 3.4367 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.5537 3.5455 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.5341 3.4489 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.5187 3.4824 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.5037 3.4718 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.4883 3.4390 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.4749 3.4726 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.4624 3.4958 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.4510 3.4706 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.4401 3.4521 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.4299 3.4492 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.4210 3.4417 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.4128 3.4615 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.4040 3.4410 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.3963 3.4519 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.3891 3.4471 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.3831 3.4658 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.3766 3.4348 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.3700 3.4447 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.3646 3.4314 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.3587 3.4496 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.3537 3.4119 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.3479 3.4422 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.3429 3.4232 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.3377 3.4520 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.3329 3.4653 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.3283 3.4529 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.3242 3.4051 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.3199 3.4281 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.3158 3.4351 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.3117 3.4196 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.3081 3.4164 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.3048 3.4278 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.3016 3.4497 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.2985 3.4570 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.2955 3.4684 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.2925 3.5412 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.2895 3.4603 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.2868 3.4534 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.2838 3.4393 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.2812 3.4248 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.2782 3.4481 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.2756 3.4408 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.2730 3.4398 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.2704 3.4259 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.2680 3.4322 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.2657 3.4305 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.2637 3.4196 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.2618 3.4624 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.2594 3.4486 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.2571 3.4288 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.2552 3.4481 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.2533 3.4526 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.2507 3.4169 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.2484 3.4673 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.2465 3.4425 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.2444 3.4260 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.2426 3.4569 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.2405 3.4455 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.2385 3.4463 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.2366 3.4352 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.2349 3.4739 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.2331 3.4216 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.2311 3.4413 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.2291 3.4448 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.2269 3.5150 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.2248 3.4689 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.2227 3.4353 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.2207 3.4327 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.2186 3.4797 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.2162 3.5019 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.2140 3.5103 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.2115 3.4449 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.2091 3.4609 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.2069 3.4069 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.2046 3.4296 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.2021 3.4033 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.1994 3.4224 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.1967 3.4843 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.1941 3.4569 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.1919 3.4344 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.1893 3.4252 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.1869 3.4808 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.1841 3.4283 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.1815 3.4462 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.1789 3.4914 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.1761 3.4472 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.1734 3.4671 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.1705 3.4762 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.1675 3.4866 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.1647 3.4662 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.1617 3.4601 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.1585 3.4839 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.1554 3.4701 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.1525 3.4338 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.1491 3.4740 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.1459 3.4433 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.1429 3.4040 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.1395 3.4341 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.1361 3.4376 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.1327 3.4535 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.1292 3.4134 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.1257 3.4640 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 118/890 Training loss: 3.1225 3.4405 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.1193 3.4176 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.1160 3.5241 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.1127 3.4499 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.1094 3.4536 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.1060 3.4205 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.1027 3.4874 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.0993 3.4979 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.0956 3.4514 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.0922 3.4565 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.0889 3.4702 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.0853 3.4795 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.0817 3.4837 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.0782 3.4635 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.0746 3.4511 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.0711 3.4382 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.0676 3.4710 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 3.0639 3.4685 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 3.0603 3.4661 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 3.0567 3.4656 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 3.0531 3.4692 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 3.0496 3.4981 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 3.0460 3.4751 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 3.0427 3.4725 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 3.0390 3.4906 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 3.0355 3.4606 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 3.0319 3.4535 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 3.0284 3.4698 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 3.0252 3.4415 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 3.0218 3.4881 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 3.0186 3.4602 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 3.0151 3.4605 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 3.0116 3.5208 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 3.0084 3.4822 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 3.0053 3.4390 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 3.0020 3.5032 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 2.9989 3.5139 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 2.9955 3.6032 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 2.9922 3.4932 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 2.9889 3.4622 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 2.9856 3.4932 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 2.9822 3.4652 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 2.9791 3.4321 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 2.9759 3.4464 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 2.9726 3.4712 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 2.9693 3.4639 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 2.9661 3.4777 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 2.9630 3.4498 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 2.9600 3.4525 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 2.9569 3.4346 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 2.9539 3.4588 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 2.9509 3.4465 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 2.9477 3.4732 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 2.9447 3.4534 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 2.9419 3.4452 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 2.9392 3.4845 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 2.9365 3.4461 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 2.9338 3.4404 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 2.9308 3.4588 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 2.9279 3.4288 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 2.9248 3.4625 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.4545 3.4769 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.4076 3.4844 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.3969 3.4819 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.3933 3.4752 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.3915 3.5004 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.3885 3.4932 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.3878 3.4932 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.3886 3.4684 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.3888 3.4720 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.3876 3.4442 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.3849 3.5828 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.3840 3.4694 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.3828 3.4795 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.3842 3.4936 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.3824 3.4751 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.3813 3.4827 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.3800 3.4871 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.3809 3.4873 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.3800 3.4804 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.3775 3.4713 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.3754 3.4552 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.3750 3.4896 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.3734 3.4699 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.3710 3.4475 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.3693 3.4669 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.3679 3.4781 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.3660 3.4961 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.3646 3.4639 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.3638 3.4791 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.3625 3.4570 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.3617 3.4306 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.3601 3.4408 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.3584 3.4686 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.3576 3.4297 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.3560 3.4411 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.3547 3.4574 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.3531 3.4508 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.3507 3.4392 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.3487 3.4603 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.3466 3.4566 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.3449 3.4819 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.3434 3.4674 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.3413 3.4637 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.3394 3.4659 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.3377 3.4790 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.3354 3.5413 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.3343 3.4806 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.3327 3.4465 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.3313 3.4450 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.3305 3.4453 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.3288 3.4853 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.3279 3.4934 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.3264 3.4593 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.3249 3.4576 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.3233 3.4515 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.3222 3.4256 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.3209 3.4535 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.3194 3.4636 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.3179 3.4663 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.3171 3.4251 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 239/890 Training loss: 2.3156 3.4821 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.3145 3.4679 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.3135 3.4744 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.3122 3.4663 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.3107 3.4577 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.3097 3.4543 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.3085 3.4712 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.3069 3.4510 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.3052 3.4669 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.3041 3.4290 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.3030 3.4434 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.3020 3.4725 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.3009 3.4619 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.2994 3.4532 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.2981 3.4893 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.2974 3.4583 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.2960 3.5029 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.2951 3.4815 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.2936 3.4876 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.2922 3.5465 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.2907 3.4697 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.2896 3.4607 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.2880 3.4642 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.2866 3.4559 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.2850 3.4918 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.2836 3.4652 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.2825 3.4918 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.2811 3.4691 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.2796 3.4443 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.2786 3.4583 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.2772 3.4652 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.2760 3.4847 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.2746 3.4504 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.2731 3.5010 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.2717 3.4822 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.2704 3.4812 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.2691 3.4490 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.2678 3.4582 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.2663 3.4879 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.2649 3.4559 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.2639 3.4499 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.2627 3.4625 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.2612 3.4589 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.2599 3.4843 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.2586 3.4836 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.2574 3.4782 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.2561 3.4693 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.2550 3.4677 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.2539 3.4914 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.2527 3.5038 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.2516 3.4572 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.2506 3.4813 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.2493 3.4782 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.2482 3.4413 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.2469 3.5870 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.2455 3.4726 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.2444 3.5075 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.2434 3.4594 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.2424 3.4757 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.2414 3.4712 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.2404 3.4895 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.2392 3.4492 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.2380 3.4520 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.2370 3.4367 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.2359 3.4215 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.2346 3.4244 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.2337 3.4431 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.2328 3.4477 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.2318 3.4436 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.2309 3.4573 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.2298 3.4714 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.2285 3.4429 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.2276 3.4731 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.2267 3.4866 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.2256 3.4481 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.2247 3.4821 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.2237 3.4851 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.2227 3.4478 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.2220 3.4862 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.2209 3.4495 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.2201 3.4589 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.2190 3.4484 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.2180 3.4732 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.2170 3.4223 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.2160 3.4909 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.2152 3.4969 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.2144 3.4963 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.2136 3.4917 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.2126 3.5199 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.2115 3.5207 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.2107 3.4946 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.2099 3.4789 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.2090 3.4605 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.2082 3.4808 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.2072 3.4377 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.2062 3.4513 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.2054 3.4590 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.2044 3.4671 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.2033 3.4835 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.2026 3.4619 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.2018 3.4593 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.2008 3.4866 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.1999 3.5204 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.1990 3.4671 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.1981 3.4777 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.1972 3.4540 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.1963 3.4665 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.1956 4.6371 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.1947 3.4447 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.1937 3.4490 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.1928 3.5087 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.1918 3.4746 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.1911 3.4567 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.1904 3.4556 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.1896 3.4787 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.1887 3.4562 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.1878 3.4564 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.1869 3.4574 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.0928 3.4583 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.0442 3.4481 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.0331 3.4755 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 360/890 Training loss: 2.0283 3.4525 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.0257 3.6025 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.0197 3.5139 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.0204 3.4590 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.0202 3.5112 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.0219 3.4472 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.0206 3.4476 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.0164 3.4634 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.0143 3.4368 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.0142 3.4897 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.0162 3.4660 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.0152 3.4607 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.0134 3.4660 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.0123 3.4991 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.0141 3.4551 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.0131 3.4408 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.0124 3.4498 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.0111 3.4371 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.0124 3.4397 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.0112 3.4397 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.0101 3.4448 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.0095 3.4542 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.0078 3.4642 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.0064 3.4514 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.0061 3.4731 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.0066 3.4760 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.0063 3.4892 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.0058 3.4689 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.0045 3.4769 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.0037 3.4912 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.0037 3.4477 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.0024 3.4901 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.0017 3.4579 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.0007 3.4568 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 1.9991 3.4631 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 1.9974 3.4803 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 1.9960 3.5818 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 1.9948 3.4565 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 1.9941 3.4768 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 1.9931 3.4360 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 1.9916 3.4663 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 1.9911 3.4659 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 1.9893 3.4204 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 1.9886 3.4505 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 1.9875 3.4375 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 1.9868 3.4417 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 1.9870 3.4555 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 1.9860 3.4077 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 1.9861 3.4677 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 1.9853 3.4480 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 1.9845 3.4624 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 1.9836 3.4209 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 1.9831 3.4576 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 1.9826 3.4638 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 1.9818 3.4555 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 1.9809 3.4705 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 1.9809 3.4637 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 1.9802 3.4735 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 1.9803 3.4412 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 1.9800 3.4577 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 1.9795 3.4637 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 1.9788 3.4375 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 1.9787 3.4285 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 1.9783 3.4187 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 1.9775 3.4691 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 1.9769 3.4859 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 1.9762 3.4858 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 1.9760 3.4423 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 1.9757 3.4746 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 1.9754 3.4577 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 1.9746 3.4912 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 1.9740 3.5808 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 1.9738 3.4557 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 1.9731 3.4400 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 1.9729 3.4123 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 1.9719 3.4469 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 1.9713 3.4844 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 1.9702 3.4602 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 1.9699 3.4628 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 1.9688 3.4661 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 1.9682 3.4630 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 1.9672 3.4751 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 1.9664 3.4495 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 1.9658 3.4524 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 1.9651 3.4386 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 1.9642 3.4739 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 1.9637 3.4298 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 1.9629 3.4689 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 1.9623 3.4428 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 1.9613 3.4154 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 1.9604 3.4767 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 1.9596 3.4271 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 1.9589 3.4466 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 1.9583 3.4735 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 1.9575 3.4575 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 1.9566 3.4637 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 1.9556 3.4508 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 1.9551 3.4698 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 1.9546 3.5098 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 1.9538 3.4488 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 1.9532 3.4666 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 1.9524 3.4447 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 1.9519 3.4795 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 1.9513 3.4475 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 1.9508 3.4533 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 1.9504 3.5523 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 1.9499 3.4702 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 1.9494 3.4208 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 1.9488 3.4435 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 1.9481 3.4657 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 1.9475 3.4512 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 1.9468 3.4668 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 1.9458 3.4984 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 1.9453 3.4716 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 1.9446 3.4583 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 1.9441 3.4759 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 1.9435 3.4882 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 1.9430 3.4268 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 1.9421 3.4256 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 1.9414 3.4436 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 1.9410 3.4464 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 481/890 Training loss: 1.9405 3.4560 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 1.9398 3.4361 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 1.9394 3.4790 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 1.9389 3.4014 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 1.9383 3.4141 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 1.9378 3.4489 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 1.9371 3.4249 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 1.9363 3.4229 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 1.9359 3.4471 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 1.9355 3.4540 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 1.9350 3.4460 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 1.9346 3.4302 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 1.9342 3.4226 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 1.9337 3.4768 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 1.9334 3.4366 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 1.9328 3.4494 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 1.9326 3.4661 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 1.9321 3.4565 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 1.9316 3.4290 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 1.9311 3.5678 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 1.9306 3.4595 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 1.9301 3.4582 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 1.9297 3.4521 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 1.9295 3.4695 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 1.9290 3.4402 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 1.9284 3.4915 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 1.9279 3.4765 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 1.9276 3.4355 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 1.9272 3.4525 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 1.9267 3.4601 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 1.9262 3.4251 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 1.9257 3.4425 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 1.9253 3.4668 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 1.9248 3.4612 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 1.9242 3.4543 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 1.9239 3.4856 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 1.9236 3.4478 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 1.9231 3.4888 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 1.9227 3.4418 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 1.9222 3.4446 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 1.9218 3.4754 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 1.9213 3.4724 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 1.9209 3.4234 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 1.9209 3.4403 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 1.9204 3.4445 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 1.9199 3.4336 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 1.9193 3.4162 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 1.9188 3.4322 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 1.9184 3.4549 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 1.9181 3.4386 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 1.9177 3.4124 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 1.9173 3.4531 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 1.9167 3.4306 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 1.9164 3.5563 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 1.9120 3.4506 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 1.8688 3.4781 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 1.8500 3.4251 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 1.8442 3.4594 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 1.8394 3.4348 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 1.8314 3.4432 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 1.8314 3.4674 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 1.8305 3.4383 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 1.8319 3.4133 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 1.8309 3.4536 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 1.8279 3.4353 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 1.8257 3.4129 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 1.8250 3.4397 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 1.8276 3.4534 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 1.8272 3.4506 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 1.8252 3.4583 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 1.8245 3.4504 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 1.8263 3.4817 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 1.8261 3.4274 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 1.8262 3.4368 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 1.8252 3.4567 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 1.8267 3.4577 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 1.8254 3.4573 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 1.8244 3.4791 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 1.8239 3.4512 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 1.8226 3.4611 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 1.8214 3.4615 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 1.8214 3.4726 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 1.8222 3.4667 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 1.8224 3.4217 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 1.8222 3.4770 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 1.8211 3.4554 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 1.8209 3.4544 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 1.8212 3.4561 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 1.8205 3.5344 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 1.8199 3.4612 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 1.8191 3.4336 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 1.8175 3.4943 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 1.8160 3.4776 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 1.8148 3.4434 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 1.8138 3.4560 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 1.8139 3.4754 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 1.8129 3.4906 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 1.8118 3.4640 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 1.8116 3.4729 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 1.8103 3.4540 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 1.8097 3.4422 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 1.8089 3.4807 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 1.8084 3.4683 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 1.8088 3.4496 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 1.8078 3.4207 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 1.8083 3.4399 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 1.8078 3.4410 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 1.8075 3.4326 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 1.8070 3.4450 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 1.8068 3.4510 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 1.8067 3.4766 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 1.8059 3.4471 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 1.8053 3.4270 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 1.8053 3.4668 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 1.8049 3.4305 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 1.8054 3.4512 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 1.8055 3.4851 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 1.8055 3.4519 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 1.8052 3.5006 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 1.8052 3.4673 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 1.8051 3.4744 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 602/890 Training loss: 1.8046 3.4833 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 1.8043 3.4704 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 1.8040 3.5504 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 1.8040 3.4399 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 1.8039 3.4375 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 1.8041 3.4286 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 1.8036 3.4377 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 1.8032 3.4990 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 1.8032 3.4469 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 1.8028 3.4833 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 1.8026 3.4671 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 1.8019 3.4895 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 1.8015 3.4421 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 1.8007 3.4658 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 1.8005 3.4281 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 1.7998 3.4465 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 1.7995 3.4572 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 1.7988 3.4367 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 1.7982 3.4568 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 1.7978 3.4616 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 1.7972 3.4589 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 1.7966 3.4944 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 1.7964 3.4857 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 1.7957 3.4459 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 1.7953 3.4708 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 1.7946 3.4491 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 1.7941 3.4490 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 1.7935 3.4905 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 1.7932 3.4535 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 1.7927 3.4625 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 1.7920 3.4508 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 1.7914 3.4622 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 1.7907 3.4685 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 1.7904 3.4620 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 1.7901 3.4818 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 1.7895 3.4982 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 1.7891 3.5482 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 1.7886 3.4689 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 1.7882 3.4675 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 1.7878 3.4504 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 1.7875 3.4890 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 1.7872 3.4474 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 1.7870 3.4927 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 1.7867 3.4699 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 1.7862 3.4793 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 1.7858 3.4681 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 1.7854 3.4515 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 1.7849 3.4590 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 1.7843 3.4777 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 1.7839 3.4666 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 1.7836 3.4334 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 1.7833 3.4832 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 1.7829 3.4413 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 1.7827 3.4412 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 1.7820 3.4638 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 1.7814 3.4577 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 1.7812 3.4773 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 1.7809 3.4378 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 1.7804 3.4537 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 1.7802 3.4533 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 1.7800 3.4368 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 1.7796 3.4468 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 1.7792 3.4875 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 1.7786 3.4431 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 1.7781 3.4736 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 1.7779 3.4385 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 1.7777 3.4552 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 1.7774 3.4785 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 1.7771 3.4361 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 1.7770 3.4373 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 1.7768 3.4805 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 1.7766 3.5476 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 1.7763 3.4709 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 1.7763 3.4944 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 1.7759 3.4511 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 1.7757 3.4347 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 1.7754 3.4428 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 1.7750 3.4988 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 1.7748 3.4944 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 1.7746 3.4708 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 1.7744 3.4674 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 1.7742 3.4735 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 1.7738 3.4671 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 1.7734 3.4390 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 1.7733 3.4497 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 1.7730 3.4570 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 1.7728 3.4879 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 1.7726 3.4475 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 1.7723 3.4742 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 1.7721 3.4458 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 1.7718 3.4562 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 1.7713 3.4508 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 1.7712 3.4439 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 1.7711 3.4511 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 1.7708 3.4375 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 1.7706 3.4569 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 1.7704 3.4726 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 1.7700 3.4458 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 1.7697 3.4567 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 1.7695 3.4380 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 1.7696 3.4773 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 1.7693 3.4370 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 1.7690 3.4661 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 1.7686 3.4882 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 1.7682 3.4683 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 1.7681 3.5656 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 1.7679 3.5027 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 1.7677 3.5233 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 1.7674 3.4466 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 1.7670 3.4448 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 1.7669 3.4566 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 1.7918 3.4537 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 1.7542 3.4737 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 1.7404 3.4216 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 1.7338 3.4682 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 1.7284 3.4834 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 1.7172 3.4669 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 1.7184 3.4855 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 1.7162 3.4570 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 1.7187 3.4577 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 1.7179 3.4824 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 723/890 Training loss: 1.7150 3.4530 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 1.7141 3.4967 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 1.7133 3.4827 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 1.7156 3.4498 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 1.7144 3.4865 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 1.7126 3.4829 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 1.7122 3.4396 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 1.7138 3.4386 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 1.7144 3.4702 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 1.7144 3.4598 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 1.7136 3.4548 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 1.7147 3.4798 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 1.7137 3.4798 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 1.7133 3.4582 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 1.7128 3.4643 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 1.7116 3.4844 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 1.7099 3.4740 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 1.7100 3.4553 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 1.7107 3.5125 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 1.7106 3.5785 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 1.7103 3.5035 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 1.7092 3.4838 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 1.7092 3.4715 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 1.7094 3.4556 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 1.7088 3.4557 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 1.7083 3.5081 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 1.7074 3.4808 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 1.7062 3.4942 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 1.7049 3.4845 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 1.7039 3.4794 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 1.7031 3.4864 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 1.7033 3.5331 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 1.7023 3.4331 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 1.7011 3.4719 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 1.7011 3.4763 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 1.7000 3.5106 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 1.6994 3.4697 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 1.6986 3.4606 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 1.6982 3.4695 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 1.6986 3.4867 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 1.6979 3.5127 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 1.6986 3.4624 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 1.6983 3.4675 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 1.6982 3.4579 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 1.6977 3.4979 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 1.6975 3.4827 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 1.6978 3.5179 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 1.6974 3.4982 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 1.6970 3.5145 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 1.6973 3.4883 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 1.6970 3.4346 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 1.6977 3.4700 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 1.6979 3.4456 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 1.6980 3.5556 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 1.6978 3.4668 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 1.6980 3.4212 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 1.6980 3.4612 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 1.6975 3.4158 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 1.6972 3.4964 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 1.6969 3.4508 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 1.6972 3.4356 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 1.6972 3.4433 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 1.6975 3.4304 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 1.6969 3.4696 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 1.6966 3.4743 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 1.6965 3.4674 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 1.6962 3.5038 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 1.6960 3.5233 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 1.6953 3.4630 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 1.6949 3.4660 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 1.6942 3.4664 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 1.6942 3.4691 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 1.6935 3.4740 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 1.6933 3.4352 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 1.6927 3.4938 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 1.6923 3.4775 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 1.6918 3.4420 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 1.6914 3.4869 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 1.6907 3.4778 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 1.6905 3.4737 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 1.6901 3.4812 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 1.6896 3.5085 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 1.6890 3.4431 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 1.6885 3.5050 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 1.6880 3.4860 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 1.6877 3.4493 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 1.6875 3.4695 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 1.6870 3.5140 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 1.6864 3.5814 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 1.6858 3.4774 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 1.6856 3.4682 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 1.6853 3.4751 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 1.6848 3.4820 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 1.6844 3.4778 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 1.6840 3.4640 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 1.6837 3.4202 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 1.6835 3.4597 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 1.6832 3.4674 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 1.6830 3.4606 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 1.6829 3.4655 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 1.6826 3.4615 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 1.6822 3.4584 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 1.6819 3.4469 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 1.6816 3.4974 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 1.6812 3.4638 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 1.6806 3.4637 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 1.6804 3.4883 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 1.6802 3.4861 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 1.6800 3.4850 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 1.6798 3.5049 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 1.6796 3.4994 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 1.6791 3.4328 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 1.6786 3.4683 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 1.6785 3.4685 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 1.6784 3.4623 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 1.6779 3.4797 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 1.6778 3.4682 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 1.6776 3.4757 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 1.6774 3.4674 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 1.6770 3.4871 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 1.6765 3.4987 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 844/890 Training loss: 1.6761 3.4780 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 1.6761 3.5717 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 1.6760 3.4580 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 1.6758 3.4898 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 1.6757 3.4669 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 1.6757 3.4457 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 1.6755 3.4496 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 1.6755 3.4570 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 1.6752 3.4667 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 1.6754 3.4787 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 1.6751 3.5031 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 1.6748 3.4523 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 1.6747 3.4622 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 1.6745 3.4721 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 1.6744 3.4545 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 1.6743 3.4667 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 1.6743 3.4565 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 1.6742 3.4528 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 1.6739 3.4397 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 1.6735 3.4203 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 1.6734 3.4754 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 1.6733 3.4511 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 1.6732 3.4330 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 1.6730 3.4362 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 1.6729 3.4781 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 1.6727 3.4708 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 1.6726 3.4661 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 1.6722 3.4422 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 1.6721 3.4649 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 1.6722 3.4650 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 1.6720 3.4582 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 1.6720 3.5084 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 1.6718 3.4717 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 1.6716 3.4866 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 1.6714 3.4429 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 1.6714 3.4296 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 1.6716 3.5746 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 1.6715 3.4804 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 1.6712 3.4777 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 1.6710 3.4642 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 1.6707 4.5660 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 1.6706 3.4840 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 1.6705 3.4565 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 1.6705 3.4801 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 1.6702 3.4266 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 1.6699 3.4674 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 1.6699 3.4422 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4201 3.5838 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.4040 3.4774 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.3809 3.4464 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.3333 3.5083 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.2248 3.4601 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.1517 3.4759 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 4.0808 3.4467 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 4.0093 3.4596 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 3.9452 3.4900 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.8946 3.4524 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.8504 3.4690 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.8120 3.4490 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.7787 3.4810 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.7481 3.4405 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.7198 3.4489 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.6950 3.4802 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.6713 3.4582 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.6521 3.4688 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.6331 3.4626 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.6142 3.4416 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.5975 3.4681 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.5825 3.5151 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.5683 3.5997 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.5551 3.4644 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.5421 3.4739 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.5309 3.4907 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.5205 3.4814 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.5098 3.4684 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.5003 3.4701 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.4910 3.4739 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.4831 3.4944 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.4745 3.5047 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.4664 3.5052 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.4592 3.4957 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.4518 3.5096 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.4455 3.4511 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.4384 3.4882 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.4317 3.4560 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.4254 3.4892 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.4194 3.4832 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.4137 3.4652 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.4080 3.4966 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.4026 3.4703 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.3975 3.4941 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.3924 3.4823 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.3879 3.5120 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.3836 3.4777 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.3795 3.4628 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.3755 3.4694 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.3717 3.4860 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.3680 3.5064 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.3642 3.5619 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.3606 3.4816 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.3569 3.4996 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.3536 3.4951 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.3501 3.4590 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.3469 3.4850 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.3439 3.5673 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.3407 3.4827 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.3378 3.4997 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.3350 3.4334 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.3325 3.4703 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.3301 3.4866 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.3272 3.4539 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.3244 3.4601 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.3220 3.4793 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.3197 3.4580 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.3168 3.4469 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.3142 3.4866 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.3120 3.4932 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.3097 3.5049 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.3078 3.4376 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.3057 3.4643 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.3036 3.4597 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.3017 3.4410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 76/890 Training loss: 3.2998 3.4567 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.2981 3.4824 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.2962 3.4639 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.2943 3.4314 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.2923 3.4638 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.2903 3.4580 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.2886 3.4189 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.2870 3.4631 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.2852 3.4761 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.2833 3.4670 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.2816 3.4249 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.2798 3.4643 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.2781 3.4542 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.2766 3.4838 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.2751 3.4702 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.2737 3.4783 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.2722 3.5874 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.2706 3.5129 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.2692 3.5201 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.2677 3.4714 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.2661 3.4886 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.2648 3.4759 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.2632 3.4602 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.2618 3.4975 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.2604 3.4309 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.2590 3.5102 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.2577 3.4909 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.2564 3.4810 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.2550 3.4300 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.2536 3.4789 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.2523 3.4759 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.2507 3.4678 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.2492 3.4487 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.2478 3.4472 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.2462 3.4295 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.2448 3.4490 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.2434 3.4572 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.2419 3.4753 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.2404 3.4714 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.2389 3.4863 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.2374 3.4755 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.2359 3.4697 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.2345 3.4547 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.2332 3.4702 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.2316 3.5296 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.2303 3.4692 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.2291 3.4705 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.2277 3.4801 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.2266 3.5181 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.2252 3.4767 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.2236 3.4903 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.2222 3.6139 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.2209 3.5162 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.2193 3.4745 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.2179 3.5287 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.2164 3.4736 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.2148 3.4784 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.2132 3.4451 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.2116 3.4823 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 3.2097 3.4641 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 3.2078 3.4914 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 3.2060 3.4649 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 3.2042 3.5134 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 3.2024 3.4908 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 3.2005 3.4915 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 3.1987 3.4335 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 3.1966 3.4726 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 3.1947 3.4511 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 3.1926 3.4774 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 3.1906 3.4837 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 3.1886 3.4597 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 3.1866 3.4687 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 3.1846 3.4505 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 3.1824 3.4854 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 3.1801 3.4478 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 3.1781 3.4674 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 3.1761 3.4582 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 3.1739 3.4967 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 3.1716 3.4800 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 3.1693 3.4568 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 3.1670 3.4862 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 3.1646 3.4394 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 3.1621 3.4656 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 3.1596 3.4722 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 3.1572 3.4344 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 3.1550 3.5543 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 3.1527 3.4629 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 3.1502 3.4435 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 3.1479 3.4224 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 3.1454 3.4668 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 3.1431 3.4674 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 3.1407 3.4022 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 3.1383 3.4446 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 3.1359 3.4350 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 3.1334 3.4379 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 3.1309 3.4740 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 3.1287 3.4532 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 3.1265 3.5101 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 3.1242 3.4382 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 3.1219 3.4020 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 3.1194 3.4163 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 3.1169 3.4625 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 3.1142 3.4460 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.7286 3.4077 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.6750 3.4552 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.6610 3.4230 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.6541 3.4289 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.6480 3.4626 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.6441 3.4420 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.6400 3.4309 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.6374 3.4643 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.6357 3.4965 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.6320 3.4501 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.6279 3.4882 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.6263 3.4369 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.6237 3.4395 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.6236 3.4558 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.6209 3.4553 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.6186 3.4592 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.6166 3.3987 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.6171 3.5831 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 197/890 Training loss: 2.6151 3.4335 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.6118 3.4911 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.6095 3.4593 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.6086 3.4455 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.6068 3.4763 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.6047 3.4578 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.6024 3.4641 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.6006 3.4714 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.5984 3.4471 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.5964 3.4659 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.5948 3.4695 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.5931 3.4722 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.5920 3.4949 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.5898 3.4846 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.5877 3.5240 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.5863 3.4495 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.5844 3.4795 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.5829 3.4927 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.5808 3.4593 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.5785 3.4516 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.5765 3.4922 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.5746 3.4569 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.5726 3.4573 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.5706 3.4670 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.5685 3.4305 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.5665 3.4384 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.5646 3.4451 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.5622 3.4522 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.5610 3.5069 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.5593 3.4834 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.5576 3.4457 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.5564 3.4849 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.5546 3.4781 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.5531 3.5790 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.5516 3.5022 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.5499 3.4735 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.5480 3.4749 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.5467 3.4574 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.5451 3.4591 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.5436 3.4810 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.5421 3.4399 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.5409 3.5060 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.5394 3.4572 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.5381 3.4484 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.5370 3.4908 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.5355 3.4814 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.5341 3.4859 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.5330 3.4944 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.5317 3.4903 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.5300 3.4294 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.5284 3.4826 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.5273 3.4547 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.5262 3.4321 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.5252 3.4783 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.5239 3.4531 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.5225 3.4755 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.5212 3.4559 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.5204 3.4789 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.5191 3.4789 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.5181 3.4785 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.5167 3.4939 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.5155 3.4962 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.5142 3.4397 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.5131 3.4646 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.5118 3.4812 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.5105 3.5052 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.5088 3.4248 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.5075 3.4779 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.5063 3.5698 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.5051 3.4429 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.5039 3.4726 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.5028 3.4647 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.5016 3.4549 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.5005 3.4298 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.4994 3.4677 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.4980 3.4509 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.4967 3.4307 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.4954 3.4765 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.4942 3.4508 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.4930 3.4584 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.4918 3.4483 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.4906 3.5151 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.4897 3.4467 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.4887 3.4957 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.4873 3.4545 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.4862 3.4754 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.4849 3.4666 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.4839 3.4879 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.4828 3.4645 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.4819 3.4563 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.4809 3.4834 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.4796 3.4579 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.4786 3.5064 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.4777 3.4705 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.4767 3.4572 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.4756 3.4597 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.4745 3.4715 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.4731 3.4774 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.4721 3.4423 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.4711 3.4526 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.4703 3.4901 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.4693 3.4665 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.4684 3.5553 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.4674 3.5181 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.4664 3.4494 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.4656 3.4557 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.4645 3.4697 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.4634 3.4655 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.4625 3.4660 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.4616 3.4655 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.4606 3.4804 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.4597 3.4630 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.4587 3.5042 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.4576 3.4582 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.4567 3.4932 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.4558 3.5122 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.4547 3.4807 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.4538 3.4623 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.4528 3.4337 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.4519 3.5153 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.4511 3.4728 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 318/890 Training loss: 2.4502 3.4449 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.4494 3.4348 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.4484 3.4511 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.4475 3.4473 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.4465 3.4882 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.4456 3.4538 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.4449 3.4806 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.4440 3.4763 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.4433 3.4753 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.4423 3.4827 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.4413 3.4396 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.4406 3.4593 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.4399 3.4862 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.4391 3.4616 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.4383 3.4885 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.4373 3.4713 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.4364 3.5982 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.4355 3.4831 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.4346 3.4471 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.4335 3.4357 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.4328 3.4804 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.4320 3.4915 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.4310 3.4487 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.4301 3.4762 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.4292 3.4889 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.4284 3.4166 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.4275 3.4515 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.4267 3.4567 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.4259 3.4565 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.4250 3.5073 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.4240 3.4515 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.4231 3.4526 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.4224 3.4714 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.4217 3.4462 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.4210 3.4650 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.4203 3.4855 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.4196 3.4441 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.4186 3.4607 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.4177 3.4893 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.3497 3.4885 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.2933 3.4634 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.2743 3.4643 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.2675 3.4636 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.2634 3.5108 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.2571 3.4744 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.2563 3.4372 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.2573 3.4493 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.2570 3.4732 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.2562 3.4380 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.2531 3.4534 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.2518 3.5834 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.2519 3.4914 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.2539 3.4453 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.2533 3.4530 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.2527 3.4372 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.2517 3.4473 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.2535 3.4520 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.2530 3.4695 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.2513 3.5343 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.2503 3.4337 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.2511 3.4797 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.2504 3.4654 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.2495 3.4349 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.2484 3.4354 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.2473 3.4364 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.2462 3.3999 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.2459 3.4480 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.2457 3.4455 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.2450 3.4253 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.2448 3.4689 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.2439 3.4710 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.2429 3.4600 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.2429 3.4916 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.2420 3.4259 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.2413 3.4581 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.2404 3.4549 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.2387 3.5101 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.2375 3.4503 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.2362 3.4671 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.2353 3.4712 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.2345 3.5174 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.2332 3.4614 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.2319 3.4601 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 2.2309 3.4258 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.2291 3.4425 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.2285 3.6448 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 2.2274 3.4752 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.2266 3.4831 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.2264 3.4670 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.2251 3.4652 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.2251 3.4371 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.2240 3.4714 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.2232 3.4459 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.2223 3.4627 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.2218 3.4604 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.2211 3.4122 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.2202 3.4750 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.2192 3.4628 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.2191 3.4577 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.2183 3.4699 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.2180 3.4514 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.2177 3.4618 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.2171 3.5057 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.2163 3.4402 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.2158 3.4545 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.2153 3.4694 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.2142 3.5028 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.2133 3.4671 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.2128 3.5032 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.2124 3.4676 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.2118 3.4928 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.2115 3.4764 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.2107 3.5128 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.2100 3.4576 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.2099 3.4690 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.2091 3.4884 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.2086 3.4699 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.2078 3.4812 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.2071 3.4855 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.2061 3.5642 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.2056 3.4567 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 439/890 Training loss: 2.2046 3.4618 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.2040 3.4340 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.2030 3.4711 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 2.2022 3.4494 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.2017 3.4575 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.2010 3.4917 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 2.2001 3.4520 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.1996 3.4697 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.1989 3.4986 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.1982 3.4886 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.1974 3.4535 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.1965 3.4655 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.1957 3.4667 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.1949 3.4154 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.1943 3.4529 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.1935 3.5011 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.1925 3.4687 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.1916 3.4604 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.1911 3.4849 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.1906 3.4804 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.1898 3.4981 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.1891 3.4914 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.1884 3.4692 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.1878 3.4873 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.1872 3.4759 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.1867 3.5071 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.1862 3.4564 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.1855 3.4745 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 2.1850 3.4583 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 2.1844 3.4492 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 2.1837 3.4365 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 2.1831 3.4768 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 2.1824 3.4759 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 2.1814 3.6036 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 2.1809 3.4653 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 2.1802 3.4798 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 2.1798 3.4649 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 2.1793 3.4746 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 2.1789 3.4677 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 2.1782 3.4630 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 2.1776 3.4604 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 2.1772 3.4241 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 2.1766 3.4941 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 2.1758 3.4977 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 2.1753 3.5283 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 2.1749 3.4754 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 2.1743 3.4990 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 2.1739 3.4946 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 2.1732 3.4686 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 2.1725 3.4371 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 2.1720 3.4418 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 2.1716 3.4988 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 2.1710 3.4064 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 2.1706 3.4138 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 2.1701 3.4276 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 2.1696 3.4687 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 2.1693 3.4646 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 2.1687 3.4452 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 2.1683 3.4289 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 2.1677 3.4582 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 2.1671 3.4594 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 2.1666 3.4636 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 2.1660 3.4580 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 2.1656 3.4413 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 2.1651 3.4834 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 2.1647 3.4700 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 2.1641 3.4553 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 2.1636 3.6050 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 2.1631 3.5128 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 2.1629 3.5236 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 2.1624 3.4669 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 2.1620 3.4710 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 2.1614 3.4486 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 2.1608 3.4385 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 2.1603 3.4976 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 2.1597 3.4385 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 2.1591 3.4571 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 2.1588 3.5180 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 2.1585 3.4377 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 2.1579 3.4788 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 2.1574 3.4565 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 2.1569 3.4573 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 2.1564 3.4664 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 2.1559 3.4641 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 2.1555 3.5000 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 2.1552 3.4471 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 2.1546 3.4662 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 2.1541 3.4839 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 2.1535 3.4565 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 2.1530 3.4993 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 2.1526 3.4841 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 2.1523 3.5096 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 2.1519 3.5100 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 2.1514 3.4597 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 2.1508 3.4471 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 2.1503 3.4596 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 2.1470 3.4361 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 2.0908 3.4838 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 2.0724 3.4887 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 2.0652 3.4848 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 2.0639 3.5043 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 2.0584 3.4487 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 2.0585 3.6227 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 2.0576 3.4652 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 2.0596 3.4503 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 2.0584 3.4327 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 2.0550 4.4804 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 2.0535 3.5183 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 2.0529 3.4798 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 2.0552 3.4690 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 2.0537 3.4891 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 2.0521 3.4780 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 2.0513 3.4573 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 2.0530 3.4564 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 2.0520 3.4508 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 2.0515 3.4630 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 2.0503 3.4798 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 2.0514 3.4923 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 2.0506 3.4433 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 2.0493 3.5141 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 2.0488 3.4298 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 560/890 Training loss: 2.0476 3.4612 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 2.0463 3.4651 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 2.0466 3.4512 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 2.0472 3.4897 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 2.0475 3.4545 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 2.0469 3.4633 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 2.0464 3.4620 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 2.0460 3.4611 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 2.0464 3.4701 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 2.0457 3.4585 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 2.0451 3.4979 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 2.0447 3.5069 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 2.0432 3.4454 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 2.0417 3.4931 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 2.0408 3.4465 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 2.0401 3.5859 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 2.0394 3.4849 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 2.0385 3.4848 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 2.0373 3.4710 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 2.0370 3.4778 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 2.0353 3.4973 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 2.0352 3.5061 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 2.0343 3.4547 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 2.0339 3.4992 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 2.0341 3.4851 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 2.0333 3.4571 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 2.0336 3.4696 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 2.0330 3.4623 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 2.0325 3.4462 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 2.0317 3.4620 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 2.0315 3.4968 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 2.0313 3.4830 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 2.0307 3.4299 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 2.0301 3.4406 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 2.0301 3.4435 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 2.0296 3.4976 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 2.0300 3.4951 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 2.0301 3.4865 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 2.0299 3.4871 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 2.0294 3.4632 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 2.0294 3.4714 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 2.0291 3.4477 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 2.0283 3.4627 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 2.0277 3.4569 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 2.0272 3.4784 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 2.0272 3.4425 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 2.0271 3.4863 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 2.0270 3.4509 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 2.0264 3.4438 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 2.0260 3.6091 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 2.0260 3.4982 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 2.0256 3.4673 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 2.0253 3.4868 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 2.0245 3.4831 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 2.0240 3.4752 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 2.0233 3.4724 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 2.0231 3.4559 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 2.0222 3.4847 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 2.0218 3.4350 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 2.0210 3.4586 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 2.0204 3.4330 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 2.0200 3.4745 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 2.0194 3.4818 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 2.0186 3.4383 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 2.0184 3.4702 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 2.0178 3.4608 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 2.0173 3.4443 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 2.0165 3.4679 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 2.0159 3.4319 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 2.0153 3.4354 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 2.0149 3.4832 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 2.0144 3.4510 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 2.0137 3.4446 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 2.0131 3.4782 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 2.0123 3.4550 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 2.0121 3.4499 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 2.0116 3.4460 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 2.0110 3.4530 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 2.0104 3.4529 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 2.0098 3.4451 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 2.0095 3.4419 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 2.0090 3.4701 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 2.0087 3.4695 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 2.0085 3.4590 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 2.0081 3.5747 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 2.0077 3.4882 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 2.0073 3.4790 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 2.0069 3.4565 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 2.0064 3.4575 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 2.0059 3.4395 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 2.0052 3.4625 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 2.0048 3.4401 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 2.0044 3.4705 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 2.0042 3.4026 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 2.0036 3.4663 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 2.0034 3.4725 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 2.0027 3.4489 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 2.0022 3.4470 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 2.0020 3.4161 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 2.0016 3.4203 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 2.0010 3.4241 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 2.0007 3.4259 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 2.0004 3.4870 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 2.0000 3.4285 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 1.9997 3.4657 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 1.9991 3.4218 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 1.9985 3.4775 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 1.9983 3.4198 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 1.9980 3.4202 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 1.9976 3.4353 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 1.9974 3.4179 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 1.9971 3.4245 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 1.9968 3.4297 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 1.9967 3.4376 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 1.9963 3.4714 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 1.9962 3.4320 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 1.9958 3.4074 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 1.9954 3.4553 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 1.9951 3.4774 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 1.9947 3.5623 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 1.9945 3.4839 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 681/890 Training loss: 1.9942 3.4421 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 1.9940 3.4715 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 1.9937 3.4189 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 1.9932 3.4382 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 1.9929 3.4799 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 1.9928 3.4520 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 1.9925 3.4507 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 1.9923 3.4845 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 1.9920 3.4639 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 1.9916 3.4651 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 1.9913 3.4466 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 1.9909 3.4798 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 1.9904 3.4843 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 1.9903 3.4219 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 1.9901 3.4708 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 1.9897 3.4788 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 1.9894 3.4892 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 1.9891 3.4697 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 1.9888 3.4778 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 1.9884 3.4694 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 1.9882 3.5127 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 1.9882 3.4844 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 1.9878 3.4495 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 1.9875 3.4996 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 1.9871 3.4581 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 1.9867 3.4762 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 1.9865 3.4794 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 1.9864 3.4778 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 1.9861 3.4348 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 1.9859 3.4467 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 1.9855 3.4012 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 1.9852 3.4635 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 2.0103 3.5720 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 1.9592 3.4415 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 1.9441 3.4557 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 1.9371 3.4386 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 1.9362 3.4677 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 1.9267 3.4714 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 1.9273 3.4676 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 1.9270 3.4872 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 1.9295 3.4387 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 1.9287 3.4370 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 1.9251 3.4592 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 1.9230 3.4718 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 1.9226 3.4747 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 1.9252 3.4597 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 1.9239 3.4324 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 1.9221 3.4328 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 1.9209 3.4294 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 1.9230 3.4422 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 1.9225 3.4704 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 1.9227 3.4302 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 1.9218 3.4608 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 1.9231 3.4578 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 1.9226 3.4677 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 1.9218 3.4604 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 1.9215 3.4353 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 1.9201 3.4836 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 1.9187 3.4662 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 1.9184 3.4458 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 1.9191 3.4752 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 1.9191 3.4869 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 1.9190 3.4555 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 1.9179 3.4886 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 1.9179 3.4362 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 1.9181 3.4473 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 1.9174 3.4519 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 1.9171 3.5934 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 1.9164 3.4626 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 1.9147 3.4642 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 1.9130 3.4497 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 1.9120 3.4668 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 1.9111 3.4375 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 1.9111 3.4464 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 1.9104 3.4637 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 1.9095 3.5095 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 1.9093 3.4554 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 1.9080 3.4243 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 1.9078 3.4917 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 1.9068 3.4695 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 1.9064 3.4962 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 1.9069 3.4550 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 1.9062 3.4938 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 1.9069 3.4848 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 1.9063 3.4674 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 1.9061 3.4498 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 1.9054 3.4536 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 1.9053 3.4983 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 1.9054 3.4711 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 1.9050 3.4933 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 1.9044 3.4524 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 1.9047 3.4745 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 1.9043 3.4863 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 1.9047 3.4833 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 1.9049 3.4999 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 1.9050 3.5110 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 1.9046 3.4946 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 1.9047 3.5117 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 1.9046 3.4545 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 1.9037 3.4922 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 1.9032 3.4997 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 1.9028 3.5685 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 1.9029 3.4845 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 1.9028 3.4673 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 1.9028 3.5076 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 1.9024 3.4138 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 1.9020 3.4590 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 1.9021 3.4733 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 1.9017 3.4514 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 1.9017 3.4756 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 1.9011 3.4585 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 1.9007 3.4508 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 1.9000 3.4634 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 1.8998 3.4353 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 1.8990 3.4556 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 1.8987 3.4486 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 1.8980 3.4527 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 1.8976 3.4191 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 1.8972 3.4829 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 1.8967 3.4367 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 1.8961 3.4837 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 802/890 Training loss: 1.8958 3.4534 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 1.8954 3.4537 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 1.8951 3.4608 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 1.8944 3.4465 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 1.8940 3.4763 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 1.8935 3.4659 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 1.8932 3.4239 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 1.8930 3.4288 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 1.8924 3.4516 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 1.8919 3.4855 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 1.8913 3.4201 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 1.8911 3.4282 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 1.8909 3.4274 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 1.8904 3.4567 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 1.8899 3.4528 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 1.8894 3.5861 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 1.8892 3.4844 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 1.8888 3.4225 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 1.8885 3.4679 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 1.8883 3.4603 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 1.8880 3.4504 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 1.8878 3.4307 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 1.8874 3.4810 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 1.8872 3.4502 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 1.8869 3.4408 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 1.8865 3.4747 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 1.8859 3.4537 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 1.8856 3.4518 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 1.8853 3.4825 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 1.8852 3.4834 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 1.8848 3.4537 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 1.8847 3.4535 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 1.8841 3.5039 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 1.8837 3.4588 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 1.8836 3.4811 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 1.8834 3.4275 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 1.8828 3.4411 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 1.8827 3.4589 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 1.8826 3.4685 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 1.8822 3.4763 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 1.8820 3.4719 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 1.8815 3.4285 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 1.8811 3.4868 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 1.8809 3.4865 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 1.8807 3.4934 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 1.8805 3.4519 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 1.8804 3.4652 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 1.8802 3.4685 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 1.8800 3.4932 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 1.8800 3.6106 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 1.8798 3.4456 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 1.8798 3.4833 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 1.8795 3.4792 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 1.8793 3.4756 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 1.8790 3.4645 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 1.8787 3.4429 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 1.8785 3.4591 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 1.8784 3.4628 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 1.8784 3.4511 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 1.8782 3.4669 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 1.8780 3.4942 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 1.8777 3.4958 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 1.8777 3.4653 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 1.8776 3.4832 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 1.8774 3.4723 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 1.8772 3.4566 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 1.8769 3.4505 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 1.8767 3.4772 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 1.8764 3.4863 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 1.8761 3.4681 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 1.8760 3.4615 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 1.8760 3.4896 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 1.8757 3.4593 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 1.8756 3.4879 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 1.8753 3.4989 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 1.8751 3.4479 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 1.8748 3.4408 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 1.8746 3.4508 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 1.8747 3.4666 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 1.8745 3.4823 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 1.8742 3.4167 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 1.8739 3.4622 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 1.8736 3.4663 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 1.8735 3.5020 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 1.8734 3.5929 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 1.8732 3.4521 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 1.8730 3.4860 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 1.8727 3.4690 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 1.8725 3.4851 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4228 4.8117 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.3751 4.8101 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.3148 4.7698 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.1856 4.7166 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.1116 4.6785 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.0394 4.7211 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 3.9642 4.6812 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 3.8908 4.7161 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 3.8253 4.7290 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.7721 4.8059 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.7253 4.7307 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.6857 4.7547 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.6507 4.7790 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.6203 4.7100 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.5925 4.8057 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.5676 4.7614 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.5452 4.7998 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.5266 4.8086 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.5086 4.7510 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.4901 4.7599 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.4742 4.7478 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.4593 4.8756 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.4455 4.8040 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.4326 4.7109 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.4208 4.7338 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.4098 4.7328 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.4000 4.7205 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.3898 4.7458 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.3804 4.7204 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.3717 4.8109 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.3641 4.7130 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.3559 4.8148 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.3479 4.7251 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 34/890 Training loss: 3.3408 4.8074 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.3335 4.8014 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.3272 4.7420 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.3202 4.7537 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.3136 4.7054 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.3071 4.7457 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.3011 4.7525 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.2951 4.7454 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.2895 4.7758 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.2839 4.7265 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.2784 4.7977 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.2731 4.7419 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.2681 4.7245 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.2634 4.8576 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.2589 4.7359 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.2545 4.7154 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.2501 4.6668 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.2457 4.7686 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.2412 4.7266 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.2371 4.7157 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.2326 4.7713 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.2285 4.7406 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.2242 4.6901 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.2201 4.7224 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.2160 4.6919 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.2118 4.7149 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.2078 4.7406 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.2037 4.7590 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.1999 4.7104 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.1963 4.6907 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.1920 4.7127 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.1877 4.7239 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.1838 4.8106 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.1799 4.8124 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.1751 4.6960 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.1705 4.7105 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.1664 4.7844 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.1620 4.7363 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.1578 4.8584 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.1533 4.7693 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.1487 4.7678 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.1440 4.7427 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.1395 4.7371 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.1349 4.7155 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.1303 4.7275 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.1253 4.7721 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.1202 4.7626 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.1152 4.8000 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.1102 4.6943 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.1050 4.7518 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.0997 4.7125 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.0942 4.7445 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.0888 4.8045 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.0834 4.7873 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.0780 4.7255 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.0726 4.7574 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.0674 4.7293 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.0626 4.7326 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.0578 4.7250 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.0525 4.7788 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.0472 4.7378 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.0419 4.8154 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.0367 4.7198 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.0315 4.7268 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.0263 4.8873 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.0213 4.7248 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.0162 4.7135 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.0114 4.7728 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.0063 4.7580 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.0012 4.6790 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 2.9961 4.7014 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 2.9911 4.7214 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 2.9862 4.7104 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 2.9814 4.7193 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 2.9770 4.7264 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 2.9724 4.7479 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 2.9674 4.7381 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 2.9628 4.7887 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 2.9584 4.7219 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 2.9537 4.7269 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 2.9489 4.7225 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 2.9443 4.7025 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 2.9395 4.7858 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 2.9350 4.7096 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 2.9304 4.8276 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 2.9262 4.8138 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 2.9218 4.7701 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 2.9176 4.7303 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 2.9132 4.7574 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 2.9089 4.8356 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 2.9047 4.7656 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 2.9005 4.7928 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 2.8962 4.7160 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 2.8921 4.8292 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 2.8881 4.7330 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 2.8840 4.7817 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 2.8799 4.7792 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 2.8758 4.7113 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 2.8716 5.1025 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 2.8677 5.3881 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 2.8638 4.7617 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 2.8598 4.7775 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 2.8559 4.7839 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 2.8520 4.7662 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 2.8482 4.7664 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 2.8447 4.7151 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 2.8408 4.8369 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 2.8374 4.7102 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 2.8337 4.7647 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 2.8302 4.7243 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 2.8265 4.7914 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 2.8229 4.7376 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 2.8195 4.7298 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 2.8160 4.7284 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 2.8126 4.8884 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 2.8091 4.7105 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 2.8056 4.7345 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 2.8022 4.7764 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 2.7991 4.7852 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 2.7958 4.7169 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 2.7926 4.7283 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 2.7893 4.7761 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 156/890 Training loss: 2.7860 4.7668 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 2.7826 4.7543 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 2.7793 4.8170 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 2.7760 4.7229 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 2.7730 4.7438 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 2.7699 4.7088 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 2.7667 4.7067 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 2.7635 4.7727 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 2.7604 4.7455 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 2.7574 4.6935 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 2.7544 4.7830 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 2.7514 4.7810 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 2.7484 4.7533 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 2.7455 4.7904 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 2.7425 4.7523 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 2.7396 4.7672 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 2.7372 4.7096 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 2.7347 4.8720 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 2.7323 4.7093 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 2.7298 4.7290 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 2.7278 4.7909 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 2.7255 4.7241 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 2.7229 4.8090 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.3081 4.6829 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.2540 4.7163 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.2389 4.7091 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.2329 4.6811 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.2284 4.7737 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.2237 4.7433 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.2236 4.7561 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.2230 4.8155 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.2243 4.6811 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.2232 4.7534 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.2197 4.7674 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.2175 4.7136 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.2166 4.7660 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.2173 4.7299 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.2153 4.7359 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.2133 4.7758 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.2118 4.7782 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.2127 4.7275 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.2116 4.7853 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.2101 4.8863 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.2084 4.7328 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.2095 4.7612 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.2078 4.7801 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.2059 4.7141 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.2044 4.7249 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.2025 4.7410 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.2005 4.8084 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.1995 4.8444 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.1993 4.7026 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.1980 4.7247 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.1967 4.7330 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.1951 4.7232 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.1936 4.7394 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.1928 4.7800 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.1916 4.7024 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.1903 4.6635 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.1891 4.6793 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.1866 4.6814 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.1849 4.7620 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.1830 4.7337 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.1814 4.7135 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.1801 4.7263 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.1787 4.7180 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.1768 4.7772 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.1758 4.8221 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.1734 4.7644 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.1726 4.6927 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.1710 4.7810 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.1697 4.7147 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.1691 4.6961 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.1674 4.6954 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.1667 4.7638 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.1654 4.6945 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.1640 4.7632 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.1627 4.7060 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.1618 4.7454 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.1609 4.7641 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.1597 4.7446 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.1582 4.7295 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.1580 4.7311 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.1568 4.7391 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.1562 4.7477 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.1556 4.7805 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.1547 4.7925 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.1535 4.6971 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.1528 4.7694 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.1518 4.7055 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.1504 4.7259 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.1491 4.7417 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.1481 4.7121 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.1475 4.8808 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.1466 4.7179 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.1459 4.6917 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.1446 4.7432 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.1436 4.6803 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.1430 4.7089 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.1420 4.6953 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.1412 4.7755 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.1398 4.7639 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.1387 4.7448 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.1374 4.7414 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.1366 4.8296 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.1353 4.7262 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.1342 4.8024 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.1327 4.7204 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.1316 4.7411 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.1306 4.7902 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.1295 4.7788 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.1282 4.8177 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.1273 4.8093 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.1263 4.8163 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.1255 4.7546 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.1241 4.8047 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.1230 4.7278 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.1218 4.7768 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.1207 4.8761 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.1197 4.7507 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.1185 4.7392 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 277/890 Training loss: 2.1172 4.7527 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.1158 4.7354 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.1150 4.6740 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.1141 4.7120 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.1129 4.7645 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.1119 4.7749 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.1108 4.7754 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.1098 4.7352 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.1089 4.8185 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.1082 4.7315 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.1074 4.7363 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.1064 4.7284 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.1056 4.8074 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.1046 4.7386 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.1037 4.7308 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.1028 4.7221 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.1018 4.6991 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.1005 4.7513 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.0996 4.7692 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.0987 4.7283 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.0979 4.6913 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.0971 4.7066 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.0963 4.8643 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.0953 4.6949 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.0943 4.7780 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.0936 4.7225 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.0927 4.7225 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.0916 4.7496 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.0910 4.7114 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.0903 4.7012 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.0895 4.7033 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.0887 4.7146 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.0878 4.7899 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.0868 4.7024 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.0860 4.7538 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.0852 4.7112 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.0845 4.6967 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.0837 4.7396 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.0830 4.7735 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.0823 4.7789 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.0818 4.7345 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.0809 4.8131 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.0802 4.7231 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.0794 4.7448 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.0786 4.7353 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.0778 4.7339 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.0769 4.7316 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.0763 4.8539 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.0755 4.7803 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.0750 4.7212 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.0743 4.7617 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.0735 4.7479 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.0726 4.7303 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.0721 4.7264 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.0714 4.7776 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.0707 4.7933 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.0700 4.7368 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.0693 4.7379 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.0685 4.7938 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.0678 4.7460 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.0669 4.8029 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.0666 4.7378 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.0660 4.8026 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.0653 4.7517 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.0647 4.7865 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.0640 4.7757 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.0634 4.7240 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.0626 4.6928 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.0621 4.7851 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.0617 4.7390 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.0609 4.7306 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.0602 4.6848 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.0594 4.7391 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.0588 4.8971 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.0582 4.7225 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.0576 4.7741 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.0571 4.7495 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.0565 4.7194 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.0557 4.7764 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.0551 4.7673 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 2.0043 4.6804 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 1.9618 4.7167 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 1.9514 4.7684 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 1.9412 4.7628 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 1.9386 4.7206 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 1.9299 4.7287 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 1.9304 4.7866 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 1.9305 4.7712 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 1.9320 4.7433 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 1.9304 4.7626 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 1.9267 4.7074 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 1.9233 4.6951 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 1.9229 4.7462 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 1.9243 4.7266 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 1.9223 4.7953 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 1.9203 4.7694 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 1.9191 4.7201 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 1.9206 4.7690 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 1.9197 4.8378 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 1.9193 4.7301 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 1.9180 4.7664 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 1.9189 4.7085 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 1.9175 4.7095 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 1.9162 4.7653 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 1.9150 4.7710 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 1.9133 4.7962 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 1.9116 4.7141 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 1.9114 4.7024 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 1.9114 4.7186 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 1.9108 4.6988 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 1.9098 4.7309 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 1.9083 4.6892 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 1.9077 4.7246 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 1.9077 4.7289 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 1.9067 4.7073 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 1.9058 4.7713 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 1.9048 4.8094 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 1.9031 4.7477 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 1.9013 4.7392 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 1.9000 4.7920 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 1.8989 4.7336 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 398/890 Training loss: 1.8985 4.7575 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 1.8974 4.7087 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 1.8959 4.8626 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 1.8956 4.7037 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 1.8937 4.6965 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 1.8931 4.7123 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 1.8921 4.7064 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 1.8913 4.6749 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 1.8914 4.7362 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 1.8905 4.8177 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 1.8905 4.7123 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 1.8899 4.7358 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 1.8890 4.7397 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 1.8882 4.7615 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 1.8878 4.6913 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 1.8875 4.7684 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 1.8869 4.7427 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 1.8858 4.8133 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 1.8858 4.7453 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 1.8851 4.7303 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 1.8852 4.6764 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 1.8851 4.7279 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 1.8848 4.7558 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 1.8841 4.7158 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 1.8839 4.7131 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 1.8835 4.7466 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 1.8826 4.7646 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 1.8819 4.9113 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 1.8813 4.8065 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 1.8814 4.7133 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 1.8811 4.7923 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 1.8809 4.8218 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 1.8802 4.7590 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 1.8797 4.7135 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 1.8797 4.8168 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 1.8792 4.7652 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 1.8789 4.7443 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 1.8780 4.8193 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 1.8774 4.7805 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 1.8765 4.7629 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 1.8762 4.7690 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 1.8752 4.6937 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 1.8748 4.8171 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 1.8738 4.6822 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 1.8730 4.7511 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 1.8724 4.7807 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 1.8716 4.7668 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 1.8707 4.7494 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 1.8704 4.7387 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 1.8696 4.7659 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 1.8689 4.7015 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 1.8680 4.7396 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 1.8672 4.8180 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 1.8664 4.7885 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 1.8658 4.7682 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 1.8652 4.7214 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 1.8643 4.7333 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 1.8635 4.6819 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 1.8625 4.6924 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 1.8620 4.7540 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 1.8615 4.7288 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 1.8608 4.6847 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 1.8601 4.7189 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 1.8594 4.7028 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 1.8588 4.7236 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 1.8582 4.7832 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 1.8577 4.7687 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 1.8572 4.7984 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 1.8567 4.7025 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 1.8562 4.7476 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 1.8556 4.7335 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 1.8550 4.7192 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 1.8543 4.8042 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 1.8537 4.8036 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 1.8528 4.7216 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 1.8523 4.7513 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 1.8517 4.7047 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 1.8512 4.7846 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 1.8506 4.9100 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 1.8501 4.7376 sec/batch\n",
      "Epoch 3/5  Iteration 478/890 Training loss: 1.8492 4.7122 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 1.8484 4.7796 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 1.8480 4.7235 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 1.8475 4.7399 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 1.8467 4.7236 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 1.8464 4.7193 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 1.8459 4.7444 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 1.8454 4.7948 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 1.8450 4.7671 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 1.8442 4.7397 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 1.8436 4.7250 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 1.8431 4.7040 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 1.8427 4.7036 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 1.8422 4.7409 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 1.8418 4.7065 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 1.8414 4.6935 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 1.8410 4.7387 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 1.8408 4.7564 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 1.8402 4.7044 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 1.8400 5.6843 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 1.8395 4.6936 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 1.8390 4.7651 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 1.8386 4.6614 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 1.8380 4.9238 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 1.8377 4.7274 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 1.8373 4.6979 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 1.8370 4.7516 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 1.8367 4.7100 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 1.8362 4.7524 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 1.8356 4.7202 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 1.8353 4.7215 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 1.8348 4.7789 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 1.8345 4.7304 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 1.8340 4.7774 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 1.8336 4.7877 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 1.8332 4.7416 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 1.8327 4.7510 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 1.8321 4.6736 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 1.8318 4.7517 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 1.8315 4.7658 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 1.8311 4.7221 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 519/890 Training loss: 1.8308 4.7571 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 1.8304 4.7583 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 1.8300 4.8179 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 1.8296 4.8261 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 1.8293 4.7799 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 1.8292 4.7880 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 1.8287 4.7234 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 1.8283 4.9020 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 1.8278 4.7642 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 1.8273 4.7271 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 1.8269 4.7058 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 1.8265 4.7527 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 1.8261 4.7245 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 1.8257 4.7682 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 1.8251 4.7358 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 1.8248 4.7754 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 1.8201 4.6786 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 1.7840 4.6820 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 1.7741 4.6858 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 1.7676 4.7061 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 1.7619 4.7632 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 1.7504 4.7240 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 1.7499 4.8006 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 1.7473 4.7672 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 1.7489 4.8130 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 1.7465 4.7728 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 1.7426 4.7096 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 1.7400 4.6967 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 1.7400 4.7142 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 1.7414 4.7885 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 1.7402 4.7184 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 1.7384 4.7180 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 1.7382 4.8856 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 1.7403 4.7820 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 1.7398 4.7766 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 1.7403 4.7748 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 1.7390 4.7172 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 1.7397 4.7564 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 1.7385 4.7194 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 1.7378 4.7656 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 1.7373 4.7204 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 1.7357 4.7468 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 1.7340 4.7287 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 1.7344 4.6919 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 1.7350 4.6879 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 1.7350 4.6747 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 1.7345 4.6962 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 1.7332 4.6694 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 1.7332 4.7266 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 1.7335 4.6975 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 1.7329 4.7245 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 1.7323 4.7030 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 1.7313 4.7115 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 1.7297 4.7480 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 1.7281 4.6782 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 1.7271 4.6951 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 1.7262 4.7414 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 1.7263 4.7266 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 1.7253 4.9384 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 1.7242 4.7018 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 1.7242 4.7094 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 1.7230 4.7525 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 1.7222 4.7937 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 1.7216 4.7613 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 1.7212 4.7517 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 1.7216 4.7108 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 1.7208 4.6938 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 1.7213 4.7063 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 1.7209 4.7156 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 1.7206 4.7093 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 1.7199 4.7176 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 1.7198 4.7490 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 1.7198 4.7664 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 1.7192 4.7416 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 1.7184 4.7225 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 1.7187 4.7187 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 1.7183 4.7088 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 1.7187 4.7268 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 1.7187 4.7032 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 1.7186 4.7298 sec/batch\n",
      "Epoch 4/5  Iteration 599/890 Training loss: 1.7182 4.7036 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 1.7181 4.7074 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 1.7180 4.7324 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 1.7173 4.8423 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 1.7169 4.7311 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 1.7164 4.6896 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 1.7168 4.8013 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 1.7166 4.7168 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 1.7167 4.6951 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 1.7163 4.7702 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 1.7159 4.7111 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 1.7159 4.7769 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 1.7156 4.7626 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 1.7154 4.6919 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 1.7146 4.7729 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 1.7142 4.7062 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 1.7135 4.7790 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 1.7133 4.7929 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 1.7125 4.7142 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 1.7124 4.7896 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 1.7118 4.7699 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 1.7112 4.7816 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 1.7107 4.6966 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 1.7102 4.7142 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 1.7095 4.7290 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 1.7093 4.6944 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 1.7087 4.7586 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 1.7082 4.7740 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 1.7076 4.9309 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 1.7070 4.7136 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 1.7063 4.7790 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 1.7060 4.7169 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 1.7056 4.7619 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 1.7049 4.7502 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 1.7043 4.7013 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 1.7035 4.6880 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 1.7031 4.7818 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 1.7027 4.7137 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 1.7022 4.7153 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 1.7017 4.7092 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 1.7013 4.7656 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 640/890 Training loss: 1.7008 4.7290 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 1.7005 4.7117 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 1.7002 4.7719 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 1.7000 4.7509 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 1.6997 4.7601 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 1.6993 4.7073 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 1.6990 4.7888 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 1.6986 4.7203 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 1.6982 4.8343 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 1.6978 4.7644 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 1.6971 4.7324 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 1.6969 4.7543 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 1.6967 4.8386 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 1.6963 4.7772 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 1.6960 4.7765 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 1.6956 4.6982 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 1.6951 4.7559 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 1.6945 4.8070 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 1.6942 4.7602 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 1.6939 4.6999 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 1.6933 4.7811 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 1.6931 4.7509 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 1.6929 4.7726 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 1.6926 4.7696 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 1.6921 4.7702 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 1.6915 4.7202 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 1.6910 4.7185 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 1.6907 4.7403 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 1.6905 4.7313 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 1.6902 4.8040 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 1.6899 4.8000 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 1.6897 4.7737 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 1.6896 4.7805 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 1.6894 4.8091 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 1.6890 4.7203 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 1.6889 4.8187 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 1.6886 4.8028 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 1.6883 4.8535 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 1.6881 4.7018 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 1.6877 4.7126 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 1.6874 4.8089 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 1.6872 4.7476 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 1.6871 4.7024 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 1.6868 4.7281 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 1.6864 4.7107 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 1.6859 4.7358 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 1.6856 4.7044 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 1.6854 4.6939 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 1.6852 4.7720 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 1.6850 4.7952 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 1.6847 4.7211 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 1.6844 4.7565 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 1.6842 4.7179 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 1.6836 4.6891 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 1.6835 4.7047 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 1.6834 4.7434 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 1.6831 4.7874 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 1.6829 4.6812 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 1.6827 4.7987 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 1.6824 4.7397 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 1.6821 4.8042 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 1.6819 4.6879 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 1.6820 4.7601 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 1.6817 4.8322 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 1.6814 4.7878 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 1.6810 4.7605 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 1.6806 4.7649 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 1.6805 4.7171 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 1.6802 4.7197 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 1.6800 4.8123 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 1.6797 4.7193 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 1.6792 4.8052 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 1.6790 4.7385 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 1.7118 4.7889 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 1.6710 4.8015 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 1.6556 4.7169 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 1.6497 4.7255 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 1.6451 4.7952 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 1.6334 4.7445 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 1.6334 4.7186 sec/batch\n",
      "Epoch 5/5  Iteration 720/890 Training loss: 1.6312 4.7106 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 1.6319 4.7844 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 1.6305 4.8249 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 1.6262 4.7394 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 1.6238 4.7223 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 1.6239 4.7491 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 1.6260 4.7856 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 1.6246 4.7624 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 1.6223 4.8888 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 1.6225 4.8507 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 1.6237 4.7607 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 1.6234 4.7439 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 1.6242 4.8218 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 1.6234 4.7909 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 1.6236 4.7753 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 1.6222 4.7345 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 1.6217 4.7686 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 1.6221 4.7135 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 1.6202 4.8056 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 1.6185 4.7006 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 1.6189 4.7402 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 1.6191 4.7864 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 1.6193 4.7241 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 1.6189 4.7311 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 1.6177 4.7300 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 1.6178 4.7650 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 1.6182 4.7415 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 1.6178 4.7503 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 1.6175 4.7367 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 1.6166 4.7378 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 1.6150 4.7910 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 1.6133 4.7560 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 1.6126 4.7625 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 1.6118 4.8299 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 1.6124 4.7132 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 1.6115 4.7323 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 1.6105 4.7383 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 1.6106 4.7301 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 1.6096 4.8042 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 1.6091 4.7553 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 1.6085 4.7322 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 761/890 Training loss: 1.6080 4.7633 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 1.6085 4.7410 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 1.6079 4.7667 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 1.6084 4.6866 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 1.6081 4.7084 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 1.6080 4.7970 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 1.6076 4.7533 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 1.6076 4.7379 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 1.6078 4.7765 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 1.6073 4.7712 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 1.6065 4.6997 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 1.6070 4.6994 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 1.6067 4.7179 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 1.6073 4.8238 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 1.6074 4.7873 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 1.6074 4.8051 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 1.6070 4.7185 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 1.6069 4.8756 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 1.6068 4.7048 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 1.6064 4.6995 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 1.6061 4.7845 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 1.6060 4.7030 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 1.6066 4.7178 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 1.6064 4.7091 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 1.6065 4.7231 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 1.6063 4.7913 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 1.6059 4.7267 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 1.6059 4.7526 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 1.6056 4.8022 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 1.6053 4.7606 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 1.6046 4.7269 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 1.6042 4.7344 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 1.6036 4.7195 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 1.6035 4.7207 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 1.6027 4.6815 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 1.6026 4.8179 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 1.6020 4.7302 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 1.6015 4.7983 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 1.6011 4.7159 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 1.6006 4.7391 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 1.6000 4.7034 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 1.5999 4.7972 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 1.5994 4.8579 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 1.5990 4.7800 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 1.5984 4.7222 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 1.5979 4.7201 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 1.5973 4.8061 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 1.5970 4.7308 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 1.5969 4.7669 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 1.5962 4.7220 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 1.5957 4.7075 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 1.5950 4.7954 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 1.5947 4.7067 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 1.5943 4.7001 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 1.5940 4.7250 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 1.5937 4.7358 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 1.5934 4.7731 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 1.5931 4.8104 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 1.5929 4.7296 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 1.5926 4.8025 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 1.5924 4.7318 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 1.5923 4.7486 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 1.5918 4.7830 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 1.5916 4.7411 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 1.5913 4.7257 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 1.5908 4.7819 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 1.5904 4.7634 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 1.5899 4.7584 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 1.5898 4.9055 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 1.5897 4.8061 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 1.5895 4.7979 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 1.5893 4.8121 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 1.5891 4.7140 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 1.5886 4.7424 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 1.5880 4.7511 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 1.5879 4.7274 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 1.5877 4.7171 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 1.5871 4.7663 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 1.5870 4.7830 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 1.5869 4.7685 sec/batch\n",
      "Epoch 5/5  Iteration 841/890 Training loss: 1.5866 4.8123 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 1.5861 4.7179 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 1.5856 4.7501 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 1.5852 4.7145 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 1.5850 4.7699 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 1.5849 4.8282 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 1.5847 4.7425 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 1.5845 4.7478 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 1.5844 4.7584 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 1.5843 4.7387 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 1.5841 4.7499 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 1.5839 4.7198 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 1.5840 4.7921 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 1.5837 4.9464 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 1.5834 4.7557 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 1.5834 4.8119 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 1.5830 4.8278 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 1.5829 4.8122 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 1.5827 4.7083 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 1.5828 4.7813 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 1.5827 4.7860 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 1.5824 4.7776 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 1.5818 4.7748 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 1.5817 4.7292 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 1.5815 4.7067 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 1.5813 4.7289 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 1.5811 4.7364 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 1.5810 4.8253 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 1.5808 4.7728 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 1.5806 4.7580 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 1.5802 4.8167 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 1.5801 4.7287 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 1.5801 4.7036 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 1.5800 4.8121 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 1.5798 4.7160 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 1.5796 4.7489 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 1.5795 4.7980 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 1.5792 4.7419 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 1.5792 4.8784 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 1.5795 4.7892 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 1.5792 4.8107 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 882/890 Training loss: 1.5790 4.7521 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 1.5787 4.7443 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 1.5784 4.7444 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 1.5783 4.7872 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 1.5781 4.8078 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 1.5780 4.8406 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 1.5777 4.7878 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 1.5773 4.8254 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 1.5773 4.7794 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4183 4.7732 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.3923 4.7402 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.3504 4.7250 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.2205 4.7543 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.1182 4.8084 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.0272 4.8227 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 3.9525 5.7384 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 3.8832 4.8065 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 3.8221 4.7832 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.7750 4.8577 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.7322 4.7814 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.6959 4.7348 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.6642 4.9126 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.6370 4.8422 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.6115 4.8073 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.5885 4.7480 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.5666 4.7212 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.5482 4.7733 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.5307 4.7510 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.5135 4.8080 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.4988 4.7592 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.4849 4.7577 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.4715 4.8085 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.4596 4.7332 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.4480 4.7534 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.4376 4.7500 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.4280 4.7831 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.4179 4.7586 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.4087 4.7387 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.4000 4.7205 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.3928 4.7959 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.3850 4.7771 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.3775 4.8087 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.3707 4.7534 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.3638 4.8197 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.3577 4.7271 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.3510 4.7485 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.3448 4.8681 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.3387 4.7395 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.3330 4.7689 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.3273 4.7552 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.3220 4.7625 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.3166 4.7763 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.3115 4.8146 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.3063 4.7434 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.3018 4.7308 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.2975 4.8062 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.2933 4.8097 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.2892 4.7353 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.2854 4.7472 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.2813 4.8268 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.2772 4.8218 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.2734 4.7368 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.2695 4.7610 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.2660 4.7223 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.2621 4.8402 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.2586 4.7723 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.2552 4.7730 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.2516 4.7912 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.2484 4.7979 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.2450 4.7258 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.2422 4.7366 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.2394 4.8562 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.2361 4.8249 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.2328 4.7854 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.2300 4.7494 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.2271 4.7657 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.2235 4.7855 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.2204 4.7187 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.2177 4.7535 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.2146 4.6794 sec/batch\n",
      "Epoch 1/5  Iteration 72/890 Training loss: 3.2119 4.7173 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.2089 4.7604 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.2060 4.7326 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.2032 4.7243 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.2005 4.7660 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.1976 4.7346 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.1948 4.8315 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.1918 4.7951 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.1887 4.7071 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.1857 4.7657 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.1828 4.6993 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.1799 4.7182 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.1769 4.7272 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.1737 4.7104 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.1704 4.7692 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.1673 4.7495 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.1640 4.7205 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.1609 4.8863 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.1579 4.7063 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.1549 4.7666 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.1518 4.7382 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.1488 4.8145 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.1454 4.7332 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.1421 4.7668 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.1388 4.7483 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.1355 4.7404 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.1320 4.7372 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.1286 4.8022 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.1251 4.7707 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.1218 4.6990 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.1182 4.7009 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.1145 4.7147 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.1109 4.7148 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.1072 4.7751 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.1035 4.7519 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.0997 4.7170 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.0961 4.6796 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.0927 4.7791 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.0895 4.7725 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.0859 4.7309 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.0826 4.7831 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.0790 4.7276 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.0752 4.8691 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 115/890 Training loss: 3.0716 4.7582 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.0678 4.7973 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.0641 4.7332 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.0606 4.8262 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.0572 4.7549 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.0536 4.8093 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.0501 4.7584 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.0465 4.7733 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.0428 4.8367 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.0392 4.8077 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 3.0356 4.7498 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 3.0318 4.8027 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 3.0282 4.7958 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 3.0248 4.6869 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 3.0213 4.7301 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 3.0177 4.8113 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 3.0141 4.7080 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 3.0105 4.7281 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 3.0070 4.7322 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 3.0036 4.7160 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 2.9999 4.8095 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 2.9963 4.7412 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 2.9928 4.7459 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 2.9893 4.7546 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 2.9859 4.8590 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 2.9824 4.7448 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 2.9792 4.7641 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 2.9756 4.7068 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 2.9722 4.7255 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 2.9688 4.7990 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 2.9654 4.7185 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 2.9622 4.7410 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 2.9589 4.8034 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 2.9558 4.8062 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 2.9524 4.7890 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 2.9490 4.7193 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 2.9459 4.7258 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 2.9430 4.7609 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 2.9399 4.7142 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 2.9368 4.7233 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 2.9336 4.7746 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 2.9304 4.7927 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 2.9271 4.8190 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 2.9239 4.7410 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 2.9207 4.8384 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 2.9178 4.7391 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 2.9148 4.7508 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 2.9116 4.7228 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 2.9084 4.7337 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 2.9054 4.9134 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 2.9024 4.8330 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 2.8994 4.7632 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 2.8965 4.7440 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 2.8937 4.7312 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 2.8909 4.7865 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 2.8879 4.7597 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 2.8851 4.8206 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 2.8823 4.7072 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 2.8796 4.7193 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 2.8770 4.8000 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 2.8745 4.7807 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 2.8724 4.8347 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 2.8702 4.7684 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 2.8679 4.7869 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.4738 4.8119 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.4047 4.7231 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.3878 4.7160 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.3829 4.7815 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.3809 4.7366 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.3759 4.7258 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.3732 4.7450 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.3731 4.8247 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.3738 4.8027 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.3735 4.7183 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.3697 4.9263 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.3688 4.7639 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.3670 4.7891 sec/batch\n",
      "Epoch 2/5  Iteration 192/890 Training loss: 2.3683 4.7908 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.3669 4.7243 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.3654 4.7448 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.3645 4.7178 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.3649 4.7495 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.3640 4.7694 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.3618 4.7764 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.3602 4.7509 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.3603 4.7749 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.3585 4.7311 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.3566 4.7211 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.3551 4.7473 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.3535 4.7410 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.3521 4.7221 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.3508 4.7437 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.3504 4.7344 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.3495 4.7508 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.3488 4.7439 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.3475 4.7330 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.3459 4.7627 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.3449 4.7132 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.3436 4.7659 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.3427 4.7460 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.3413 4.9395 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.3392 4.8128 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.3376 4.7473 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.3366 4.7732 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.3352 4.6779 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.3340 4.8199 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.3325 4.7317 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.3309 4.8265 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.3297 4.7280 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.3279 4.7166 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.3270 4.7095 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.3256 4.8044 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.3244 4.7880 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.3239 4.7393 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.3225 4.7931 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.3220 4.7014 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.3208 4.7045 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.3196 4.7133 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.3185 4.7177 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.3177 4.7071 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.3168 4.7435 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 236/890 Training loss: 2.3155 4.8401 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.3143 4.7868 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.3138 4.7956 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.3126 4.7105 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.3118 4.9691 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.3114 4.7219 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.3105 4.8208 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.3094 4.7467 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.3088 4.7418 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.3078 4.8093 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.3066 4.8606 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.3055 4.7553 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.3046 4.7652 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.3039 4.7652 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.3031 4.8271 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.3023 4.7473 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.3012 4.7491 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.3002 4.7534 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.2997 4.7954 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.2986 4.7709 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.2980 4.8223 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.2968 4.8213 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.2958 4.7951 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.2946 4.7458 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.2940 4.7857 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.2929 4.7911 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.2918 4.8127 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.2904 4.7461 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.2893 4.7346 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.2885 4.8590 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.2875 4.7488 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.2864 4.7609 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.2857 4.8269 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.2847 4.7996 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.2840 4.7372 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.2828 4.8167 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.2817 4.7461 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.2807 4.7542 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.2797 4.7623 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.2788 4.7643 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.2779 4.7455 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.2768 4.7596 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.2757 4.7733 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.2750 4.7879 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.2742 4.7618 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.2731 4.7228 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.2721 4.7878 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.2712 4.7801 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.2703 4.7496 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.2696 4.7846 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.2690 4.7675 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.2683 4.7400 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.2675 4.7585 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.2667 4.7514 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.2661 4.8803 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.2653 4.7592 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.2644 4.7526 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.2635 4.7409 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.2624 4.7809 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.2618 4.8200 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.2610 4.7203 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.2604 4.7405 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.2597 4.7699 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.2591 4.7525 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.2584 4.8096 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.2576 4.8108 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.2570 4.7728 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.2564 4.8325 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.2556 4.8333 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.2550 4.7553 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.2545 4.7760 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.2539 4.7765 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.2533 4.7408 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.2525 4.8140 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.2517 4.7741 sec/batch\n",
      "Epoch 2/5  Iteration 311/890 Training loss: 2.2511 4.7857 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.2506 4.7942 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.2500 4.7576 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.2494 4.7797 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.2488 4.9369 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.2483 4.8592 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.2480 4.8049 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.2473 4.7327 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.2469 4.7847 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.2463 4.7527 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.2456 4.7387 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.2450 4.7573 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.2444 4.7651 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 2.2440 4.7967 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 2.2434 4.7606 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 2.2429 4.7587 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 2.2423 4.7855 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 2.2416 4.7776 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 2.2410 4.7392 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 2.2406 4.6938 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 2.2401 4.7982 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 2.2396 4.7771 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 2.2390 4.8364 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 2.2384 4.7689 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 2.2378 4.8562 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 2.2372 4.8321 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 2.2364 4.8238 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 2.2361 4.7492 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 2.2356 4.7947 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 2.2350 4.9263 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 2.2345 4.7513 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 2.2339 4.7469 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 2.2334 4.7973 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 2.2328 4.8247 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 2.2323 4.7463 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 2.2318 4.8106 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 2.2313 4.7722 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 2.2306 4.7423 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 2.2300 4.7160 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 2.2294 4.7425 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 2.2290 4.7310 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 2.2285 4.7266 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 2.2281 4.7192 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 2.2275 4.7255 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 2.2268 4.7598 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 2.2263 4.7550 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 357/890 Training loss: 2.2124 4.7956 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 2.1549 4.7534 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 2.1394 4.7040 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 2.1322 4.7273 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 2.1284 4.8243 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 2.1213 4.7804 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 2.1217 4.8128 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 2.1221 5.7458 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 2.1238 4.8548 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 2.1228 4.7977 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 2.1200 4.7778 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 2.1180 4.7091 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 2.1177 4.7957 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 2.1194 4.7552 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 2.1183 4.7667 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 2.1173 4.7303 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 2.1166 4.8155 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 2.1182 4.7696 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 2.1184 4.7258 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 2.1178 4.7576 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 2.1172 4.7433 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 2.1186 4.7404 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 2.1178 4.7416 sec/batch\n",
      "Epoch 3/5  Iteration 380/890 Training loss: 2.1169 4.7169 sec/batch\n",
      "Epoch 3/5  Iteration 381/890 Training loss: 2.1166 4.7875 sec/batch\n",
      "Epoch 3/5  Iteration 382/890 Training loss: 2.1151 4.7412 sec/batch\n",
      "Epoch 3/5  Iteration 383/890 Training loss: 2.1138 4.8372 sec/batch\n",
      "Epoch 3/5  Iteration 384/890 Training loss: 2.1136 4.7431 sec/batch\n",
      "Epoch 3/5  Iteration 385/890 Training loss: 2.1140 4.7675 sec/batch\n",
      "Epoch 3/5  Iteration 386/890 Training loss: 2.1136 4.7700 sec/batch\n",
      "Epoch 3/5  Iteration 387/890 Training loss: 2.1132 4.7944 sec/batch\n",
      "Epoch 3/5  Iteration 388/890 Training loss: 2.1123 4.7816 sec/batch\n",
      "Epoch 3/5  Iteration 389/890 Training loss: 2.1114 4.7280 sec/batch\n",
      "Epoch 3/5  Iteration 390/890 Training loss: 2.1115 4.8640 sec/batch\n",
      "Epoch 3/5  Iteration 391/890 Training loss: 2.1108 4.7341 sec/batch\n",
      "Epoch 3/5  Iteration 392/890 Training loss: 2.1103 4.7743 sec/batch\n",
      "Epoch 3/5  Iteration 393/890 Training loss: 2.1097 4.8033 sec/batch\n",
      "Epoch 3/5  Iteration 394/890 Training loss: 2.1080 4.7959 sec/batch\n",
      "Epoch 3/5  Iteration 395/890 Training loss: 2.1067 4.7741 sec/batch\n",
      "Epoch 3/5  Iteration 396/890 Training loss: 2.1056 4.7793 sec/batch\n",
      "Epoch 3/5  Iteration 397/890 Training loss: 2.1048 4.7033 sec/batch\n",
      "Epoch 3/5  Iteration 398/890 Training loss: 2.1044 4.7499 sec/batch\n",
      "Epoch 3/5  Iteration 399/890 Training loss: 2.1036 4.7507 sec/batch\n",
      "Epoch 3/5  Iteration 400/890 Training loss: 2.1023 4.7459 sec/batch\n",
      "Epoch 3/5  Iteration 401/890 Training loss: 2.1019 4.7159 sec/batch\n",
      "Epoch 3/5  Iteration 402/890 Training loss: 2.1002 4.7712 sec/batch\n",
      "Epoch 3/5  Iteration 403/890 Training loss: 2.0997 4.7571 sec/batch\n",
      "Epoch 3/5  Iteration 404/890 Training loss: 2.0987 4.8277 sec/batch\n",
      "Epoch 3/5  Iteration 405/890 Training loss: 2.0980 4.7425 sec/batch\n",
      "Epoch 3/5  Iteration 406/890 Training loss: 2.0983 4.8062 sec/batch\n",
      "Epoch 3/5  Iteration 407/890 Training loss: 2.0974 4.7587 sec/batch\n",
      "Epoch 3/5  Iteration 408/890 Training loss: 2.0976 4.7513 sec/batch\n",
      "Epoch 3/5  Iteration 409/890 Training loss: 2.0969 4.7945 sec/batch\n",
      "Epoch 3/5  Iteration 410/890 Training loss: 2.0964 4.7657 sec/batch\n",
      "Epoch 3/5  Iteration 411/890 Training loss: 2.0958 4.7486 sec/batch\n",
      "Epoch 3/5  Iteration 412/890 Training loss: 2.0956 4.7533 sec/batch\n",
      "Epoch 3/5  Iteration 413/890 Training loss: 2.0953 4.8382 sec/batch\n",
      "Epoch 3/5  Iteration 414/890 Training loss: 2.0947 4.7587 sec/batch\n",
      "Epoch 3/5  Iteration 415/890 Training loss: 2.0939 4.8693 sec/batch\n",
      "Epoch 3/5  Iteration 416/890 Training loss: 2.0943 4.7350 sec/batch\n",
      "Epoch 3/5  Iteration 417/890 Training loss: 2.0939 4.7163 sec/batch\n",
      "Epoch 3/5  Iteration 418/890 Training loss: 2.0941 4.7294 sec/batch\n",
      "Epoch 3/5  Iteration 419/890 Training loss: 2.0941 4.6882 sec/batch\n",
      "Epoch 3/5  Iteration 420/890 Training loss: 2.0939 4.6964 sec/batch\n",
      "Epoch 3/5  Iteration 421/890 Training loss: 2.0933 4.7587 sec/batch\n",
      "Epoch 3/5  Iteration 422/890 Training loss: 2.0932 4.7163 sec/batch\n",
      "Epoch 3/5  Iteration 423/890 Training loss: 2.0929 4.7981 sec/batch\n",
      "Epoch 3/5  Iteration 424/890 Training loss: 2.0921 4.7738 sec/batch\n",
      "Epoch 3/5  Iteration 425/890 Training loss: 2.0914 4.7246 sec/batch\n",
      "Epoch 3/5  Iteration 426/890 Training loss: 2.0911 4.7162 sec/batch\n",
      "Epoch 3/5  Iteration 427/890 Training loss: 2.0912 4.7573 sec/batch\n",
      "Epoch 3/5  Iteration 428/890 Training loss: 2.0908 4.7179 sec/batch\n",
      "Epoch 3/5  Iteration 429/890 Training loss: 2.0907 4.7677 sec/batch\n",
      "Epoch 3/5  Iteration 430/890 Training loss: 2.0901 4.8290 sec/batch\n",
      "Epoch 3/5  Iteration 431/890 Training loss: 2.0895 4.7700 sec/batch\n",
      "Epoch 3/5  Iteration 432/890 Training loss: 2.0895 4.8114 sec/batch\n",
      "Epoch 3/5  Iteration 433/890 Training loss: 2.0890 4.8158 sec/batch\n",
      "Epoch 3/5  Iteration 434/890 Training loss: 2.0889 4.7413 sec/batch\n",
      "Epoch 3/5  Iteration 435/890 Training loss: 2.0881 4.7769 sec/batch\n",
      "Epoch 3/5  Iteration 436/890 Training loss: 2.0875 4.7707 sec/batch\n",
      "Epoch 3/5  Iteration 437/890 Training loss: 2.0868 4.7047 sec/batch\n",
      "Epoch 3/5  Iteration 438/890 Training loss: 2.0866 4.7359 sec/batch\n",
      "Epoch 3/5  Iteration 439/890 Training loss: 2.0857 4.7419 sec/batch\n",
      "Epoch 3/5  Iteration 440/890 Training loss: 2.0852 4.9099 sec/batch\n",
      "Epoch 3/5  Iteration 441/890 Training loss: 2.0843 4.7564 sec/batch\n",
      "Epoch 3/5  Iteration 442/890 Training loss: 2.0837 4.7052 sec/batch\n",
      "Epoch 3/5  Iteration 443/890 Training loss: 2.0832 4.7234 sec/batch\n",
      "Epoch 3/5  Iteration 444/890 Training loss: 2.0825 4.7582 sec/batch\n",
      "Epoch 3/5  Iteration 445/890 Training loss: 2.0816 4.7148 sec/batch\n",
      "Epoch 3/5  Iteration 446/890 Training loss: 2.0813 4.8243 sec/batch\n",
      "Epoch 3/5  Iteration 447/890 Training loss: 2.0806 4.7428 sec/batch\n",
      "Epoch 3/5  Iteration 448/890 Training loss: 2.0802 4.7281 sec/batch\n",
      "Epoch 3/5  Iteration 449/890 Training loss: 2.0792 4.7607 sec/batch\n",
      "Epoch 3/5  Iteration 450/890 Training loss: 2.0786 4.7622 sec/batch\n",
      "Epoch 3/5  Iteration 451/890 Training loss: 2.0779 4.7504 sec/batch\n",
      "Epoch 3/5  Iteration 452/890 Training loss: 2.0773 4.7480 sec/batch\n",
      "Epoch 3/5  Iteration 453/890 Training loss: 2.0767 4.7731 sec/batch\n",
      "Epoch 3/5  Iteration 454/890 Training loss: 2.0760 4.7485 sec/batch\n",
      "Epoch 3/5  Iteration 455/890 Training loss: 2.0753 4.7721 sec/batch\n",
      "Epoch 3/5  Iteration 456/890 Training loss: 2.0744 4.7911 sec/batch\n",
      "Epoch 3/5  Iteration 457/890 Training loss: 2.0740 4.7944 sec/batch\n",
      "Epoch 3/5  Iteration 458/890 Training loss: 2.0735 4.7427 sec/batch\n",
      "Epoch 3/5  Iteration 459/890 Training loss: 2.0728 4.8145 sec/batch\n",
      "Epoch 3/5  Iteration 460/890 Training loss: 2.0722 4.7357 sec/batch\n",
      "Epoch 3/5  Iteration 461/890 Training loss: 2.0715 4.7292 sec/batch\n",
      "Epoch 3/5  Iteration 462/890 Training loss: 2.0710 4.8208 sec/batch\n",
      "Epoch 3/5  Iteration 463/890 Training loss: 2.0706 4.7495 sec/batch\n",
      "Epoch 3/5  Iteration 464/890 Training loss: 2.0703 4.7344 sec/batch\n",
      "Epoch 3/5  Iteration 465/890 Training loss: 2.0700 4.7709 sec/batch\n",
      "Epoch 3/5  Iteration 466/890 Training loss: 2.0696 4.9516 sec/batch\n",
      "Epoch 3/5  Iteration 467/890 Training loss: 2.0691 4.7325 sec/batch\n",
      "Epoch 3/5  Iteration 468/890 Training loss: 2.0687 4.8238 sec/batch\n",
      "Epoch 3/5  Iteration 469/890 Training loss: 2.0683 4.7963 sec/batch\n",
      "Epoch 3/5  Iteration 470/890 Training loss: 2.0677 4.8039 sec/batch\n",
      "Epoch 3/5  Iteration 471/890 Training loss: 2.0671 4.7655 sec/batch\n",
      "Epoch 3/5  Iteration 472/890 Training loss: 2.0663 4.8046 sec/batch\n",
      "Epoch 3/5  Iteration 473/890 Training loss: 2.0657 4.8420 sec/batch\n",
      "Epoch 3/5  Iteration 474/890 Training loss: 2.0652 4.7392 sec/batch\n",
      "Epoch 3/5  Iteration 475/890 Training loss: 2.0648 4.7205 sec/batch\n",
      "Epoch 3/5  Iteration 476/890 Training loss: 2.0644 4.7518 sec/batch\n",
      "Epoch 3/5  Iteration 477/890 Training loss: 2.0641 4.7432 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5  Iteration 478/890 Training loss: 2.0634 4.7771 sec/batch\n",
      "Epoch 3/5  Iteration 479/890 Training loss: 2.0628 4.8070 sec/batch\n",
      "Epoch 3/5  Iteration 480/890 Training loss: 2.0626 4.7342 sec/batch\n",
      "Epoch 3/5  Iteration 481/890 Training loss: 2.0621 4.7220 sec/batch\n",
      "Epoch 3/5  Iteration 482/890 Training loss: 2.0614 4.7495 sec/batch\n",
      "Epoch 3/5  Iteration 483/890 Training loss: 2.0611 4.8070 sec/batch\n",
      "Epoch 3/5  Iteration 484/890 Training loss: 2.0608 4.6928 sec/batch\n",
      "Epoch 3/5  Iteration 485/890 Training loss: 2.0604 4.7274 sec/batch\n",
      "Epoch 3/5  Iteration 486/890 Training loss: 2.0600 4.7059 sec/batch\n",
      "Epoch 3/5  Iteration 487/890 Training loss: 2.0594 4.8139 sec/batch\n",
      "Epoch 3/5  Iteration 488/890 Training loss: 2.0588 4.7012 sec/batch\n",
      "Epoch 3/5  Iteration 489/890 Training loss: 2.0584 4.8240 sec/batch\n",
      "Epoch 3/5  Iteration 490/890 Training loss: 2.0580 4.7220 sec/batch\n",
      "Epoch 3/5  Iteration 491/890 Training loss: 2.0577 4.8501 sec/batch\n",
      "Epoch 3/5  Iteration 492/890 Training loss: 2.0574 4.7384 sec/batch\n",
      "Epoch 3/5  Iteration 493/890 Training loss: 2.0571 4.8531 sec/batch\n",
      "Epoch 3/5  Iteration 494/890 Training loss: 2.0567 4.7314 sec/batch\n",
      "Epoch 3/5  Iteration 495/890 Training loss: 2.0566 4.7776 sec/batch\n",
      "Epoch 3/5  Iteration 496/890 Training loss: 2.0562 4.7439 sec/batch\n",
      "Epoch 3/5  Iteration 497/890 Training loss: 2.0559 4.7413 sec/batch\n",
      "Epoch 3/5  Iteration 498/890 Training loss: 2.0555 4.7317 sec/batch\n",
      "Epoch 3/5  Iteration 499/890 Training loss: 2.0551 4.7350 sec/batch\n",
      "Epoch 3/5  Iteration 500/890 Training loss: 2.0547 4.7856 sec/batch\n",
      "Epoch 3/5  Iteration 501/890 Training loss: 2.0543 4.8076 sec/batch\n",
      "Epoch 3/5  Iteration 502/890 Training loss: 2.0540 4.7428 sec/batch\n",
      "Epoch 3/5  Iteration 503/890 Training loss: 2.0538 4.7267 sec/batch\n",
      "Epoch 3/5  Iteration 504/890 Training loss: 2.0536 4.7879 sec/batch\n",
      "Epoch 3/5  Iteration 505/890 Training loss: 2.0533 4.7566 sec/batch\n",
      "Epoch 3/5  Iteration 506/890 Training loss: 2.0527 4.7691 sec/batch\n",
      "Epoch 3/5  Iteration 507/890 Training loss: 2.0522 4.7305 sec/batch\n",
      "Epoch 3/5  Iteration 508/890 Training loss: 2.0521 4.7321 sec/batch\n",
      "Epoch 3/5  Iteration 509/890 Training loss: 2.0518 4.7339 sec/batch\n",
      "Epoch 3/5  Iteration 510/890 Training loss: 2.0516 4.7441 sec/batch\n",
      "Epoch 3/5  Iteration 511/890 Training loss: 2.0512 4.7602 sec/batch\n",
      "Epoch 3/5  Iteration 512/890 Training loss: 2.0508 4.8448 sec/batch\n",
      "Epoch 3/5  Iteration 513/890 Training loss: 2.0504 4.7532 sec/batch\n",
      "Epoch 3/5  Iteration 514/890 Training loss: 2.0499 4.7387 sec/batch\n",
      "Epoch 3/5  Iteration 515/890 Training loss: 2.0494 4.7287 sec/batch\n",
      "Epoch 3/5  Iteration 516/890 Training loss: 2.0493 4.8119 sec/batch\n",
      "Epoch 3/5  Iteration 517/890 Training loss: 2.0491 4.7533 sec/batch\n",
      "Epoch 3/5  Iteration 518/890 Training loss: 2.0488 4.7423 sec/batch\n",
      "Epoch 3/5  Iteration 519/890 Training loss: 2.0485 4.7443 sec/batch\n",
      "Epoch 3/5  Iteration 520/890 Training loss: 2.0482 4.7149 sec/batch\n",
      "Epoch 3/5  Iteration 521/890 Training loss: 2.0479 4.7176 sec/batch\n",
      "Epoch 3/5  Iteration 522/890 Training loss: 2.0475 4.6937 sec/batch\n",
      "Epoch 3/5  Iteration 523/890 Training loss: 2.0473 4.7226 sec/batch\n",
      "Epoch 3/5  Iteration 524/890 Training loss: 2.0472 4.7180 sec/batch\n",
      "Epoch 3/5  Iteration 525/890 Training loss: 2.0469 4.7318 sec/batch\n",
      "Epoch 3/5  Iteration 526/890 Training loss: 2.0465 4.7290 sec/batch\n",
      "Epoch 3/5  Iteration 527/890 Training loss: 2.0460 4.7437 sec/batch\n",
      "Epoch 3/5  Iteration 528/890 Training loss: 2.0456 4.7198 sec/batch\n",
      "Epoch 3/5  Iteration 529/890 Training loss: 2.0453 4.7251 sec/batch\n",
      "Epoch 3/5  Iteration 530/890 Training loss: 2.0450 4.7322 sec/batch\n",
      "Epoch 3/5  Iteration 531/890 Training loss: 2.0447 4.8338 sec/batch\n",
      "Epoch 3/5  Iteration 532/890 Training loss: 2.0444 4.7245 sec/batch\n",
      "Epoch 3/5  Iteration 533/890 Training loss: 2.0439 4.8585 sec/batch\n",
      "Epoch 3/5  Iteration 534/890 Training loss: 2.0436 4.8450 sec/batch\n",
      "Epoch 4/5  Iteration 535/890 Training loss: 2.0636 4.7378 sec/batch\n",
      "Epoch 4/5  Iteration 536/890 Training loss: 2.0122 4.7157 sec/batch\n",
      "Epoch 4/5  Iteration 537/890 Training loss: 1.9984 4.7555 sec/batch\n",
      "Epoch 4/5  Iteration 538/890 Training loss: 1.9899 4.7521 sec/batch\n",
      "Epoch 4/5  Iteration 539/890 Training loss: 1.9848 4.8131 sec/batch\n",
      "Epoch 4/5  Iteration 540/890 Training loss: 1.9749 4.7696 sec/batch\n",
      "Epoch 4/5  Iteration 541/890 Training loss: 1.9742 4.8616 sec/batch\n",
      "Epoch 4/5  Iteration 542/890 Training loss: 1.9731 4.7995 sec/batch\n",
      "Epoch 4/5  Iteration 543/890 Training loss: 1.9762 4.7964 sec/batch\n",
      "Epoch 4/5  Iteration 544/890 Training loss: 1.9761 4.7733 sec/batch\n",
      "Epoch 4/5  Iteration 545/890 Training loss: 1.9735 4.7325 sec/batch\n",
      "Epoch 4/5  Iteration 546/890 Training loss: 1.9706 4.7031 sec/batch\n",
      "Epoch 4/5  Iteration 547/890 Training loss: 1.9702 4.7489 sec/batch\n",
      "Epoch 4/5  Iteration 548/890 Training loss: 1.9721 4.7258 sec/batch\n",
      "Epoch 4/5  Iteration 549/890 Training loss: 1.9713 4.7257 sec/batch\n",
      "Epoch 4/5  Iteration 550/890 Training loss: 1.9700 4.6908 sec/batch\n",
      "Epoch 4/5  Iteration 551/890 Training loss: 1.9695 4.7243 sec/batch\n",
      "Epoch 4/5  Iteration 552/890 Training loss: 1.9714 4.7580 sec/batch\n",
      "Epoch 4/5  Iteration 553/890 Training loss: 1.9715 4.7344 sec/batch\n",
      "Epoch 4/5  Iteration 554/890 Training loss: 1.9718 4.7053 sec/batch\n",
      "Epoch 4/5  Iteration 555/890 Training loss: 1.9713 4.7732 sec/batch\n",
      "Epoch 4/5  Iteration 556/890 Training loss: 1.9722 4.7539 sec/batch\n",
      "Epoch 4/5  Iteration 557/890 Training loss: 1.9712 4.7647 sec/batch\n",
      "Epoch 4/5  Iteration 558/890 Training loss: 1.9704 4.7080 sec/batch\n",
      "Epoch 4/5  Iteration 559/890 Training loss: 1.9699 4.6936 sec/batch\n",
      "Epoch 4/5  Iteration 560/890 Training loss: 1.9683 4.7372 sec/batch\n",
      "Epoch 4/5  Iteration 561/890 Training loss: 1.9670 4.7269 sec/batch\n",
      "Epoch 4/5  Iteration 562/890 Training loss: 1.9673 4.7540 sec/batch\n",
      "Epoch 4/5  Iteration 563/890 Training loss: 1.9680 4.7727 sec/batch\n",
      "Epoch 4/5  Iteration 564/890 Training loss: 1.9678 4.7069 sec/batch\n",
      "Epoch 4/5  Iteration 565/890 Training loss: 1.9675 4.8023 sec/batch\n",
      "Epoch 4/5  Iteration 566/890 Training loss: 1.9666 4.9332 sec/batch\n",
      "Epoch 4/5  Iteration 567/890 Training loss: 1.9661 4.7317 sec/batch\n",
      "Epoch 4/5  Iteration 568/890 Training loss: 1.9665 4.7248 sec/batch\n",
      "Epoch 4/5  Iteration 569/890 Training loss: 1.9658 4.7180 sec/batch\n",
      "Epoch 4/5  Iteration 570/890 Training loss: 1.9651 4.7414 sec/batch\n",
      "Epoch 4/5  Iteration 571/890 Training loss: 1.9647 4.7699 sec/batch\n",
      "Epoch 4/5  Iteration 572/890 Training loss: 1.9631 4.7934 sec/batch\n",
      "Epoch 4/5  Iteration 573/890 Training loss: 1.9617 4.7812 sec/batch\n",
      "Epoch 4/5  Iteration 574/890 Training loss: 1.9607 4.8237 sec/batch\n",
      "Epoch 4/5  Iteration 575/890 Training loss: 1.9600 4.7574 sec/batch\n",
      "Epoch 4/5  Iteration 576/890 Training loss: 1.9598 4.7714 sec/batch\n",
      "Epoch 4/5  Iteration 577/890 Training loss: 1.9590 4.7374 sec/batch\n",
      "Epoch 4/5  Iteration 578/890 Training loss: 1.9579 4.8232 sec/batch\n",
      "Epoch 4/5  Iteration 579/890 Training loss: 1.9577 4.7749 sec/batch\n",
      "Epoch 4/5  Iteration 580/890 Training loss: 1.9561 4.7863 sec/batch\n",
      "Epoch 4/5  Iteration 581/890 Training loss: 1.9557 4.7178 sec/batch\n",
      "Epoch 4/5  Iteration 582/890 Training loss: 1.9549 4.7362 sec/batch\n",
      "Epoch 4/5  Iteration 583/890 Training loss: 1.9544 4.7229 sec/batch\n",
      "Epoch 4/5  Iteration 584/890 Training loss: 1.9550 4.7202 sec/batch\n",
      "Epoch 4/5  Iteration 585/890 Training loss: 1.9541 4.7188 sec/batch\n",
      "Epoch 4/5  Iteration 586/890 Training loss: 1.9545 4.7702 sec/batch\n",
      "Epoch 4/5  Iteration 587/890 Training loss: 1.9539 4.7191 sec/batch\n",
      "Epoch 4/5  Iteration 588/890 Training loss: 1.9537 4.8047 sec/batch\n",
      "Epoch 4/5  Iteration 589/890 Training loss: 1.9531 4.8031 sec/batch\n",
      "Epoch 4/5  Iteration 590/890 Training loss: 1.9530 4.7641 sec/batch\n",
      "Epoch 4/5  Iteration 591/890 Training loss: 1.9529 4.9271 sec/batch\n",
      "Epoch 4/5  Iteration 592/890 Training loss: 1.9523 4.7515 sec/batch\n",
      "Epoch 4/5  Iteration 593/890 Training loss: 1.9514 4.7608 sec/batch\n",
      "Epoch 4/5  Iteration 594/890 Training loss: 1.9518 4.8479 sec/batch\n",
      "Epoch 4/5  Iteration 595/890 Training loss: 1.9513 4.8018 sec/batch\n",
      "Epoch 4/5  Iteration 596/890 Training loss: 1.9515 4.7958 sec/batch\n",
      "Epoch 4/5  Iteration 597/890 Training loss: 1.9515 4.7652 sec/batch\n",
      "Epoch 4/5  Iteration 598/890 Training loss: 1.9516 4.7260 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5  Iteration 599/890 Training loss: 1.9511 4.7455 sec/batch\n",
      "Epoch 4/5  Iteration 600/890 Training loss: 1.9510 4.8009 sec/batch\n",
      "Epoch 4/5  Iteration 601/890 Training loss: 1.9507 4.8061 sec/batch\n",
      "Epoch 4/5  Iteration 602/890 Training loss: 1.9499 4.6944 sec/batch\n",
      "Epoch 4/5  Iteration 603/890 Training loss: 1.9493 4.7136 sec/batch\n",
      "Epoch 4/5  Iteration 604/890 Training loss: 1.9490 4.8879 sec/batch\n",
      "Epoch 4/5  Iteration 605/890 Training loss: 1.9491 4.8236 sec/batch\n",
      "Epoch 4/5  Iteration 606/890 Training loss: 1.9489 4.7173 sec/batch\n",
      "Epoch 4/5  Iteration 607/890 Training loss: 1.9490 4.8099 sec/batch\n",
      "Epoch 4/5  Iteration 608/890 Training loss: 1.9483 4.7900 sec/batch\n",
      "Epoch 4/5  Iteration 609/890 Training loss: 1.9479 4.7299 sec/batch\n",
      "Epoch 4/5  Iteration 610/890 Training loss: 1.9481 4.6851 sec/batch\n",
      "Epoch 4/5  Iteration 611/890 Training loss: 1.9478 4.7317 sec/batch\n",
      "Epoch 4/5  Iteration 612/890 Training loss: 1.9477 4.8161 sec/batch\n",
      "Epoch 4/5  Iteration 613/890 Training loss: 1.9469 4.7376 sec/batch\n",
      "Epoch 4/5  Iteration 614/890 Training loss: 1.9464 4.7157 sec/batch\n",
      "Epoch 4/5  Iteration 615/890 Training loss: 1.9457 4.7423 sec/batch\n",
      "Epoch 4/5  Iteration 616/890 Training loss: 1.9456 4.9670 sec/batch\n",
      "Epoch 4/5  Iteration 617/890 Training loss: 1.9448 4.7964 sec/batch\n",
      "Epoch 4/5  Iteration 618/890 Training loss: 1.9444 4.8600 sec/batch\n",
      "Epoch 4/5  Iteration 619/890 Training loss: 1.9435 4.7540 sec/batch\n",
      "Epoch 4/5  Iteration 620/890 Training loss: 1.9429 4.7783 sec/batch\n",
      "Epoch 4/5  Iteration 621/890 Training loss: 1.9424 4.7252 sec/batch\n",
      "Epoch 4/5  Iteration 622/890 Training loss: 1.9419 4.8646 sec/batch\n",
      "Epoch 4/5  Iteration 623/890 Training loss: 1.9411 4.7598 sec/batch\n",
      "Epoch 4/5  Iteration 624/890 Training loss: 1.9408 4.7849 sec/batch\n",
      "Epoch 4/5  Iteration 625/890 Training loss: 1.9403 4.7312 sec/batch\n",
      "Epoch 4/5  Iteration 626/890 Training loss: 1.9398 4.8056 sec/batch\n",
      "Epoch 4/5  Iteration 627/890 Training loss: 1.9390 4.8149 sec/batch\n",
      "Epoch 4/5  Iteration 628/890 Training loss: 1.9384 4.7307 sec/batch\n",
      "Epoch 4/5  Iteration 629/890 Training loss: 1.9378 4.7381 sec/batch\n",
      "Epoch 4/5  Iteration 630/890 Training loss: 1.9374 4.7437 sec/batch\n",
      "Epoch 4/5  Iteration 631/890 Training loss: 1.9369 4.7248 sec/batch\n",
      "Epoch 4/5  Iteration 632/890 Training loss: 1.9362 4.7775 sec/batch\n",
      "Epoch 4/5  Iteration 633/890 Training loss: 1.9356 4.7262 sec/batch\n",
      "Epoch 4/5  Iteration 634/890 Training loss: 1.9347 4.7333 sec/batch\n",
      "Epoch 4/5  Iteration 635/890 Training loss: 1.9343 4.7304 sec/batch\n",
      "Epoch 4/5  Iteration 636/890 Training loss: 1.9340 4.7075 sec/batch\n",
      "Epoch 4/5  Iteration 637/890 Training loss: 1.9334 4.8010 sec/batch\n",
      "Epoch 4/5  Iteration 638/890 Training loss: 1.9330 4.8632 sec/batch\n",
      "Epoch 4/5  Iteration 639/890 Training loss: 1.9325 4.7343 sec/batch\n",
      "Epoch 4/5  Iteration 640/890 Training loss: 1.9320 4.8205 sec/batch\n",
      "Epoch 4/5  Iteration 641/890 Training loss: 1.9316 4.7948 sec/batch\n",
      "Epoch 4/5  Iteration 642/890 Training loss: 1.9312 4.8566 sec/batch\n",
      "Epoch 4/5  Iteration 643/890 Training loss: 1.9309 4.7607 sec/batch\n",
      "Epoch 4/5  Iteration 644/890 Training loss: 1.9306 4.7614 sec/batch\n",
      "Epoch 4/5  Iteration 645/890 Training loss: 1.9303 4.7542 sec/batch\n",
      "Epoch 4/5  Iteration 646/890 Training loss: 1.9299 4.7543 sec/batch\n",
      "Epoch 4/5  Iteration 647/890 Training loss: 1.9295 4.7047 sec/batch\n",
      "Epoch 4/5  Iteration 648/890 Training loss: 1.9290 4.7846 sec/batch\n",
      "Epoch 4/5  Iteration 649/890 Training loss: 1.9285 4.7405 sec/batch\n",
      "Epoch 4/5  Iteration 650/890 Training loss: 1.9278 4.7474 sec/batch\n",
      "Epoch 4/5  Iteration 651/890 Training loss: 1.9274 4.7718 sec/batch\n",
      "Epoch 4/5  Iteration 652/890 Training loss: 1.9270 4.7613 sec/batch\n",
      "Epoch 4/5  Iteration 653/890 Training loss: 1.9267 4.8278 sec/batch\n",
      "Epoch 4/5  Iteration 654/890 Training loss: 1.9264 4.7310 sec/batch\n",
      "Epoch 4/5  Iteration 655/890 Training loss: 1.9261 4.8006 sec/batch\n",
      "Epoch 4/5  Iteration 656/890 Training loss: 1.9254 4.7875 sec/batch\n",
      "Epoch 4/5  Iteration 657/890 Training loss: 1.9249 4.8218 sec/batch\n",
      "Epoch 4/5  Iteration 658/890 Training loss: 1.9247 4.7314 sec/batch\n",
      "Epoch 4/5  Iteration 659/890 Training loss: 1.9244 4.7374 sec/batch\n",
      "Epoch 4/5  Iteration 660/890 Training loss: 1.9238 4.6997 sec/batch\n",
      "Epoch 4/5  Iteration 661/890 Training loss: 1.9236 4.7683 sec/batch\n",
      "Epoch 4/5  Iteration 662/890 Training loss: 1.9234 4.7324 sec/batch\n",
      "Epoch 4/5  Iteration 663/890 Training loss: 1.9230 4.7879 sec/batch\n",
      "Epoch 4/5  Iteration 664/890 Training loss: 1.9226 4.7686 sec/batch\n",
      "Epoch 4/5  Iteration 665/890 Training loss: 1.9221 4.7256 sec/batch\n",
      "Epoch 4/5  Iteration 666/890 Training loss: 1.9215 4.7843 sec/batch\n",
      "Epoch 4/5  Iteration 667/890 Training loss: 1.9212 4.9931 sec/batch\n",
      "Epoch 4/5  Iteration 668/890 Training loss: 1.9209 4.7397 sec/batch\n",
      "Epoch 4/5  Iteration 669/890 Training loss: 1.9206 4.7197 sec/batch\n",
      "Epoch 4/5  Iteration 670/890 Training loss: 1.9203 4.7995 sec/batch\n",
      "Epoch 4/5  Iteration 671/890 Training loss: 1.9201 4.7464 sec/batch\n",
      "Epoch 4/5  Iteration 672/890 Training loss: 1.9198 4.7639 sec/batch\n",
      "Epoch 4/5  Iteration 673/890 Training loss: 1.9198 4.7512 sec/batch\n",
      "Epoch 4/5  Iteration 674/890 Training loss: 1.9194 4.7616 sec/batch\n",
      "Epoch 4/5  Iteration 675/890 Training loss: 1.9193 4.7493 sec/batch\n",
      "Epoch 4/5  Iteration 676/890 Training loss: 1.9191 4.7371 sec/batch\n",
      "Epoch 4/5  Iteration 677/890 Training loss: 1.9188 4.7407 sec/batch\n",
      "Epoch 4/5  Iteration 678/890 Training loss: 1.9185 4.7903 sec/batch\n",
      "Epoch 4/5  Iteration 679/890 Training loss: 1.9182 4.7755 sec/batch\n",
      "Epoch 4/5  Iteration 680/890 Training loss: 1.9180 4.7278 sec/batch\n",
      "Epoch 4/5  Iteration 681/890 Training loss: 1.9178 4.7278 sec/batch\n",
      "Epoch 4/5  Iteration 682/890 Training loss: 1.9177 4.7806 sec/batch\n",
      "Epoch 4/5  Iteration 683/890 Training loss: 1.9174 4.7671 sec/batch\n",
      "Epoch 4/5  Iteration 684/890 Training loss: 1.9170 4.7667 sec/batch\n",
      "Epoch 4/5  Iteration 685/890 Training loss: 1.9165 4.7580 sec/batch\n",
      "Epoch 4/5  Iteration 686/890 Training loss: 1.9164 4.7113 sec/batch\n",
      "Epoch 4/5  Iteration 687/890 Training loss: 1.9163 4.7481 sec/batch\n",
      "Epoch 4/5  Iteration 688/890 Training loss: 1.9161 4.7883 sec/batch\n",
      "Epoch 4/5  Iteration 689/890 Training loss: 1.9158 4.7283 sec/batch\n",
      "Epoch 4/5  Iteration 690/890 Training loss: 1.9155 4.7397 sec/batch\n",
      "Epoch 4/5  Iteration 691/890 Training loss: 1.9153 4.8156 sec/batch\n",
      "Epoch 4/5  Iteration 692/890 Training loss: 1.9150 4.9008 sec/batch\n",
      "Epoch 4/5  Iteration 693/890 Training loss: 1.9146 4.7524 sec/batch\n",
      "Epoch 4/5  Iteration 694/890 Training loss: 1.9145 4.7207 sec/batch\n",
      "Epoch 4/5  Iteration 695/890 Training loss: 1.9144 4.8074 sec/batch\n",
      "Epoch 4/5  Iteration 696/890 Training loss: 1.9141 4.7546 sec/batch\n",
      "Epoch 4/5  Iteration 697/890 Training loss: 1.9139 4.7369 sec/batch\n",
      "Epoch 4/5  Iteration 698/890 Training loss: 1.9137 4.7189 sec/batch\n",
      "Epoch 4/5  Iteration 699/890 Training loss: 1.9134 4.7521 sec/batch\n",
      "Epoch 4/5  Iteration 700/890 Training loss: 1.9131 4.7180 sec/batch\n",
      "Epoch 4/5  Iteration 701/890 Training loss: 1.9129 4.7842 sec/batch\n",
      "Epoch 4/5  Iteration 702/890 Training loss: 1.9129 4.7147 sec/batch\n",
      "Epoch 4/5  Iteration 703/890 Training loss: 1.9126 4.7249 sec/batch\n",
      "Epoch 4/5  Iteration 704/890 Training loss: 1.9123 4.6938 sec/batch\n",
      "Epoch 4/5  Iteration 705/890 Training loss: 1.9120 4.8268 sec/batch\n",
      "Epoch 4/5  Iteration 706/890 Training loss: 1.9115 4.7120 sec/batch\n",
      "Epoch 4/5  Iteration 707/890 Training loss: 1.9114 4.7870 sec/batch\n",
      "Epoch 4/5  Iteration 708/890 Training loss: 1.9111 4.7677 sec/batch\n",
      "Epoch 4/5  Iteration 709/890 Training loss: 1.9109 4.7083 sec/batch\n",
      "Epoch 4/5  Iteration 710/890 Training loss: 1.9106 4.7590 sec/batch\n",
      "Epoch 4/5  Iteration 711/890 Training loss: 1.9102 4.8607 sec/batch\n",
      "Epoch 4/5  Iteration 712/890 Training loss: 1.9100 4.7486 sec/batch\n",
      "Epoch 5/5  Iteration 713/890 Training loss: 1.9482 4.7378 sec/batch\n",
      "Epoch 5/5  Iteration 714/890 Training loss: 1.8981 4.7626 sec/batch\n",
      "Epoch 5/5  Iteration 715/890 Training loss: 1.8836 4.7681 sec/batch\n",
      "Epoch 5/5  Iteration 716/890 Training loss: 1.8728 4.7765 sec/batch\n",
      "Epoch 5/5  Iteration 717/890 Training loss: 1.8673 4.9247 sec/batch\n",
      "Epoch 5/5  Iteration 718/890 Training loss: 1.8567 4.7305 sec/batch\n",
      "Epoch 5/5  Iteration 719/890 Training loss: 1.8560 4.7202 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 720/890 Training loss: 1.8541 4.7299 sec/batch\n",
      "Epoch 5/5  Iteration 721/890 Training loss: 1.8568 4.7838 sec/batch\n",
      "Epoch 5/5  Iteration 722/890 Training loss: 1.8550 4.7685 sec/batch\n",
      "Epoch 5/5  Iteration 723/890 Training loss: 1.8523 4.7219 sec/batch\n",
      "Epoch 5/5  Iteration 724/890 Training loss: 1.8498 4.7697 sec/batch\n",
      "Epoch 5/5  Iteration 725/890 Training loss: 1.8497 4.7419 sec/batch\n",
      "Epoch 5/5  Iteration 726/890 Training loss: 1.8521 4.8079 sec/batch\n",
      "Epoch 5/5  Iteration 727/890 Training loss: 1.8515 4.7164 sec/batch\n",
      "Epoch 5/5  Iteration 728/890 Training loss: 1.8500 4.8274 sec/batch\n",
      "Epoch 5/5  Iteration 729/890 Training loss: 1.8494 4.7285 sec/batch\n",
      "Epoch 5/5  Iteration 730/890 Training loss: 1.8517 4.7016 sec/batch\n",
      "Epoch 5/5  Iteration 731/890 Training loss: 1.8516 4.7349 sec/batch\n",
      "Epoch 5/5  Iteration 732/890 Training loss: 1.8523 4.6887 sec/batch\n",
      "Epoch 5/5  Iteration 733/890 Training loss: 1.8515 4.7309 sec/batch\n",
      "Epoch 5/5  Iteration 734/890 Training loss: 1.8524 4.7300 sec/batch\n",
      "Epoch 5/5  Iteration 735/890 Training loss: 1.8518 4.7313 sec/batch\n",
      "Epoch 5/5  Iteration 736/890 Training loss: 1.8513 4.7420 sec/batch\n",
      "Epoch 5/5  Iteration 737/890 Training loss: 1.8510 4.7300 sec/batch\n",
      "Epoch 5/5  Iteration 738/890 Training loss: 1.8495 4.8594 sec/batch\n",
      "Epoch 5/5  Iteration 739/890 Training loss: 1.8481 4.7577 sec/batch\n",
      "Epoch 5/5  Iteration 740/890 Training loss: 1.8483 4.7922 sec/batch\n",
      "Epoch 5/5  Iteration 741/890 Training loss: 1.8491 4.7057 sec/batch\n",
      "Epoch 5/5  Iteration 742/890 Training loss: 1.8490 4.9166 sec/batch\n",
      "Epoch 5/5  Iteration 743/890 Training loss: 1.8488 4.7759 sec/batch\n",
      "Epoch 5/5  Iteration 744/890 Training loss: 1.8473 4.7310 sec/batch\n",
      "Epoch 5/5  Iteration 745/890 Training loss: 1.8469 4.7999 sec/batch\n",
      "Epoch 5/5  Iteration 746/890 Training loss: 1.8473 4.7751 sec/batch\n",
      "Epoch 5/5  Iteration 747/890 Training loss: 1.8467 4.7273 sec/batch\n",
      "Epoch 5/5  Iteration 748/890 Training loss: 1.8460 4.7102 sec/batch\n",
      "Epoch 5/5  Iteration 749/890 Training loss: 1.8452 4.7006 sec/batch\n",
      "Epoch 5/5  Iteration 750/890 Training loss: 1.8438 4.7040 sec/batch\n",
      "Epoch 5/5  Iteration 751/890 Training loss: 1.8426 4.7794 sec/batch\n",
      "Epoch 5/5  Iteration 752/890 Training loss: 1.8418 4.7304 sec/batch\n",
      "Epoch 5/5  Iteration 753/890 Training loss: 1.8410 4.7766 sec/batch\n",
      "Epoch 5/5  Iteration 754/890 Training loss: 1.8413 4.7544 sec/batch\n",
      "Epoch 5/5  Iteration 755/890 Training loss: 1.8404 4.7868 sec/batch\n",
      "Epoch 5/5  Iteration 756/890 Training loss: 1.8392 4.7482 sec/batch\n",
      "Epoch 5/5  Iteration 757/890 Training loss: 1.8391 4.8001 sec/batch\n",
      "Epoch 5/5  Iteration 758/890 Training loss: 1.8377 4.7276 sec/batch\n",
      "Epoch 5/5  Iteration 759/890 Training loss: 1.8372 5.7441 sec/batch\n",
      "Epoch 5/5  Iteration 760/890 Training loss: 1.8365 4.7988 sec/batch\n",
      "Epoch 5/5  Iteration 761/890 Training loss: 1.8361 4.7119 sec/batch\n",
      "Epoch 5/5  Iteration 762/890 Training loss: 1.8368 4.7178 sec/batch\n",
      "Epoch 5/5  Iteration 763/890 Training loss: 1.8360 4.7577 sec/batch\n",
      "Epoch 5/5  Iteration 764/890 Training loss: 1.8366 4.7173 sec/batch\n",
      "Epoch 5/5  Iteration 765/890 Training loss: 1.8361 4.7383 sec/batch\n",
      "Epoch 5/5  Iteration 766/890 Training loss: 1.8359 4.7517 sec/batch\n",
      "Epoch 5/5  Iteration 767/890 Training loss: 1.8354 4.9004 sec/batch\n",
      "Epoch 5/5  Iteration 768/890 Training loss: 1.8354 4.7872 sec/batch\n",
      "Epoch 5/5  Iteration 769/890 Training loss: 1.8355 4.7436 sec/batch\n",
      "Epoch 5/5  Iteration 770/890 Training loss: 1.8350 4.7224 sec/batch\n",
      "Epoch 5/5  Iteration 771/890 Training loss: 1.8342 4.7718 sec/batch\n",
      "Epoch 5/5  Iteration 772/890 Training loss: 1.8346 4.7434 sec/batch\n",
      "Epoch 5/5  Iteration 773/890 Training loss: 1.8343 4.7249 sec/batch\n",
      "Epoch 5/5  Iteration 774/890 Training loss: 1.8348 4.7992 sec/batch\n",
      "Epoch 5/5  Iteration 775/890 Training loss: 1.8350 4.8060 sec/batch\n",
      "Epoch 5/5  Iteration 776/890 Training loss: 1.8351 4.7953 sec/batch\n",
      "Epoch 5/5  Iteration 777/890 Training loss: 1.8347 4.7861 sec/batch\n",
      "Epoch 5/5  Iteration 778/890 Training loss: 1.8348 4.6789 sec/batch\n",
      "Epoch 5/5  Iteration 779/890 Training loss: 1.8346 4.7294 sec/batch\n",
      "Epoch 5/5  Iteration 780/890 Training loss: 1.8342 4.7770 sec/batch\n",
      "Epoch 5/5  Iteration 781/890 Training loss: 1.8339 4.7981 sec/batch\n",
      "Epoch 5/5  Iteration 782/890 Training loss: 1.8336 4.7314 sec/batch\n",
      "Epoch 5/5  Iteration 783/890 Training loss: 1.8339 4.7944 sec/batch\n",
      "Epoch 5/5  Iteration 784/890 Training loss: 1.8339 4.7384 sec/batch\n",
      "Epoch 5/5  Iteration 785/890 Training loss: 1.8341 4.7824 sec/batch\n",
      "Epoch 5/5  Iteration 786/890 Training loss: 1.8335 4.7526 sec/batch\n",
      "Epoch 5/5  Iteration 787/890 Training loss: 1.8332 4.7988 sec/batch\n",
      "Epoch 5/5  Iteration 788/890 Training loss: 1.8333 4.7452 sec/batch\n",
      "Epoch 5/5  Iteration 789/890 Training loss: 1.8330 4.7853 sec/batch\n",
      "Epoch 5/5  Iteration 790/890 Training loss: 1.8329 4.7343 sec/batch\n",
      "Epoch 5/5  Iteration 791/890 Training loss: 1.8322 4.7141 sec/batch\n",
      "Epoch 5/5  Iteration 792/890 Training loss: 1.8318 4.8896 sec/batch\n",
      "Epoch 5/5  Iteration 793/890 Training loss: 1.8310 4.7280 sec/batch\n",
      "Epoch 5/5  Iteration 794/890 Training loss: 1.8308 4.7747 sec/batch\n",
      "Epoch 5/5  Iteration 795/890 Training loss: 1.8300 4.7229 sec/batch\n",
      "Epoch 5/5  Iteration 796/890 Training loss: 1.8298 4.7352 sec/batch\n",
      "Epoch 5/5  Iteration 797/890 Training loss: 1.8291 4.8034 sec/batch\n",
      "Epoch 5/5  Iteration 798/890 Training loss: 1.8288 4.7776 sec/batch\n",
      "Epoch 5/5  Iteration 799/890 Training loss: 1.8284 4.7629 sec/batch\n",
      "Epoch 5/5  Iteration 800/890 Training loss: 1.8279 4.7358 sec/batch\n",
      "Epoch 5/5  Iteration 801/890 Training loss: 1.8273 4.7337 sec/batch\n",
      "Epoch 5/5  Iteration 802/890 Training loss: 1.8273 4.7354 sec/batch\n",
      "Epoch 5/5  Iteration 803/890 Training loss: 1.8269 4.7968 sec/batch\n",
      "Epoch 5/5  Iteration 804/890 Training loss: 1.8265 4.7312 sec/batch\n",
      "Epoch 5/5  Iteration 805/890 Training loss: 1.8258 4.7476 sec/batch\n",
      "Epoch 5/5  Iteration 806/890 Training loss: 1.8254 4.8044 sec/batch\n",
      "Epoch 5/5  Iteration 807/890 Training loss: 1.8249 4.8230 sec/batch\n",
      "Epoch 5/5  Iteration 808/890 Training loss: 1.8247 4.7391 sec/batch\n",
      "Epoch 5/5  Iteration 809/890 Training loss: 1.8244 4.7258 sec/batch\n",
      "Epoch 5/5  Iteration 810/890 Training loss: 1.8238 4.7033 sec/batch\n",
      "Epoch 5/5  Iteration 811/890 Training loss: 1.8233 4.7425 sec/batch\n",
      "Epoch 5/5  Iteration 812/890 Training loss: 1.8225 4.7657 sec/batch\n",
      "Epoch 5/5  Iteration 813/890 Training loss: 1.8223 4.7505 sec/batch\n",
      "Epoch 5/5  Iteration 814/890 Training loss: 1.8220 4.8340 sec/batch\n",
      "Epoch 5/5  Iteration 815/890 Training loss: 1.8217 4.7659 sec/batch\n",
      "Epoch 5/5  Iteration 816/890 Training loss: 1.8213 4.8147 sec/batch\n",
      "Epoch 5/5  Iteration 817/890 Training loss: 1.8209 4.8645 sec/batch\n",
      "Epoch 5/5  Iteration 818/890 Training loss: 1.8206 4.7355 sec/batch\n",
      "Epoch 5/5  Iteration 819/890 Training loss: 1.8203 4.7213 sec/batch\n",
      "Epoch 5/5  Iteration 820/890 Training loss: 1.8200 4.7409 sec/batch\n",
      "Epoch 5/5  Iteration 821/890 Training loss: 1.8198 4.7955 sec/batch\n",
      "Epoch 5/5  Iteration 822/890 Training loss: 1.8196 4.8593 sec/batch\n",
      "Epoch 5/5  Iteration 823/890 Training loss: 1.8193 4.7246 sec/batch\n",
      "Epoch 5/5  Iteration 824/890 Training loss: 1.8190 4.7568 sec/batch\n",
      "Epoch 5/5  Iteration 825/890 Training loss: 1.8186 4.7520 sec/batch\n",
      "Epoch 5/5  Iteration 826/890 Training loss: 1.8182 4.7335 sec/batch\n",
      "Epoch 5/5  Iteration 827/890 Training loss: 1.8178 4.7129 sec/batch\n",
      "Epoch 5/5  Iteration 828/890 Training loss: 1.8172 4.7760 sec/batch\n",
      "Epoch 5/5  Iteration 829/890 Training loss: 1.8169 4.7687 sec/batch\n",
      "Epoch 5/5  Iteration 830/890 Training loss: 1.8166 4.7345 sec/batch\n",
      "Epoch 5/5  Iteration 831/890 Training loss: 1.8162 4.7282 sec/batch\n",
      "Epoch 5/5  Iteration 832/890 Training loss: 1.8158 4.7798 sec/batch\n",
      "Epoch 5/5  Iteration 833/890 Training loss: 1.8156 4.7485 sec/batch\n",
      "Epoch 5/5  Iteration 834/890 Training loss: 1.8150 4.7673 sec/batch\n",
      "Epoch 5/5  Iteration 835/890 Training loss: 1.8145 4.7186 sec/batch\n",
      "Epoch 5/5  Iteration 836/890 Training loss: 1.8144 4.7049 sec/batch\n",
      "Epoch 5/5  Iteration 837/890 Training loss: 1.8141 4.7139 sec/batch\n",
      "Epoch 5/5  Iteration 838/890 Training loss: 1.8136 4.7471 sec/batch\n",
      "Epoch 5/5  Iteration 839/890 Training loss: 1.8135 4.8026 sec/batch\n",
      "Epoch 5/5  Iteration 840/890 Training loss: 1.8133 4.7707 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5  Iteration 841/890 Training loss: 1.8130 4.7455 sec/batch\n",
      "Epoch 5/5  Iteration 842/890 Training loss: 1.8126 4.7977 sec/batch\n",
      "Epoch 5/5  Iteration 843/890 Training loss: 1.8121 4.8860 sec/batch\n",
      "Epoch 5/5  Iteration 844/890 Training loss: 1.8116 4.7329 sec/batch\n",
      "Epoch 5/5  Iteration 845/890 Training loss: 1.8113 4.7373 sec/batch\n",
      "Epoch 5/5  Iteration 846/890 Training loss: 1.8111 4.7417 sec/batch\n",
      "Epoch 5/5  Iteration 847/890 Training loss: 1.8109 4.6978 sec/batch\n",
      "Epoch 5/5  Iteration 848/890 Training loss: 1.8107 4.7378 sec/batch\n",
      "Epoch 5/5  Iteration 849/890 Training loss: 1.8107 4.8198 sec/batch\n",
      "Epoch 5/5  Iteration 850/890 Training loss: 1.8105 4.8519 sec/batch\n",
      "Epoch 5/5  Iteration 851/890 Training loss: 1.8105 4.7890 sec/batch\n",
      "Epoch 5/5  Iteration 852/890 Training loss: 1.8102 4.9818 sec/batch\n",
      "Epoch 5/5  Iteration 853/890 Training loss: 1.8102 5.1345 sec/batch\n",
      "Epoch 5/5  Iteration 854/890 Training loss: 1.8099 7.6409 sec/batch\n",
      "Epoch 5/5  Iteration 855/890 Training loss: 1.8096 5.0725 sec/batch\n",
      "Epoch 5/5  Iteration 856/890 Training loss: 1.8094 4.8252 sec/batch\n",
      "Epoch 5/5  Iteration 857/890 Training loss: 1.8091 4.8061 sec/batch\n",
      "Epoch 5/5  Iteration 858/890 Training loss: 1.8090 4.8258 sec/batch\n",
      "Epoch 5/5  Iteration 859/890 Training loss: 1.8088 4.7818 sec/batch\n",
      "Epoch 5/5  Iteration 860/890 Training loss: 1.8088 4.7670 sec/batch\n",
      "Epoch 5/5  Iteration 861/890 Training loss: 1.8086 4.7321 sec/batch\n",
      "Epoch 5/5  Iteration 862/890 Training loss: 1.8083 4.7516 sec/batch\n",
      "Epoch 5/5  Iteration 863/890 Training loss: 1.8078 4.7422 sec/batch\n",
      "Epoch 5/5  Iteration 864/890 Training loss: 1.8077 4.7313 sec/batch\n",
      "Epoch 5/5  Iteration 865/890 Training loss: 1.8075 4.7607 sec/batch\n",
      "Epoch 5/5  Iteration 866/890 Training loss: 1.8074 4.8720 sec/batch\n",
      "Epoch 5/5  Iteration 867/890 Training loss: 1.8072 4.9184 sec/batch\n",
      "Epoch 5/5  Iteration 868/890 Training loss: 1.8070 4.7641 sec/batch\n",
      "Epoch 5/5  Iteration 869/890 Training loss: 1.8068 4.7664 sec/batch\n",
      "Epoch 5/5  Iteration 870/890 Training loss: 1.8065 4.8075 sec/batch\n",
      "Epoch 5/5  Iteration 871/890 Training loss: 1.8061 4.8620 sec/batch\n",
      "Epoch 5/5  Iteration 872/890 Training loss: 1.8061 4.8824 sec/batch\n",
      "Epoch 5/5  Iteration 873/890 Training loss: 1.8060 4.7376 sec/batch\n",
      "Epoch 5/5  Iteration 874/890 Training loss: 1.8058 4.7363 sec/batch\n",
      "Epoch 5/5  Iteration 875/890 Training loss: 1.8057 4.7643 sec/batch\n",
      "Epoch 5/5  Iteration 876/890 Training loss: 1.8056 4.7897 sec/batch\n",
      "Epoch 5/5  Iteration 877/890 Training loss: 1.8053 4.7445 sec/batch\n",
      "Epoch 5/5  Iteration 878/890 Training loss: 1.8051 4.8249 sec/batch\n",
      "Epoch 5/5  Iteration 879/890 Training loss: 1.8050 4.8622 sec/batch\n",
      "Epoch 5/5  Iteration 880/890 Training loss: 1.8052 5.6437 sec/batch\n",
      "Epoch 5/5  Iteration 881/890 Training loss: 1.8049 5.9193 sec/batch\n",
      "Epoch 5/5  Iteration 882/890 Training loss: 1.8047 6.5334 sec/batch\n",
      "Epoch 5/5  Iteration 883/890 Training loss: 1.8044 6.3214 sec/batch\n",
      "Epoch 5/5  Iteration 884/890 Training loss: 1.8040 6.2421 sec/batch\n",
      "Epoch 5/5  Iteration 885/890 Training loss: 1.8039 5.5099 sec/batch\n",
      "Epoch 5/5  Iteration 886/890 Training loss: 1.8038 5.5480 sec/batch\n",
      "Epoch 5/5  Iteration 887/890 Training loss: 1.8036 5.5095 sec/batch\n",
      "Epoch 5/5  Iteration 888/890 Training loss: 1.8033 5.6108 sec/batch\n",
      "Epoch 5/5  Iteration 889/890 Training loss: 1.8029 5.5071 sec/batch\n",
      "Epoch 5/5  Iteration 890/890 Training loss: 1.8028 5.7337 sec/batch\n",
      "Epoch 1/5  Iteration 1/890 Training loss: 4.4209 13.1852 sec/batch\n",
      "Epoch 1/5  Iteration 2/890 Training loss: 4.3567 13.1002 sec/batch\n",
      "Epoch 1/5  Iteration 3/890 Training loss: 4.4437 13.0899 sec/batch\n",
      "Epoch 1/5  Iteration 4/890 Training loss: 4.4078 12.1257 sec/batch\n",
      "Epoch 1/5  Iteration 5/890 Training loss: 4.3056 11.6234 sec/batch\n",
      "Epoch 1/5  Iteration 6/890 Training loss: 4.1960 11.4987 sec/batch\n",
      "Epoch 1/5  Iteration 7/890 Training loss: 4.0939 11.5183 sec/batch\n",
      "Epoch 1/5  Iteration 8/890 Training loss: 4.0069 11.5159 sec/batch\n",
      "Epoch 1/5  Iteration 9/890 Training loss: 3.9306 11.5171 sec/batch\n",
      "Epoch 1/5  Iteration 10/890 Training loss: 3.8675 11.8465 sec/batch\n",
      "Epoch 1/5  Iteration 11/890 Training loss: 3.8123 11.7487 sec/batch\n",
      "Epoch 1/5  Iteration 12/890 Training loss: 3.7680 11.6662 sec/batch\n",
      "Epoch 1/5  Iteration 13/890 Training loss: 3.7290 11.5343 sec/batch\n",
      "Epoch 1/5  Iteration 14/890 Training loss: 3.6962 11.6160 sec/batch\n",
      "Epoch 1/5  Iteration 15/890 Training loss: 3.6661 11.6322 sec/batch\n",
      "Epoch 1/5  Iteration 16/890 Training loss: 3.6392 11.5836 sec/batch\n",
      "Epoch 1/5  Iteration 17/890 Training loss: 3.6146 11.5270 sec/batch\n",
      "Epoch 1/5  Iteration 18/890 Training loss: 3.5940 11.6332 sec/batch\n",
      "Epoch 1/5  Iteration 19/890 Training loss: 3.5745 11.5983 sec/batch\n",
      "Epoch 1/5  Iteration 20/890 Training loss: 3.5546 11.7819 sec/batch\n",
      "Epoch 1/5  Iteration 21/890 Training loss: 3.5375 11.5244 sec/batch\n",
      "Epoch 1/5  Iteration 22/890 Training loss: 3.5221 11.8310 sec/batch\n",
      "Epoch 1/5  Iteration 23/890 Training loss: 3.5074 11.7036 sec/batch\n",
      "Epoch 1/5  Iteration 24/890 Training loss: 3.4943 11.5913 sec/batch\n",
      "Epoch 1/5  Iteration 25/890 Training loss: 3.4815 11.5876 sec/batch\n",
      "Epoch 1/5  Iteration 26/890 Training loss: 3.4705 11.7593 sec/batch\n",
      "Epoch 1/5  Iteration 27/890 Training loss: 3.4602 11.5715 sec/batch\n",
      "Epoch 1/5  Iteration 28/890 Training loss: 3.4497 11.7311 sec/batch\n",
      "Epoch 1/5  Iteration 29/890 Training loss: 3.4398 11.6264 sec/batch\n",
      "Epoch 1/5  Iteration 30/890 Training loss: 3.4304 11.7059 sec/batch\n",
      "Epoch 1/5  Iteration 31/890 Training loss: 3.4228 11.5866 sec/batch\n",
      "Epoch 1/5  Iteration 32/890 Training loss: 3.4145 11.6728 sec/batch\n",
      "Epoch 1/5  Iteration 33/890 Training loss: 3.4064 11.5391 sec/batch\n",
      "Epoch 1/5  Iteration 34/890 Training loss: 3.3992 11.7013 sec/batch\n",
      "Epoch 1/5  Iteration 35/890 Training loss: 3.3917 11.5892 sec/batch\n",
      "Epoch 1/5  Iteration 36/890 Training loss: 3.3853 11.6626 sec/batch\n",
      "Epoch 1/5  Iteration 37/890 Training loss: 3.3783 11.5240 sec/batch\n",
      "Epoch 1/5  Iteration 38/890 Training loss: 3.3716 11.5628 sec/batch\n",
      "Epoch 1/5  Iteration 39/890 Training loss: 3.3652 11.5624 sec/batch\n",
      "Epoch 1/5  Iteration 40/890 Training loss: 3.3591 11.6217 sec/batch\n",
      "Epoch 1/5  Iteration 41/890 Training loss: 3.3531 11.8091 sec/batch\n",
      "Epoch 1/5  Iteration 42/890 Training loss: 3.3474 11.6004 sec/batch\n",
      "Epoch 1/5  Iteration 43/890 Training loss: 3.3417 11.5221 sec/batch\n",
      "Epoch 1/5  Iteration 44/890 Training loss: 3.3363 11.5497 sec/batch\n",
      "Epoch 1/5  Iteration 45/890 Training loss: 3.3308 11.5260 sec/batch\n",
      "Epoch 1/5  Iteration 46/890 Training loss: 3.3259 11.5009 sec/batch\n",
      "Epoch 1/5  Iteration 47/890 Training loss: 3.3212 11.6640 sec/batch\n",
      "Epoch 1/5  Iteration 48/890 Training loss: 3.3168 11.5747 sec/batch\n",
      "Epoch 1/5  Iteration 49/890 Training loss: 3.3134 11.7633 sec/batch\n",
      "Epoch 1/5  Iteration 50/890 Training loss: 3.3096 11.6803 sec/batch\n",
      "Epoch 1/5  Iteration 51/890 Training loss: 3.3055 11.7516 sec/batch\n",
      "Epoch 1/5  Iteration 52/890 Training loss: 3.3013 11.6284 sec/batch\n",
      "Epoch 1/5  Iteration 53/890 Training loss: 3.2974 11.6848 sec/batch\n",
      "Epoch 1/5  Iteration 54/890 Training loss: 3.2932 11.5976 sec/batch\n",
      "Epoch 1/5  Iteration 55/890 Training loss: 3.2894 11.6770 sec/batch\n",
      "Epoch 1/5  Iteration 56/890 Training loss: 3.2853 11.6386 sec/batch\n",
      "Epoch 1/5  Iteration 57/890 Training loss: 3.2815 11.6221 sec/batch\n",
      "Epoch 1/5  Iteration 58/890 Training loss: 3.2779 11.5375 sec/batch\n",
      "Epoch 1/5  Iteration 59/890 Training loss: 3.2741 11.5177 sec/batch\n",
      "Epoch 1/5  Iteration 60/890 Training loss: 3.2705 11.5460 sec/batch\n",
      "Epoch 1/5  Iteration 61/890 Training loss: 3.2667 11.7469 sec/batch\n",
      "Epoch 1/5  Iteration 62/890 Training loss: 3.2639 11.7137 sec/batch\n",
      "Epoch 1/5  Iteration 63/890 Training loss: 3.2609 11.7201 sec/batch\n",
      "Epoch 1/5  Iteration 64/890 Training loss: 3.2570 11.7998 sec/batch\n",
      "Epoch 1/5  Iteration 65/890 Training loss: 3.2534 11.6688 sec/batch\n",
      "Epoch 1/5  Iteration 66/890 Training loss: 3.2502 11.5952 sec/batch\n",
      "Epoch 1/5  Iteration 67/890 Training loss: 3.2469 11.5343 sec/batch\n",
      "Epoch 1/5  Iteration 68/890 Training loss: 3.2430 11.6457 sec/batch\n",
      "Epoch 1/5  Iteration 69/890 Training loss: 3.2393 11.8158 sec/batch\n",
      "Epoch 1/5  Iteration 70/890 Training loss: 3.2360 11.6208 sec/batch\n",
      "Epoch 1/5  Iteration 71/890 Training loss: 3.2324 11.6361 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5  Iteration 72/890 Training loss: 3.2292 11.8643 sec/batch\n",
      "Epoch 1/5  Iteration 73/890 Training loss: 3.2257 11.5208 sec/batch\n",
      "Epoch 1/5  Iteration 74/890 Training loss: 3.2221 11.5343 sec/batch\n",
      "Epoch 1/5  Iteration 75/890 Training loss: 3.2185 11.6628 sec/batch\n",
      "Epoch 1/5  Iteration 76/890 Training loss: 3.2150 11.5345 sec/batch\n",
      "Epoch 1/5  Iteration 77/890 Training loss: 3.2113 11.5999 sec/batch\n",
      "Epoch 1/5  Iteration 78/890 Training loss: 3.2078 11.6216 sec/batch\n",
      "Epoch 1/5  Iteration 79/890 Training loss: 3.2040 11.4717 sec/batch\n",
      "Epoch 1/5  Iteration 80/890 Training loss: 3.2000 11.4812 sec/batch\n",
      "Epoch 1/5  Iteration 81/890 Training loss: 3.1962 11.5479 sec/batch\n",
      "Epoch 1/5  Iteration 82/890 Training loss: 3.1923 11.8010 sec/batch\n",
      "Epoch 1/5  Iteration 83/890 Training loss: 3.1885 11.5663 sec/batch\n",
      "Epoch 1/5  Iteration 84/890 Training loss: 3.1845 11.7164 sec/batch\n",
      "Epoch 1/5  Iteration 85/890 Training loss: 3.1801 11.7271 sec/batch\n",
      "Epoch 1/5  Iteration 86/890 Training loss: 3.1761 11.6696 sec/batch\n",
      "Epoch 1/5  Iteration 87/890 Training loss: 3.1719 11.5401 sec/batch\n",
      "Epoch 1/5  Iteration 88/890 Training loss: 3.1677 11.5573 sec/batch\n",
      "Epoch 1/5  Iteration 89/890 Training loss: 3.1635 11.7363 sec/batch\n",
      "Epoch 1/5  Iteration 90/890 Training loss: 3.1595 11.6216 sec/batch\n",
      "Epoch 1/5  Iteration 91/890 Training loss: 3.1553 11.6918 sec/batch\n",
      "Epoch 1/5  Iteration 92/890 Training loss: 3.1511 11.7484 sec/batch\n",
      "Epoch 1/5  Iteration 93/890 Training loss: 3.1469 12.6827 sec/batch\n",
      "Epoch 1/5  Iteration 94/890 Training loss: 3.1426 11.6364 sec/batch\n",
      "Epoch 1/5  Iteration 95/890 Training loss: 3.1380 11.6825 sec/batch\n",
      "Epoch 1/5  Iteration 96/890 Training loss: 3.1334 11.7136 sec/batch\n",
      "Epoch 1/5  Iteration 97/890 Training loss: 3.1290 11.7510 sec/batch\n",
      "Epoch 1/5  Iteration 98/890 Training loss: 3.1246 11.6373 sec/batch\n",
      "Epoch 1/5  Iteration 99/890 Training loss: 3.1200 11.7168 sec/batch\n",
      "Epoch 1/5  Iteration 100/890 Training loss: 3.1154 11.6602 sec/batch\n",
      "Epoch 1/5  Iteration 101/890 Training loss: 3.1110 11.7122 sec/batch\n",
      "Epoch 1/5  Iteration 102/890 Training loss: 3.1067 11.7291 sec/batch\n",
      "Epoch 1/5  Iteration 103/890 Training loss: 3.1021 11.5476 sec/batch\n",
      "Epoch 1/5  Iteration 104/890 Training loss: 3.0975 11.6218 sec/batch\n",
      "Epoch 1/5  Iteration 105/890 Training loss: 3.0929 11.7356 sec/batch\n",
      "Epoch 1/5  Iteration 106/890 Training loss: 3.0883 11.6570 sec/batch\n",
      "Epoch 1/5  Iteration 107/890 Training loss: 3.0835 11.5898 sec/batch\n",
      "Epoch 1/5  Iteration 108/890 Training loss: 3.0790 11.6764 sec/batch\n",
      "Epoch 1/5  Iteration 109/890 Training loss: 3.0744 11.5244 sec/batch\n",
      "Epoch 1/5  Iteration 110/890 Training loss: 3.0694 11.6877 sec/batch\n",
      "Epoch 1/5  Iteration 111/890 Training loss: 3.0647 11.6706 sec/batch\n",
      "Epoch 1/5  Iteration 112/890 Training loss: 3.0602 11.5895 sec/batch\n",
      "Epoch 1/5  Iteration 113/890 Training loss: 3.0554 11.9028 sec/batch\n",
      "Epoch 1/5  Iteration 114/890 Training loss: 3.0506 11.6469 sec/batch\n",
      "Epoch 1/5  Iteration 115/890 Training loss: 3.0457 11.5619 sec/batch\n",
      "Epoch 1/5  Iteration 116/890 Training loss: 3.0407 11.6075 sec/batch\n",
      "Epoch 1/5  Iteration 117/890 Training loss: 3.0360 11.6121 sec/batch\n",
      "Epoch 1/5  Iteration 118/890 Training loss: 3.0313 11.6460 sec/batch\n",
      "Epoch 1/5  Iteration 119/890 Training loss: 3.0268 11.4864 sec/batch\n",
      "Epoch 1/5  Iteration 120/890 Training loss: 3.0222 11.5621 sec/batch\n",
      "Epoch 1/5  Iteration 121/890 Training loss: 3.0177 11.5722 sec/batch\n",
      "Epoch 1/5  Iteration 122/890 Training loss: 3.0131 11.7178 sec/batch\n",
      "Epoch 1/5  Iteration 123/890 Training loss: 3.0085 11.7845 sec/batch\n",
      "Epoch 1/5  Iteration 124/890 Training loss: 3.0039 11.6337 sec/batch\n",
      "Epoch 1/5  Iteration 125/890 Training loss: 2.9994 11.5423 sec/batch\n",
      "Epoch 1/5  Iteration 126/890 Training loss: 2.9947 11.5165 sec/batch\n",
      "Epoch 1/5  Iteration 127/890 Training loss: 2.9903 11.5959 sec/batch\n",
      "Epoch 1/5  Iteration 128/890 Training loss: 2.9860 11.5513 sec/batch\n",
      "Epoch 1/5  Iteration 129/890 Training loss: 2.9816 11.7880 sec/batch\n",
      "Epoch 1/5  Iteration 130/890 Training loss: 2.9772 11.6357 sec/batch\n",
      "Epoch 1/5  Iteration 131/890 Training loss: 2.9728 11.6292 sec/batch\n",
      "Epoch 1/5  Iteration 132/890 Training loss: 2.9683 11.5767 sec/batch\n",
      "Epoch 1/5  Iteration 133/890 Training loss: 2.9640 11.7639 sec/batch\n",
      "Epoch 1/5  Iteration 134/890 Training loss: 2.9599 11.7816 sec/batch\n",
      "Epoch 1/5  Iteration 135/890 Training loss: 2.9555 11.7450 sec/batch\n",
      "Epoch 1/5  Iteration 136/890 Training loss: 2.9512 11.6084 sec/batch\n",
      "Epoch 1/5  Iteration 137/890 Training loss: 2.9469 11.7283 sec/batch\n",
      "Epoch 1/5  Iteration 138/890 Training loss: 2.9428 11.6649 sec/batch\n",
      "Epoch 1/5  Iteration 139/890 Training loss: 2.9388 11.7579 sec/batch\n",
      "Epoch 1/5  Iteration 140/890 Training loss: 2.9345 11.6981 sec/batch\n",
      "Epoch 1/5  Iteration 141/890 Training loss: 2.9306 11.5644 sec/batch\n",
      "Epoch 1/5  Iteration 142/890 Training loss: 2.9265 11.8627 sec/batch\n",
      "Epoch 1/5  Iteration 143/890 Training loss: 2.9224 11.7361 sec/batch\n",
      "Epoch 1/5  Iteration 144/890 Training loss: 2.9184 11.7716 sec/batch\n",
      "Epoch 1/5  Iteration 145/890 Training loss: 2.9143 11.6063 sec/batch\n",
      "Epoch 1/5  Iteration 146/890 Training loss: 2.9106 11.8529 sec/batch\n",
      "Epoch 1/5  Iteration 147/890 Training loss: 2.9067 11.6394 sec/batch\n",
      "Epoch 1/5  Iteration 148/890 Training loss: 2.9029 11.6502 sec/batch\n",
      "Epoch 1/5  Iteration 149/890 Training loss: 2.8989 11.7615 sec/batch\n",
      "Epoch 1/5  Iteration 150/890 Training loss: 2.8950 11.7292 sec/batch\n",
      "Epoch 1/5  Iteration 151/890 Training loss: 2.8913 11.6637 sec/batch\n",
      "Epoch 1/5  Iteration 152/890 Training loss: 2.8877 11.7313 sec/batch\n",
      "Epoch 1/5  Iteration 153/890 Training loss: 2.8840 11.5768 sec/batch\n",
      "Epoch 1/5  Iteration 154/890 Training loss: 2.8802 11.7762 sec/batch\n",
      "Epoch 1/5  Iteration 155/890 Training loss: 2.8764 11.6155 sec/batch\n",
      "Epoch 1/5  Iteration 156/890 Training loss: 2.8727 11.6219 sec/batch\n",
      "Epoch 1/5  Iteration 157/890 Training loss: 2.8690 11.8115 sec/batch\n",
      "Epoch 1/5  Iteration 158/890 Training loss: 2.8652 11.6696 sec/batch\n",
      "Epoch 1/5  Iteration 159/890 Training loss: 2.8614 11.7230 sec/batch\n",
      "Epoch 1/5  Iteration 160/890 Training loss: 2.8579 11.6661 sec/batch\n",
      "Epoch 1/5  Iteration 161/890 Training loss: 2.8542 11.5487 sec/batch\n",
      "Epoch 1/5  Iteration 162/890 Training loss: 2.8505 11.6062 sec/batch\n",
      "Epoch 1/5  Iteration 163/890 Training loss: 2.8468 11.5439 sec/batch\n",
      "Epoch 1/5  Iteration 164/890 Training loss: 2.8433 11.8793 sec/batch\n",
      "Epoch 1/5  Iteration 165/890 Training loss: 2.8398 11.5506 sec/batch\n",
      "Epoch 1/5  Iteration 166/890 Training loss: 2.8362 11.7683 sec/batch\n",
      "Epoch 1/5  Iteration 167/890 Training loss: 2.8329 11.6117 sec/batch\n",
      "Epoch 1/5  Iteration 168/890 Training loss: 2.8304 11.7482 sec/batch\n",
      "Epoch 1/5  Iteration 169/890 Training loss: 2.8271 11.6933 sec/batch\n",
      "Epoch 1/5  Iteration 170/890 Training loss: 2.8236 11.6552 sec/batch\n",
      "Epoch 1/5  Iteration 171/890 Training loss: 2.8201 11.6546 sec/batch\n",
      "Epoch 1/5  Iteration 172/890 Training loss: 2.8168 11.5585 sec/batch\n",
      "Epoch 1/5  Iteration 173/890 Training loss: 2.8137 11.7890 sec/batch\n",
      "Epoch 1/5  Iteration 174/890 Training loss: 2.8105 11.8077 sec/batch\n",
      "Epoch 1/5  Iteration 175/890 Training loss: 2.8073 11.6492 sec/batch\n",
      "Epoch 1/5  Iteration 176/890 Training loss: 2.8041 11.7041 sec/batch\n",
      "Epoch 1/5  Iteration 177/890 Training loss: 2.8010 11.6232 sec/batch\n",
      "Epoch 1/5  Iteration 178/890 Training loss: 2.7977 11.6447 sec/batch\n",
      "Epoch 2/5  Iteration 179/890 Training loss: 2.2727 11.6781 sec/batch\n",
      "Epoch 2/5  Iteration 180/890 Training loss: 2.2315 11.5430 sec/batch\n",
      "Epoch 2/5  Iteration 181/890 Training loss: 2.2197 11.6404 sec/batch\n",
      "Epoch 2/5  Iteration 182/890 Training loss: 2.2142 11.7019 sec/batch\n",
      "Epoch 2/5  Iteration 183/890 Training loss: 2.2100 11.6056 sec/batch\n",
      "Epoch 2/5  Iteration 184/890 Training loss: 2.2042 11.8566 sec/batch\n",
      "Epoch 2/5  Iteration 185/890 Training loss: 2.2030 11.8609 sec/batch\n",
      "Epoch 2/5  Iteration 186/890 Training loss: 2.2024 11.6358 sec/batch\n",
      "Epoch 2/5  Iteration 187/890 Training loss: 2.2033 11.5492 sec/batch\n",
      "Epoch 2/5  Iteration 188/890 Training loss: 2.2011 11.5636 sec/batch\n",
      "Epoch 2/5  Iteration 189/890 Training loss: 2.1977 11.7356 sec/batch\n",
      "Epoch 2/5  Iteration 190/890 Training loss: 2.1940 11.6271 sec/batch\n",
      "Epoch 2/5  Iteration 191/890 Training loss: 2.1928 11.7051 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 192/890 Training loss: 2.1931 11.6471 sec/batch\n",
      "Epoch 2/5  Iteration 193/890 Training loss: 2.1910 11.6412 sec/batch\n",
      "Epoch 2/5  Iteration 194/890 Training loss: 2.1885 11.6668 sec/batch\n",
      "Epoch 2/5  Iteration 195/890 Training loss: 2.1862 11.9721 sec/batch\n",
      "Epoch 2/5  Iteration 196/890 Training loss: 2.1869 11.5848 sec/batch\n",
      "Epoch 2/5  Iteration 197/890 Training loss: 2.1854 11.5721 sec/batch\n",
      "Epoch 2/5  Iteration 198/890 Training loss: 2.1829 11.6676 sec/batch\n",
      "Epoch 2/5  Iteration 199/890 Training loss: 2.1808 11.6062 sec/batch\n",
      "Epoch 2/5  Iteration 200/890 Training loss: 2.1809 11.6629 sec/batch\n",
      "Epoch 2/5  Iteration 201/890 Training loss: 2.1784 11.7844 sec/batch\n",
      "Epoch 2/5  Iteration 202/890 Training loss: 2.1755 11.7025 sec/batch\n",
      "Epoch 2/5  Iteration 203/890 Training loss: 2.1734 11.5491 sec/batch\n",
      "Epoch 2/5  Iteration 204/890 Training loss: 2.1709 11.6519 sec/batch\n",
      "Epoch 2/5  Iteration 205/890 Training loss: 2.1682 11.7793 sec/batch\n",
      "Epoch 2/5  Iteration 206/890 Training loss: 2.1668 11.5343 sec/batch\n",
      "Epoch 2/5  Iteration 207/890 Training loss: 2.1660 11.7995 sec/batch\n",
      "Epoch 2/5  Iteration 208/890 Training loss: 2.1643 11.6756 sec/batch\n",
      "Epoch 2/5  Iteration 209/890 Training loss: 2.1628 11.7456 sec/batch\n",
      "Epoch 2/5  Iteration 210/890 Training loss: 2.1604 11.5589 sec/batch\n",
      "Epoch 2/5  Iteration 211/890 Training loss: 2.1582 11.5775 sec/batch\n",
      "Epoch 2/5  Iteration 212/890 Training loss: 2.1573 11.5817 sec/batch\n",
      "Epoch 2/5  Iteration 213/890 Training loss: 2.1555 11.7711 sec/batch\n",
      "Epoch 2/5  Iteration 214/890 Training loss: 2.1536 11.8393 sec/batch\n",
      "Epoch 2/5  Iteration 215/890 Training loss: 2.1519 11.8354 sec/batch\n",
      "Epoch 2/5  Iteration 216/890 Training loss: 2.1491 11.6743 sec/batch\n",
      "Epoch 2/5  Iteration 217/890 Training loss: 2.1464 11.6155 sec/batch\n",
      "Epoch 2/5  Iteration 218/890 Training loss: 2.1442 11.6164 sec/batch\n",
      "Epoch 2/5  Iteration 219/890 Training loss: 2.1420 11.8381 sec/batch\n",
      "Epoch 2/5  Iteration 220/890 Training loss: 2.1401 11.8062 sec/batch\n",
      "Epoch 2/5  Iteration 221/890 Training loss: 2.1380 11.6245 sec/batch\n",
      "Epoch 2/5  Iteration 222/890 Training loss: 2.1357 11.7214 sec/batch\n",
      "Epoch 2/5  Iteration 223/890 Training loss: 2.1341 11.6557 sec/batch\n",
      "Epoch 2/5  Iteration 224/890 Training loss: 2.1312 11.6063 sec/batch\n",
      "Epoch 2/5  Iteration 225/890 Training loss: 2.1296 11.6232 sec/batch\n",
      "Epoch 2/5  Iteration 226/890 Training loss: 2.1276 11.7395 sec/batch\n",
      "Epoch 2/5  Iteration 227/890 Training loss: 2.1258 11.5957 sec/batch\n",
      "Epoch 2/5  Iteration 228/890 Training loss: 2.1247 11.6192 sec/batch\n",
      "Epoch 2/5  Iteration 229/890 Training loss: 2.1225 11.6282 sec/batch\n",
      "Epoch 2/5  Iteration 230/890 Training loss: 2.1216 11.6609 sec/batch\n",
      "Epoch 2/5  Iteration 231/890 Training loss: 2.1197 11.4955 sec/batch\n",
      "Epoch 2/5  Iteration 232/890 Training loss: 2.1180 11.5392 sec/batch\n",
      "Epoch 2/5  Iteration 233/890 Training loss: 2.1163 11.6132 sec/batch\n",
      "Epoch 2/5  Iteration 234/890 Training loss: 2.1149 11.6072 sec/batch\n",
      "Epoch 2/5  Iteration 235/890 Training loss: 2.1135 11.7109 sec/batch\n",
      "Epoch 2/5  Iteration 236/890 Training loss: 2.1119 11.8215 sec/batch\n",
      "Epoch 2/5  Iteration 237/890 Training loss: 2.1100 11.7045 sec/batch\n",
      "Epoch 2/5  Iteration 238/890 Training loss: 2.1092 11.5727 sec/batch\n",
      "Epoch 2/5  Iteration 239/890 Training loss: 2.1077 11.6149 sec/batch\n",
      "Epoch 2/5  Iteration 240/890 Training loss: 2.1068 11.7673 sec/batch\n",
      "Epoch 2/5  Iteration 241/890 Training loss: 2.1058 12.8319 sec/batch\n",
      "Epoch 2/5  Iteration 242/890 Training loss: 2.1044 11.7243 sec/batch\n",
      "Epoch 2/5  Iteration 243/890 Training loss: 2.1030 11.7834 sec/batch\n",
      "Epoch 2/5  Iteration 244/890 Training loss: 2.1020 11.6617 sec/batch\n",
      "Epoch 2/5  Iteration 245/890 Training loss: 2.1006 11.6174 sec/batch\n",
      "Epoch 2/5  Iteration 246/890 Training loss: 2.0989 11.8903 sec/batch\n",
      "Epoch 2/5  Iteration 247/890 Training loss: 2.0975 11.6848 sec/batch\n",
      "Epoch 2/5  Iteration 248/890 Training loss: 2.0961 11.7053 sec/batch\n",
      "Epoch 2/5  Iteration 249/890 Training loss: 2.0952 11.8110 sec/batch\n",
      "Epoch 2/5  Iteration 250/890 Training loss: 2.0939 11.8917 sec/batch\n",
      "Epoch 2/5  Iteration 251/890 Training loss: 2.0928 11.8463 sec/batch\n",
      "Epoch 2/5  Iteration 252/890 Training loss: 2.0912 11.6292 sec/batch\n",
      "Epoch 2/5  Iteration 253/890 Training loss: 2.0898 11.6019 sec/batch\n",
      "Epoch 2/5  Iteration 254/890 Training loss: 2.0888 11.6824 sec/batch\n",
      "Epoch 2/5  Iteration 255/890 Training loss: 2.0873 11.5959 sec/batch\n",
      "Epoch 2/5  Iteration 256/890 Training loss: 2.0860 11.7696 sec/batch\n",
      "Epoch 2/5  Iteration 257/890 Training loss: 2.0843 12.1170 sec/batch\n",
      "Epoch 2/5  Iteration 258/890 Training loss: 2.0829 11.5494 sec/batch\n",
      "Epoch 2/5  Iteration 259/890 Training loss: 2.0811 11.5540 sec/batch\n",
      "Epoch 2/5  Iteration 260/890 Training loss: 2.0800 11.5611 sec/batch\n",
      "Epoch 2/5  Iteration 261/890 Training loss: 2.0783 11.6270 sec/batch\n",
      "Epoch 2/5  Iteration 262/890 Training loss: 2.0770 11.6097 sec/batch\n",
      "Epoch 2/5  Iteration 263/890 Training loss: 2.0753 11.6918 sec/batch\n",
      "Epoch 2/5  Iteration 264/890 Training loss: 2.0738 11.5164 sec/batch\n",
      "Epoch 2/5  Iteration 265/890 Training loss: 2.0724 11.6691 sec/batch\n",
      "Epoch 2/5  Iteration 266/890 Training loss: 2.0709 11.6120 sec/batch\n",
      "Epoch 2/5  Iteration 267/890 Training loss: 2.0693 11.8750 sec/batch\n",
      "Epoch 2/5  Iteration 268/890 Training loss: 2.0682 11.5506 sec/batch\n",
      "Epoch 2/5  Iteration 269/890 Training loss: 2.0668 11.6635 sec/batch\n",
      "Epoch 2/5  Iteration 270/890 Training loss: 2.0655 11.6634 sec/batch\n",
      "Epoch 2/5  Iteration 271/890 Training loss: 2.0638 11.6895 sec/batch\n",
      "Epoch 2/5  Iteration 272/890 Training loss: 2.0623 11.6849 sec/batch\n",
      "Epoch 2/5  Iteration 273/890 Training loss: 2.0608 11.6156 sec/batch\n",
      "Epoch 2/5  Iteration 274/890 Training loss: 2.0595 11.6040 sec/batch\n",
      "Epoch 2/5  Iteration 275/890 Training loss: 2.0582 11.6538 sec/batch\n",
      "Epoch 2/5  Iteration 276/890 Training loss: 2.0566 11.6081 sec/batch\n",
      "Epoch 2/5  Iteration 277/890 Training loss: 2.0550 11.9258 sec/batch\n",
      "Epoch 2/5  Iteration 278/890 Training loss: 2.0534 11.7897 sec/batch\n",
      "Epoch 2/5  Iteration 279/890 Training loss: 2.0523 11.6360 sec/batch\n",
      "Epoch 2/5  Iteration 280/890 Training loss: 2.0511 11.6241 sec/batch\n",
      "Epoch 2/5  Iteration 281/890 Training loss: 2.0497 11.7488 sec/batch\n",
      "Epoch 2/5  Iteration 282/890 Training loss: 2.0483 11.6167 sec/batch\n",
      "Epoch 2/5  Iteration 283/890 Training loss: 2.0468 11.6985 sec/batch\n",
      "Epoch 2/5  Iteration 284/890 Training loss: 2.0456 11.6221 sec/batch\n",
      "Epoch 2/5  Iteration 285/890 Training loss: 2.0444 11.6002 sec/batch\n",
      "Epoch 2/5  Iteration 286/890 Training loss: 2.0433 11.6319 sec/batch\n",
      "Epoch 2/5  Iteration 287/890 Training loss: 2.0422 11.6518 sec/batch\n",
      "Epoch 2/5  Iteration 288/890 Training loss: 2.0410 11.5893 sec/batch\n",
      "Epoch 2/5  Iteration 289/890 Training loss: 2.0398 11.5945 sec/batch\n",
      "Epoch 2/5  Iteration 290/890 Training loss: 2.0385 11.6567 sec/batch\n",
      "Epoch 2/5  Iteration 291/890 Training loss: 2.0373 11.6688 sec/batch\n",
      "Epoch 2/5  Iteration 292/890 Training loss: 2.0359 11.6266 sec/batch\n",
      "Epoch 2/5  Iteration 293/890 Training loss: 2.0345 11.6452 sec/batch\n",
      "Epoch 2/5  Iteration 294/890 Training loss: 2.0330 11.5875 sec/batch\n",
      "Epoch 2/5  Iteration 295/890 Training loss: 2.0318 11.7924 sec/batch\n",
      "Epoch 2/5  Iteration 296/890 Training loss: 2.0305 11.6281 sec/batch\n",
      "Epoch 2/5  Iteration 297/890 Training loss: 2.0294 11.7293 sec/batch\n",
      "Epoch 2/5  Iteration 298/890 Training loss: 2.0283 11.7851 sec/batch\n",
      "Epoch 2/5  Iteration 299/890 Training loss: 2.0271 11.6790 sec/batch\n",
      "Epoch 2/5  Iteration 300/890 Training loss: 2.0257 11.5942 sec/batch\n",
      "Epoch 2/5  Iteration 301/890 Training loss: 2.0244 11.7279 sec/batch\n",
      "Epoch 2/5  Iteration 302/890 Training loss: 2.0234 11.6294 sec/batch\n",
      "Epoch 2/5  Iteration 303/890 Training loss: 2.0222 11.7221 sec/batch\n",
      "Epoch 2/5  Iteration 304/890 Training loss: 2.0207 11.5228 sec/batch\n",
      "Epoch 2/5  Iteration 305/890 Training loss: 2.0197 11.6240 sec/batch\n",
      "Epoch 2/5  Iteration 306/890 Training loss: 2.0187 11.7582 sec/batch\n",
      "Epoch 2/5  Iteration 307/890 Training loss: 2.0176 11.5394 sec/batch\n",
      "Epoch 2/5  Iteration 308/890 Training loss: 2.0164 11.9008 sec/batch\n",
      "Epoch 2/5  Iteration 309/890 Training loss: 2.0151 11.6153 sec/batch\n",
      "Epoch 2/5  Iteration 310/890 Training loss: 2.0138 11.6812 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5  Iteration 311/890 Training loss: 2.0127 11.8114 sec/batch\n",
      "Epoch 2/5  Iteration 312/890 Training loss: 2.0117 11.6770 sec/batch\n",
      "Epoch 2/5  Iteration 313/890 Training loss: 2.0106 11.5499 sec/batch\n",
      "Epoch 2/5  Iteration 314/890 Training loss: 2.0095 11.6179 sec/batch\n",
      "Epoch 2/5  Iteration 315/890 Training loss: 2.0085 11.6836 sec/batch\n",
      "Epoch 2/5  Iteration 316/890 Training loss: 2.0075 11.5563 sec/batch\n",
      "Epoch 2/5  Iteration 317/890 Training loss: 2.0066 11.8022 sec/batch\n",
      "Epoch 2/5  Iteration 318/890 Training loss: 2.0055 11.9776 sec/batch\n",
      "Epoch 2/5  Iteration 319/890 Training loss: 2.0047 11.7246 sec/batch\n",
      "Epoch 2/5  Iteration 320/890 Training loss: 2.0036 11.5758 sec/batch\n",
      "Epoch 2/5  Iteration 321/890 Training loss: 2.0025 11.7774 sec/batch\n",
      "Epoch 2/5  Iteration 322/890 Training loss: 2.0015 11.5681 sec/batch\n",
      "Epoch 2/5  Iteration 323/890 Training loss: 2.0003 11.6130 sec/batch\n",
      "Epoch 2/5  Iteration 324/890 Training loss: 1.9995 11.7342 sec/batch\n",
      "Epoch 2/5  Iteration 325/890 Training loss: 1.9986 11.6479 sec/batch\n",
      "Epoch 2/5  Iteration 326/890 Training loss: 1.9978 11.6159 sec/batch\n",
      "Epoch 2/5  Iteration 327/890 Training loss: 1.9969 11.8473 sec/batch\n",
      "Epoch 2/5  Iteration 328/890 Training loss: 1.9958 11.9897 sec/batch\n",
      "Epoch 2/5  Iteration 329/890 Training loss: 1.9947 11.5931 sec/batch\n",
      "Epoch 2/5  Iteration 330/890 Training loss: 1.9939 11.6558 sec/batch\n",
      "Epoch 2/5  Iteration 331/890 Training loss: 1.9930 11.5378 sec/batch\n",
      "Epoch 2/5  Iteration 332/890 Training loss: 1.9921 11.6072 sec/batch\n",
      "Epoch 2/5  Iteration 333/890 Training loss: 1.9911 11.5971 sec/batch\n",
      "Epoch 2/5  Iteration 334/890 Training loss: 1.9901 11.6120 sec/batch\n",
      "Epoch 2/5  Iteration 335/890 Training loss: 1.9892 11.6363 sec/batch\n",
      "Epoch 2/5  Iteration 336/890 Training loss: 1.9881 11.7465 sec/batch\n",
      "Epoch 2/5  Iteration 337/890 Training loss: 1.9869 11.5833 sec/batch\n",
      "Epoch 2/5  Iteration 338/890 Training loss: 1.9861 11.6366 sec/batch\n",
      "Epoch 2/5  Iteration 339/890 Training loss: 1.9853 11.8309 sec/batch\n",
      "Epoch 2/5  Iteration 340/890 Training loss: 1.9843 11.6789 sec/batch\n",
      "Epoch 2/5  Iteration 341/890 Training loss: 1.9834 11.7139 sec/batch\n",
      "Epoch 2/5  Iteration 342/890 Training loss: 1.9824 11.5811 sec/batch\n",
      "Epoch 2/5  Iteration 343/890 Training loss: 1.9815 11.5596 sec/batch\n",
      "Epoch 2/5  Iteration 344/890 Training loss: 1.9804 11.7838 sec/batch\n",
      "Epoch 2/5  Iteration 345/890 Training loss: 1.9796 11.8105 sec/batch\n",
      "Epoch 2/5  Iteration 346/890 Training loss: 1.9789 11.6863 sec/batch\n",
      "Epoch 2/5  Iteration 347/890 Training loss: 1.9779 11.7074 sec/batch\n",
      "Epoch 2/5  Iteration 348/890 Training loss: 1.9769 11.5295 sec/batch\n",
      "Epoch 2/5  Iteration 349/890 Training loss: 1.9759 11.7923 sec/batch\n",
      "Epoch 2/5  Iteration 350/890 Training loss: 1.9748 11.6170 sec/batch\n",
      "Epoch 2/5  Iteration 351/890 Training loss: 1.9739 11.6872 sec/batch\n",
      "Epoch 2/5  Iteration 352/890 Training loss: 1.9730 11.6078 sec/batch\n",
      "Epoch 2/5  Iteration 353/890 Training loss: 1.9722 11.5774 sec/batch\n",
      "Epoch 2/5  Iteration 354/890 Training loss: 1.9712 11.6126 sec/batch\n",
      "Epoch 2/5  Iteration 355/890 Training loss: 1.9701 11.8517 sec/batch\n",
      "Epoch 2/5  Iteration 356/890 Training loss: 1.9693 11.8393 sec/batch\n",
      "Epoch 3/5  Iteration 357/890 Training loss: 1.8605 11.6549 sec/batch\n",
      "Epoch 3/5  Iteration 358/890 Training loss: 1.8276 11.5923 sec/batch\n",
      "Epoch 3/5  Iteration 359/890 Training loss: 1.8155 11.6884 sec/batch\n",
      "Epoch 3/5  Iteration 360/890 Training loss: 1.8101 11.5943 sec/batch\n",
      "Epoch 3/5  Iteration 361/890 Training loss: 1.8066 11.6234 sec/batch\n",
      "Epoch 3/5  Iteration 362/890 Training loss: 1.7960 11.6133 sec/batch\n",
      "Epoch 3/5  Iteration 363/890 Training loss: 1.7953 11.6263 sec/batch\n",
      "Epoch 3/5  Iteration 364/890 Training loss: 1.7943 11.6549 sec/batch\n",
      "Epoch 3/5  Iteration 365/890 Training loss: 1.7973 11.8651 sec/batch\n",
      "Epoch 3/5  Iteration 366/890 Training loss: 1.7959 11.5821 sec/batch\n",
      "Epoch 3/5  Iteration 367/890 Training loss: 1.7917 11.7258 sec/batch\n",
      "Epoch 3/5  Iteration 368/890 Training loss: 1.7894 11.5835 sec/batch\n",
      "Epoch 3/5  Iteration 369/890 Training loss: 1.7888 11.7475 sec/batch\n",
      "Epoch 3/5  Iteration 370/890 Training loss: 1.7905 11.7232 sec/batch\n",
      "Epoch 3/5  Iteration 371/890 Training loss: 1.7892 11.6065 sec/batch\n",
      "Epoch 3/5  Iteration 372/890 Training loss: 1.7867 11.6226 sec/batch\n",
      "Epoch 3/5  Iteration 373/890 Training loss: 1.7860 11.6262 sec/batch\n",
      "Epoch 3/5  Iteration 374/890 Training loss: 1.7876 11.7397 sec/batch\n",
      "Epoch 3/5  Iteration 375/890 Training loss: 1.7865 11.6319 sec/batch\n",
      "Epoch 3/5  Iteration 376/890 Training loss: 1.7861 11.4391 sec/batch\n",
      "Epoch 3/5  Iteration 377/890 Training loss: 1.7849 11.5560 sec/batch\n",
      "Epoch 3/5  Iteration 378/890 Training loss: 1.7855 11.6426 sec/batch\n",
      "Epoch 3/5  Iteration 379/890 Training loss: 1.7840 11.6956 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 5#20\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "for lstm_size in [128,256,512]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for learning_rate in [0.002, 0.001]:\n",
    "            log_string = 'logs/4/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n",
    "            writer = tf.summary.FileWriter(log_string)\n",
    "            model = build_rnn(len(vocab), \n",
    "                    batch_size=batch_size,\n",
    "                    num_steps=num_steps,\n",
    "                    learning_rate=learning_rate,\n",
    "                    lstm_size=lstm_size,\n",
    "                    num_layers=num_layers)\n",
    "            \n",
    "            train(model, epochs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlathit that if had so\n",
      "like it that it were. He could not trouble to his wife, and there was\n",
      "anything in them of the side of his weaky in the creature at his forteren\n",
      "to him.\n",
      "\n",
      "\"What is it? I can't bread to those,\" said Stepan Arkadyevitch. \"It's not\n",
      "my children, and there is an almost this arm, true it mays already,\n",
      "and tell you what I have say to you, and was not looking at the peasant,\n",
      "why is, I don't know him out, and she doesn't speak to me immediately, as\n",
      "you would say the countess and the more frest an angelembre, and time and\n",
      "things's silent, but I was not in my stand that is in my head. But if he\n",
      "say, and was so feeling with his soul. A child--in his soul of his\n",
      "soul of his soul. He should not see that any of that sense of. Here he\n",
      "had not been so composed and to speak for as in a whole picture, but\n",
      "all the setting and her excellent and society, who had been delighted\n",
      "and see to anywing had been being troed to thousand words on them,\n",
      "we liked him.\n",
      "\n",
      "That set in her money at the table, he came into the party. The capable\n",
      "of his she could not be as an old composure.\n",
      "\n",
      "\"That's all something there will be down becime by throe is\n",
      "such a silent, as in a countess, I should state it out and divorct.\n",
      "The discussion is not for me. I was that something was simply they are\n",
      "all three manshess of a sensitions of mind it all.\"\n",
      "\n",
      "\"No,\" he thought, shouted and lifting his soul. \"While it might see your\n",
      "honser and she, I could burst. And I had been a midelity. And I had a\n",
      "marnief are through the countess,\" he said, looking at him, a chosing\n",
      "which they had been carried out and still solied, and there was a sen that\n",
      "was to be completely, and that this matter of all the seconds of it, and\n",
      "a concipation were to her husband, who came up and conscaously, that he\n",
      "was not the station. All his fourse she was always at the country,,\n",
      "to speak oft, and though they were to hear the delightful throom and\n",
      "whether they came towards the morning, and his living and a coller and\n",
      "hold--the children. \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farnt him oste wha sorind thans tout thint asd an sesand an hires on thime sind thit aled, ban thand and out hore as the ter hos ton ho te that, was tis tart al the hand sostint him sore an tit an son thes, win he se ther san ther hher tas tarereng,.\n",
      "\n",
      "Anl at an ades in ond hesiln, ad hhe torers teans, wast tar arering tho this sos alten sorer has hhas an siton ther him he had sin he ard ate te anling the sosin her ans and\n",
      "arins asd and ther ale te tot an tand tanginge wath and ho ald, so sot th asend sat hare sother horesinnd, he hesense wing ante her so tith tir sherinn, anded and to the toul anderin he sorit he torsith she se atere an ting ot hand and thit hhe so the te wile har\n",
      "ens ont in the sersise, and we he seres tar aterer, to ato tat or has he he wan ton here won and sen heren he sosering, to to theer oo adent har herere the wosh oute, was serild ward tous hed astend..\n",
      "\n",
      "I's sint on alt in har tor tit her asd hade shithans ored he talereng an soredendere tim tot hees. Tise sor and \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard as astice her said he celatice of to seress in the raice, and to be the some and sere allats to that said to that the sark and a cast a the wither ald the pacinesse of her had astition, he said to the sount as she west at hissele. Af the cond it he was a fact onthis astisarianing.\n",
      "\n",
      "\n",
      "\"Or a ton to to be that's a more at aspestale as the sont of anstiring as\n",
      "thours and trey.\n",
      "\n",
      "The same wo dangring the\n",
      "raterst, who sore and somethy had ast out an of his book. \"We had's beane were that, and a morted a thay he had to tere. Then to\n",
      "her homent andertersed his his ancouted to the pirsted, the soution for of the pirsice inthirgest and stenciol, with the hard and and\n",
      "a colrice of to be oneres,\n",
      "the song to this anderssad.\n",
      "The could ounterss the said to serom of\n",
      "soment a carsed of sheres of she\n",
      "torded\n",
      "har and want in their of hould, but\n",
      "her told in that in he tad a the same to her. Serghing an her has and with the seed, and the camt ont his about of the\n",
      "sail, the her then all houg ant or to hus to \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrat, his felt has at it.\n",
      "\n",
      "\"When the pose ther hor exceed\n",
      "to his sheant was,\" weat a sime of his sounsed. The coment and the facily that which had began terede a marilicaly whice whether the pose of his hand, at she was alligated herself the same on she had to\n",
      "taiking to his forthing and streath how to hand\n",
      "began in a lang at some at it, this he cholded not set all her. \"Wo love that is setthing. Him anstering as seen that.\"\n",
      "\n",
      "\"Yes in the man that say the mare a crances is it?\" said Sergazy Ivancatching. \"You doon think were somether is ifficult of a mone of\n",
      "though the most at the countes that the\n",
      "mean on the come to say the most, to\n",
      "his feesing of\n",
      "a man she, whilo he\n",
      "sained and well, that he would still at to said. He wind at his for the sore in the most\n",
      "of hoss and almoved to see him. They have betine the sumper into at he his stire, and what he was that at the so steate of the\n",
      "sound, and shin should have a geest of shall feet on the conderation to she had been at that imporsing the dre\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
