{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([81,  7, 47, 54, 43,  0,  1, 58, 60, 53, 53, 53, 26, 47, 54, 54, 42,\n",
       "       58, 61, 47, 49, 52, 66, 52,  0, 67, 58, 47,  1,  0, 58, 47, 66, 66,\n",
       "       58, 47, 66, 52, 57,  0, 40, 58,  0, 68,  0,  1, 42, 58, 27, 23,  7,\n",
       "       47, 54, 54, 42, 58, 61, 47, 49, 52, 66, 42, 58, 52, 67, 58, 27, 23,\n",
       "        7, 47, 54, 54, 42, 58, 52, 23, 58, 52, 43, 67, 58, 50, 80, 23, 53,\n",
       "       80, 47, 42, 33, 53, 53, 11, 68,  0,  1, 42, 43,  7, 52, 23], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[81,  7, 47, 54, 43,  0,  1, 58, 60, 53],\n",
       "       [ 3, 23, 71, 58,  7,  0, 58, 49, 50, 68],\n",
       "       [58, 59, 47, 43, 59,  7, 52, 23, 29, 58],\n",
       "       [50, 43,  7,  0,  1, 58, 80, 50, 27, 66],\n",
       "       [58, 43,  7,  0, 58, 66, 47, 23, 71, 41],\n",
       "       [58, 78,  7,  1, 50, 27, 29,  7, 58, 66],\n",
       "       [43, 58, 43, 50, 53, 71, 50, 33, 53, 53],\n",
       "       [50, 58,  7,  0,  1, 67,  0, 66, 61, 25],\n",
       "       [ 7, 47, 43, 58, 52, 67, 58, 43,  7,  0],\n",
       "       [ 0,  1, 67,  0, 66, 61, 58, 47, 23, 71]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(lstm_size),\n",
    "                        output_keep_prob=keep_prob) for _ in range(num_layers)])    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 1/1780 Training loss: 4.4201 13.0641 sec/batch\n",
      "Epoch 1/10  Iteration 2/1780 Training loss: 4.3806 11.9964 sec/batch\n",
      "Epoch 1/10  Iteration 3/1780 Training loss: 4.2447 10.1318 sec/batch\n",
      "Epoch 1/10  Iteration 4/1780 Training loss: 4.4902 9.6179 sec/batch\n",
      "Epoch 1/10  Iteration 5/1780 Training loss: 4.4368 9.4478 sec/batch\n",
      "Epoch 1/10  Iteration 6/1780 Training loss: 4.3105 9.2586 sec/batch\n",
      "Epoch 1/10  Iteration 7/1780 Training loss: 4.1999 9.5176 sec/batch\n",
      "Epoch 1/10  Iteration 8/1780 Training loss: 4.1125 9.7781 sec/batch\n",
      "Epoch 1/10  Iteration 9/1780 Training loss: 4.0384 10.1461 sec/batch\n",
      "Epoch 1/10  Iteration 10/1780 Training loss: 3.9767 10.4247 sec/batch\n",
      "Epoch 1/10  Iteration 11/1780 Training loss: 3.9197 10.7697 sec/batch\n",
      "Epoch 1/10  Iteration 12/1780 Training loss: 3.8711 10.8080 sec/batch\n",
      "Epoch 1/10  Iteration 13/1780 Training loss: 3.8286 11.0134 sec/batch\n",
      "Epoch 1/10  Iteration 14/1780 Training loss: 3.7907 11.0898 sec/batch\n",
      "Epoch 1/10  Iteration 15/1780 Training loss: 3.7574 11.1144 sec/batch\n",
      "Epoch 1/10  Iteration 16/1780 Training loss: 3.7272 11.1337 sec/batch\n",
      "Epoch 1/10  Iteration 17/1780 Training loss: 3.6993 11.2865 sec/batch\n",
      "Epoch 1/10  Iteration 18/1780 Training loss: 3.6760 11.3477 sec/batch\n",
      "Epoch 1/10  Iteration 19/1780 Training loss: 3.6536 11.4291 sec/batch\n",
      "Epoch 1/10  Iteration 20/1780 Training loss: 3.6317 11.5759 sec/batch\n",
      "Epoch 1/10  Iteration 21/1780 Training loss: 3.6126 11.5312 sec/batch\n",
      "Epoch 1/10  Iteration 22/1780 Training loss: 3.5946 11.6066 sec/batch\n",
      "Epoch 1/10  Iteration 23/1780 Training loss: 3.5778 11.6915 sec/batch\n",
      "Epoch 1/10  Iteration 24/1780 Training loss: 3.5624 11.5869 sec/batch\n",
      "Epoch 1/10  Iteration 25/1780 Training loss: 3.5482 11.7630 sec/batch\n",
      "Epoch 1/10  Iteration 26/1780 Training loss: 3.5352 11.6446 sec/batch\n",
      "Epoch 1/10  Iteration 27/1780 Training loss: 3.5235 11.7515 sec/batch\n",
      "Epoch 1/10  Iteration 28/1780 Training loss: 3.5114 11.6207 sec/batch\n",
      "Epoch 1/10  Iteration 29/1780 Training loss: 3.5003 11.9177 sec/batch\n",
      "Epoch 1/10  Iteration 30/1780 Training loss: 3.4899 12.2837 sec/batch\n",
      "Epoch 1/10  Iteration 31/1780 Training loss: 3.4811 12.4949 sec/batch\n",
      "Epoch 1/10  Iteration 32/1780 Training loss: 3.4717 12.1331 sec/batch\n",
      "Epoch 1/10  Iteration 33/1780 Training loss: 3.4627 12.0908 sec/batch\n",
      "Epoch 1/10  Iteration 34/1780 Training loss: 3.4548 12.0715 sec/batch\n",
      "Epoch 1/10  Iteration 35/1780 Training loss: 3.4468 12.2741 sec/batch\n",
      "Epoch 1/10  Iteration 36/1780 Training loss: 3.4397 12.0749 sec/batch\n",
      "Epoch 1/10  Iteration 37/1780 Training loss: 3.4320 12.3162 sec/batch\n",
      "Epoch 1/10  Iteration 38/1780 Training loss: 3.4248 12.2177 sec/batch\n",
      "Epoch 1/10  Iteration 39/1780 Training loss: 3.4178 12.2152 sec/batch\n",
      "Epoch 1/10  Iteration 40/1780 Training loss: 3.4113 12.5342 sec/batch\n",
      "Epoch 1/10  Iteration 41/1780 Training loss: 3.4048 12.2709 sec/batch\n",
      "Epoch 1/10  Iteration 42/1780 Training loss: 3.3989 12.3423 sec/batch\n",
      "Epoch 1/10  Iteration 43/1780 Training loss: 3.3931 12.3350 sec/batch\n",
      "Epoch 1/10  Iteration 44/1780 Training loss: 3.3875 12.3174 sec/batch\n",
      "Epoch 1/10  Iteration 45/1780 Training loss: 3.3820 12.2835 sec/batch\n",
      "Epoch 1/10  Iteration 46/1780 Training loss: 3.3771 12.1189 sec/batch\n",
      "Epoch 1/10  Iteration 47/1780 Training loss: 3.3724 12.4538 sec/batch\n",
      "Epoch 1/10  Iteration 48/1780 Training loss: 3.3680 12.4103 sec/batch\n",
      "Epoch 1/10  Iteration 49/1780 Training loss: 3.3638 12.4085 sec/batch\n",
      "Epoch 1/10  Iteration 50/1780 Training loss: 3.3597 12.2366 sec/batch\n",
      "Epoch 1/10  Iteration 51/1780 Training loss: 3.3555 12.2374 sec/batch\n",
      "Epoch 1/10  Iteration 52/1780 Training loss: 3.3515 12.3250 sec/batch\n",
      "Epoch 1/10  Iteration 53/1780 Training loss: 3.3478 12.7799 sec/batch\n",
      "Epoch 1/10  Iteration 54/1780 Training loss: 3.3439 12.0920 sec/batch\n",
      "Epoch 1/10  Iteration 55/1780 Training loss: 3.3404 11.1612 sec/batch\n",
      "Epoch 1/10  Iteration 56/1780 Training loss: 3.3366 12.0022 sec/batch\n",
      "Epoch 1/10  Iteration 57/1780 Training loss: 3.3330 15.0366 sec/batch\n",
      "Epoch 1/10  Iteration 58/1780 Training loss: 3.3296 14.5921 sec/batch\n",
      "Epoch 1/10  Iteration 59/1780 Training loss: 3.3261 14.3903 sec/batch\n",
      "Epoch 1/10  Iteration 60/1780 Training loss: 3.3230 14.3488 sec/batch\n",
      "Epoch 1/10  Iteration 61/1780 Training loss: 3.3199 14.5218 sec/batch\n",
      "Epoch 1/10  Iteration 62/1780 Training loss: 3.3172 14.3205 sec/batch\n",
      "Epoch 1/10  Iteration 63/1780 Training loss: 3.3146 14.3919 sec/batch\n",
      "Epoch 1/10  Iteration 64/1780 Training loss: 3.3114 14.3816 sec/batch\n",
      "Epoch 1/10  Iteration 65/1780 Training loss: 3.3083 14.4286 sec/batch\n",
      "Epoch 1/10  Iteration 66/1780 Training loss: 3.3058 14.4455 sec/batch\n",
      "Epoch 1/10  Iteration 67/1780 Training loss: 3.3033 14.3384 sec/batch\n",
      "Epoch 1/10  Iteration 68/1780 Training loss: 3.3001 14.3642 sec/batch\n",
      "Epoch 1/10  Iteration 69/1780 Training loss: 3.2973 14.4984 sec/batch\n",
      "Epoch 1/10  Iteration 70/1780 Training loss: 3.2950 14.4792 sec/batch\n",
      "Epoch 1/10  Iteration 71/1780 Training loss: 3.2924 16.4663 sec/batch\n",
      "Epoch 1/10  Iteration 72/1780 Training loss: 3.2902 18.9174 sec/batch\n",
      "Epoch 1/10  Iteration 73/1780 Training loss: 3.2879 18.5287 sec/batch\n",
      "Epoch 1/10  Iteration 74/1780 Training loss: 3.2855 16.0810 sec/batch\n",
      "Epoch 1/10  Iteration 75/1780 Training loss: 3.2832 14.8776 sec/batch\n",
      "Epoch 1/10  Iteration 76/1780 Training loss: 3.2811 12.4328 sec/batch\n",
      "Epoch 1/10  Iteration 77/1780 Training loss: 3.2788 13.7748 sec/batch\n",
      "Epoch 1/10  Iteration 78/1780 Training loss: 3.2767 16.8782 sec/batch\n",
      "Epoch 1/10  Iteration 79/1780 Training loss: 3.2744 13.1143 sec/batch\n",
      "Epoch 1/10  Iteration 80/1780 Training loss: 3.2721 15.9450 sec/batch\n",
      "Epoch 1/10  Iteration 81/1780 Training loss: 3.2700 15.5359 sec/batch\n",
      "Epoch 1/10  Iteration 82/1780 Training loss: 3.2680 17.7271 sec/batch\n",
      "Epoch 1/10  Iteration 83/1780 Training loss: 3.2660 17.8715 sec/batch\n",
      "Epoch 1/10  Iteration 84/1780 Training loss: 3.2640 17.9808 sec/batch\n",
      "Epoch 1/10  Iteration 85/1780 Training loss: 3.2618 18.9606 sec/batch\n",
      "Epoch 1/10  Iteration 86/1780 Training loss: 3.2598 15.4944 sec/batch\n",
      "Epoch 1/10  Iteration 87/1780 Training loss: 3.2576 17.4655 sec/batch\n",
      "Epoch 1/10  Iteration 88/1780 Training loss: 3.2555 13.5364 sec/batch\n",
      "Epoch 1/10  Iteration 89/1780 Training loss: 3.2537 13.3069 sec/batch\n",
      "Epoch 1/10  Iteration 90/1780 Training loss: 3.2517 13.9232 sec/batch\n",
      "Epoch 1/10  Iteration 91/1780 Training loss: 3.2497 15.1050 sec/batch\n",
      "Epoch 1/10  Iteration 92/1780 Training loss: 3.2477 14.2732 sec/batch\n",
      "Epoch 1/10  Iteration 93/1780 Training loss: 3.2456 13.8492 sec/batch\n",
      "Epoch 1/10  Iteration 94/1780 Training loss: 3.2463 16.5926 sec/batch\n",
      "Epoch 1/10  Iteration 95/1780 Training loss: 3.2458 17.0421 sec/batch\n",
      "Epoch 1/10  Iteration 96/1780 Training loss: 3.2442 17.6329 sec/batch\n",
      "Epoch 1/10  Iteration 97/1780 Training loss: 3.2426 15.3951 sec/batch\n",
      "Epoch 1/10  Iteration 98/1780 Training loss: 3.2407 14.7788 sec/batch\n",
      "Epoch 1/10  Iteration 99/1780 Training loss: 3.2391 16.1656 sec/batch\n",
      "Epoch 1/10  Iteration 100/1780 Training loss: 3.2375 19.4093 sec/batch\n",
      "Validation loss: 3.02944 Saving checkpoint!\n",
      "Epoch 1/10  Iteration 101/1780 Training loss: 3.2358 15.8161 sec/batch\n",
      "Epoch 1/10  Iteration 102/1780 Training loss: 3.2342 14.8874 sec/batch\n",
      "Epoch 1/10  Iteration 103/1780 Training loss: 3.2325 15.3187 sec/batch\n",
      "Epoch 1/10  Iteration 104/1780 Training loss: 3.2309 16.0665 sec/batch\n",
      "Epoch 1/10  Iteration 105/1780 Training loss: 3.2292 18.7635 sec/batch\n",
      "Epoch 1/10  Iteration 106/1780 Training loss: 3.2275 14.6492 sec/batch\n",
      "Epoch 1/10  Iteration 107/1780 Training loss: 3.2257 14.4256 sec/batch\n",
      "Epoch 1/10  Iteration 108/1780 Training loss: 3.2239 13.8163 sec/batch\n",
      "Epoch 1/10  Iteration 109/1780 Training loss: 3.2223 16.5614 sec/batch\n",
      "Epoch 1/10  Iteration 110/1780 Training loss: 3.2203 14.3379 sec/batch\n",
      "Epoch 1/10  Iteration 111/1780 Training loss: 3.2185 15.8235 sec/batch\n",
      "Epoch 1/10  Iteration 112/1780 Training loss: 3.2167 15.6358 sec/batch\n",
      "Epoch 1/10  Iteration 113/1780 Training loss: 3.2149 15.2496 sec/batch\n",
      "Epoch 1/10  Iteration 114/1780 Training loss: 3.2130 14.7656 sec/batch\n",
      "Epoch 1/10  Iteration 115/1780 Training loss: 3.2110 13.2128 sec/batch\n",
      "Epoch 1/10  Iteration 116/1780 Training loss: 3.2091 13.1311 sec/batch\n",
      "Epoch 1/10  Iteration 117/1780 Training loss: 3.2071 12.1564 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 118/1780 Training loss: 3.2053 11.1554 sec/batch\n",
      "Epoch 1/10  Iteration 119/1780 Training loss: 3.2035 11.2530 sec/batch\n",
      "Epoch 1/10  Iteration 120/1780 Training loss: 3.2015 11.3239 sec/batch\n",
      "Epoch 1/10  Iteration 121/1780 Training loss: 3.2008 11.2427 sec/batch\n",
      "Epoch 1/10  Iteration 122/1780 Training loss: 3.2002 11.2076 sec/batch\n",
      "Epoch 1/10  Iteration 123/1780 Training loss: 3.1983 11.1959 sec/batch\n",
      "Epoch 1/10  Iteration 124/1780 Training loss: 3.1966 11.1456 sec/batch\n",
      "Epoch 1/10  Iteration 125/1780 Training loss: 3.1945 11.2481 sec/batch\n",
      "Epoch 1/10  Iteration 126/1780 Training loss: 3.1924 11.2236 sec/batch\n",
      "Epoch 1/10  Iteration 127/1780 Training loss: 3.1906 11.2288 sec/batch\n",
      "Epoch 1/10  Iteration 128/1780 Training loss: 3.1887 11.2023 sec/batch\n",
      "Epoch 1/10  Iteration 129/1780 Training loss: 3.1865 11.1457 sec/batch\n",
      "Epoch 1/10  Iteration 130/1780 Training loss: 3.1844 11.1483 sec/batch\n",
      "Epoch 1/10  Iteration 131/1780 Training loss: 3.1825 11.1763 sec/batch\n",
      "Epoch 1/10  Iteration 132/1780 Training loss: 3.1802 11.3310 sec/batch\n",
      "Epoch 1/10  Iteration 133/1780 Training loss: 3.1780 11.2442 sec/batch\n",
      "Epoch 1/10  Iteration 134/1780 Training loss: 3.1757 11.1869 sec/batch\n",
      "Epoch 1/10  Iteration 135/1780 Training loss: 3.1732 11.2481 sec/batch\n",
      "Epoch 1/10  Iteration 136/1780 Training loss: 3.1707 11.1758 sec/batch\n",
      "Epoch 1/10  Iteration 137/1780 Training loss: 3.1682 11.2002 sec/batch\n",
      "Epoch 1/10  Iteration 138/1780 Training loss: 3.1659 11.2277 sec/batch\n",
      "Epoch 1/10  Iteration 139/1780 Training loss: 3.1636 11.2211 sec/batch\n",
      "Epoch 1/10  Iteration 140/1780 Training loss: 3.1612 11.2634 sec/batch\n",
      "Epoch 1/10  Iteration 141/1780 Training loss: 3.1588 11.5392 sec/batch\n",
      "Epoch 1/10  Iteration 142/1780 Training loss: 3.1564 11.1915 sec/batch\n",
      "Epoch 1/10  Iteration 143/1780 Training loss: 3.1538 11.2790 sec/batch\n",
      "Epoch 1/10  Iteration 144/1780 Training loss: 3.1513 11.2073 sec/batch\n",
      "Epoch 1/10  Iteration 145/1780 Training loss: 3.1488 11.3035 sec/batch\n",
      "Epoch 1/10  Iteration 146/1780 Training loss: 3.1463 11.2735 sec/batch\n",
      "Epoch 1/10  Iteration 147/1780 Training loss: 3.1439 11.2064 sec/batch\n",
      "Epoch 1/10  Iteration 148/1780 Training loss: 3.1414 11.2128 sec/batch\n",
      "Epoch 1/10  Iteration 149/1780 Training loss: 3.1387 11.2220 sec/batch\n",
      "Epoch 1/10  Iteration 150/1780 Training loss: 3.1360 11.2542 sec/batch\n",
      "Epoch 1/10  Iteration 151/1780 Training loss: 3.1334 11.1724 sec/batch\n",
      "Epoch 1/10  Iteration 152/1780 Training loss: 3.1310 11.2276 sec/batch\n",
      "Epoch 1/10  Iteration 153/1780 Training loss: 3.1283 11.2312 sec/batch\n",
      "Epoch 1/10  Iteration 154/1780 Training loss: 3.1256 11.2152 sec/batch\n",
      "Epoch 1/10  Iteration 155/1780 Training loss: 3.1228 11.1910 sec/batch\n",
      "Epoch 1/10  Iteration 156/1780 Training loss: 3.1200 11.2008 sec/batch\n",
      "Epoch 1/10  Iteration 157/1780 Training loss: 3.1171 11.2575 sec/batch\n",
      "Epoch 1/10  Iteration 158/1780 Training loss: 3.1141 11.1243 sec/batch\n",
      "Epoch 1/10  Iteration 159/1780 Training loss: 3.1110 11.2103 sec/batch\n",
      "Epoch 1/10  Iteration 160/1780 Training loss: 3.1082 11.2170 sec/batch\n",
      "Epoch 1/10  Iteration 161/1780 Training loss: 3.1053 11.2637 sec/batch\n",
      "Epoch 1/10  Iteration 162/1780 Training loss: 3.1023 11.2286 sec/batch\n",
      "Epoch 1/10  Iteration 163/1780 Training loss: 3.0992 11.2022 sec/batch\n",
      "Epoch 1/10  Iteration 164/1780 Training loss: 3.0963 11.3576 sec/batch\n",
      "Epoch 1/10  Iteration 165/1780 Training loss: 3.0934 11.2571 sec/batch\n",
      "Epoch 1/10  Iteration 166/1780 Training loss: 3.0903 11.2720 sec/batch\n",
      "Epoch 1/10  Iteration 167/1780 Training loss: 3.0874 11.2089 sec/batch\n",
      "Epoch 1/10  Iteration 168/1780 Training loss: 3.0844 11.1855 sec/batch\n",
      "Epoch 1/10  Iteration 169/1780 Training loss: 3.0815 11.2024 sec/batch\n",
      "Epoch 1/10  Iteration 170/1780 Training loss: 3.0785 11.2537 sec/batch\n",
      "Epoch 1/10  Iteration 171/1780 Training loss: 3.0755 11.3284 sec/batch\n",
      "Epoch 1/10  Iteration 172/1780 Training loss: 3.0727 11.2299 sec/batch\n",
      "Epoch 1/10  Iteration 173/1780 Training loss: 3.0699 11.2301 sec/batch\n",
      "Epoch 1/10  Iteration 174/1780 Training loss: 3.0672 11.2048 sec/batch\n",
      "Epoch 1/10  Iteration 175/1780 Training loss: 3.0644 11.2848 sec/batch\n",
      "Epoch 1/10  Iteration 176/1780 Training loss: 3.0614 11.1422 sec/batch\n",
      "Epoch 1/10  Iteration 177/1780 Training loss: 3.0584 11.1765 sec/batch\n",
      "Epoch 1/10  Iteration 178/1780 Training loss: 3.0554 11.1739 sec/batch\n",
      "Epoch 2/10  Iteration 179/1780 Training loss: 2.5931 11.2102 sec/batch\n",
      "Epoch 2/10  Iteration 180/1780 Training loss: 2.5479 11.1594 sec/batch\n",
      "Epoch 2/10  Iteration 181/1780 Training loss: 2.5349 11.2480 sec/batch\n",
      "Epoch 2/10  Iteration 182/1780 Training loss: 2.5324 11.2305 sec/batch\n",
      "Epoch 2/10  Iteration 183/1780 Training loss: 2.5279 11.2631 sec/batch\n",
      "Epoch 2/10  Iteration 184/1780 Training loss: 2.5237 11.1831 sec/batch\n",
      "Epoch 2/10  Iteration 185/1780 Training loss: 2.5213 11.2040 sec/batch\n",
      "Epoch 2/10  Iteration 186/1780 Training loss: 2.5206 11.3343 sec/batch\n",
      "Epoch 2/10  Iteration 187/1780 Training loss: 2.5205 11.1428 sec/batch\n",
      "Epoch 2/10  Iteration 188/1780 Training loss: 2.5176 11.0721 sec/batch\n",
      "Epoch 2/10  Iteration 189/1780 Training loss: 2.5150 11.2327 sec/batch\n",
      "Epoch 2/10  Iteration 190/1780 Training loss: 2.5134 11.1604 sec/batch\n",
      "Epoch 2/10  Iteration 191/1780 Training loss: 2.5115 11.2590 sec/batch\n",
      "Epoch 2/10  Iteration 192/1780 Training loss: 2.5123 11.2534 sec/batch\n",
      "Epoch 2/10  Iteration 193/1780 Training loss: 2.5114 11.1774 sec/batch\n",
      "Epoch 2/10  Iteration 194/1780 Training loss: 2.5097 11.2169 sec/batch\n",
      "Epoch 2/10  Iteration 195/1780 Training loss: 2.5082 11.2714 sec/batch\n",
      "Epoch 2/10  Iteration 196/1780 Training loss: 2.5086 11.2851 sec/batch\n",
      "Epoch 2/10  Iteration 197/1780 Training loss: 2.5074 11.1681 sec/batch\n",
      "Epoch 2/10  Iteration 198/1780 Training loss: 2.5045 11.2632 sec/batch\n",
      "Epoch 2/10  Iteration 199/1780 Training loss: 2.5023 11.2357 sec/batch\n",
      "Epoch 2/10  Iteration 200/1780 Training loss: 2.5025 11.2125 sec/batch\n",
      "Validation loss: 2.38312 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 201/1780 Training loss: 2.5018 11.2382 sec/batch\n",
      "Epoch 2/10  Iteration 202/1780 Training loss: 2.4998 11.2245 sec/batch\n",
      "Epoch 2/10  Iteration 203/1780 Training loss: 2.4977 11.2129 sec/batch\n",
      "Epoch 2/10  Iteration 204/1780 Training loss: 2.4960 11.2547 sec/batch\n",
      "Epoch 2/10  Iteration 205/1780 Training loss: 2.4942 11.2115 sec/batch\n",
      "Epoch 2/10  Iteration 206/1780 Training loss: 2.4925 11.2718 sec/batch\n",
      "Epoch 2/10  Iteration 207/1780 Training loss: 2.4912 11.2512 sec/batch\n",
      "Epoch 2/10  Iteration 208/1780 Training loss: 2.4897 11.2005 sec/batch\n",
      "Epoch 2/10  Iteration 209/1780 Training loss: 2.4887 11.2690 sec/batch\n",
      "Epoch 2/10  Iteration 210/1780 Training loss: 2.4868 11.1404 sec/batch\n",
      "Epoch 2/10  Iteration 211/1780 Training loss: 2.4845 11.2883 sec/batch\n",
      "Epoch 2/10  Iteration 212/1780 Training loss: 2.4828 11.1786 sec/batch\n",
      "Epoch 2/10  Iteration 213/1780 Training loss: 2.4809 11.1657 sec/batch\n",
      "Epoch 2/10  Iteration 214/1780 Training loss: 2.4796 11.2476 sec/batch\n",
      "Epoch 2/10  Iteration 215/1780 Training loss: 2.4778 11.2443 sec/batch\n",
      "Epoch 2/10  Iteration 216/1780 Training loss: 2.4754 11.2388 sec/batch\n",
      "Epoch 2/10  Iteration 217/1780 Training loss: 2.4735 11.2328 sec/batch\n",
      "Epoch 2/10  Iteration 218/1780 Training loss: 2.4715 11.1877 sec/batch\n",
      "Epoch 2/10  Iteration 219/1780 Training loss: 2.4694 11.1404 sec/batch\n",
      "Epoch 2/10  Iteration 220/1780 Training loss: 2.4674 11.2598 sec/batch\n",
      "Epoch 2/10  Iteration 221/1780 Training loss: 2.4654 11.2765 sec/batch\n",
      "Epoch 2/10  Iteration 222/1780 Training loss: 2.4633 11.3338 sec/batch\n",
      "Epoch 2/10  Iteration 223/1780 Training loss: 2.4615 11.1906 sec/batch\n",
      "Epoch 2/10  Iteration 224/1780 Training loss: 2.4589 11.1700 sec/batch\n",
      "Epoch 2/10  Iteration 225/1780 Training loss: 2.4575 11.1857 sec/batch\n",
      "Epoch 2/10  Iteration 226/1780 Training loss: 2.4557 11.2419 sec/batch\n",
      "Epoch 2/10  Iteration 227/1780 Training loss: 2.4542 11.1764 sec/batch\n",
      "Epoch 2/10  Iteration 228/1780 Training loss: 2.4529 11.2032 sec/batch\n",
      "Epoch 2/10  Iteration 229/1780 Training loss: 2.4511 11.1262 sec/batch\n",
      "Epoch 2/10  Iteration 230/1780 Training loss: 2.4498 11.2302 sec/batch\n",
      "Epoch 2/10  Iteration 231/1780 Training loss: 2.4482 11.1099 sec/batch\n",
      "Epoch 2/10  Iteration 232/1780 Training loss: 2.4465 11.2080 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10  Iteration 233/1780 Training loss: 2.4448 11.2180 sec/batch\n",
      "Epoch 2/10  Iteration 234/1780 Training loss: 2.4437 11.2415 sec/batch\n",
      "Epoch 2/10  Iteration 235/1780 Training loss: 2.4424 11.2729 sec/batch\n",
      "Epoch 2/10  Iteration 236/1780 Training loss: 2.4408 11.2408 sec/batch\n",
      "Epoch 2/10  Iteration 237/1780 Training loss: 2.4393 11.2036 sec/batch\n",
      "Epoch 2/10  Iteration 238/1780 Training loss: 2.4382 11.1518 sec/batch\n",
      "Epoch 2/10  Iteration 239/1780 Training loss: 2.4367 11.1604 sec/batch\n",
      "Epoch 2/10  Iteration 240/1780 Training loss: 2.4356 11.2002 sec/batch\n",
      "Epoch 2/10  Iteration 241/1780 Training loss: 2.4345 11.3227 sec/batch\n",
      "Epoch 2/10  Iteration 242/1780 Training loss: 2.4331 11.1783 sec/batch\n",
      "Epoch 2/10  Iteration 243/1780 Training loss: 2.4316 11.3378 sec/batch\n",
      "Epoch 2/10  Iteration 244/1780 Training loss: 2.4306 11.1362 sec/batch\n",
      "Epoch 2/10  Iteration 245/1780 Training loss: 2.4292 11.1976 sec/batch\n",
      "Epoch 2/10  Iteration 246/1780 Training loss: 2.4274 11.2108 sec/batch\n",
      "Epoch 2/10  Iteration 247/1780 Training loss: 2.4258 11.1988 sec/batch\n",
      "Epoch 2/10  Iteration 248/1780 Training loss: 2.4246 11.2324 sec/batch\n",
      "Epoch 2/10  Iteration 249/1780 Training loss: 2.4236 11.1937 sec/batch\n",
      "Epoch 2/10  Iteration 250/1780 Training loss: 2.4224 11.2560 sec/batch\n",
      "Epoch 2/10  Iteration 251/1780 Training loss: 2.4212 11.1907 sec/batch\n",
      "Epoch 2/10  Iteration 252/1780 Training loss: 2.4197 11.1457 sec/batch\n",
      "Epoch 2/10  Iteration 253/1780 Training loss: 2.4183 12.1989 sec/batch\n",
      "Epoch 2/10  Iteration 254/1780 Training loss: 2.4174 11.2528 sec/batch\n",
      "Epoch 2/10  Iteration 255/1780 Training loss: 2.4160 11.2233 sec/batch\n",
      "Epoch 2/10  Iteration 256/1780 Training loss: 2.4149 11.2468 sec/batch\n",
      "Epoch 2/10  Iteration 257/1780 Training loss: 2.4134 11.2727 sec/batch\n",
      "Epoch 2/10  Iteration 258/1780 Training loss: 2.4119 11.2669 sec/batch\n",
      "Epoch 2/10  Iteration 259/1780 Training loss: 2.4104 11.1803 sec/batch\n",
      "Epoch 2/10  Iteration 260/1780 Training loss: 2.4093 11.1429 sec/batch\n",
      "Epoch 2/10  Iteration 261/1780 Training loss: 2.4079 11.1768 sec/batch\n",
      "Epoch 2/10  Iteration 262/1780 Training loss: 2.4063 11.2290 sec/batch\n",
      "Epoch 2/10  Iteration 263/1780 Training loss: 2.4046 11.2712 sec/batch\n",
      "Epoch 2/10  Iteration 264/1780 Training loss: 2.4032 11.2395 sec/batch\n",
      "Epoch 2/10  Iteration 265/1780 Training loss: 2.4019 11.2484 sec/batch\n",
      "Epoch 2/10  Iteration 266/1780 Training loss: 2.4006 11.1739 sec/batch\n",
      "Epoch 2/10  Iteration 267/1780 Training loss: 2.3991 11.3059 sec/batch\n",
      "Epoch 2/10  Iteration 268/1780 Training loss: 2.3979 11.1370 sec/batch\n",
      "Epoch 2/10  Iteration 269/1780 Training loss: 2.3966 11.1739 sec/batch\n",
      "Epoch 2/10  Iteration 270/1780 Training loss: 2.3954 11.1942 sec/batch\n",
      "Epoch 2/10  Iteration 271/1780 Training loss: 2.3940 11.1528 sec/batch\n",
      "Epoch 2/10  Iteration 272/1780 Training loss: 2.3927 11.1413 sec/batch\n",
      "Epoch 2/10  Iteration 273/1780 Training loss: 2.3912 11.1476 sec/batch\n",
      "Epoch 2/10  Iteration 274/1780 Training loss: 2.3897 11.1383 sec/batch\n",
      "Epoch 2/10  Iteration 275/1780 Training loss: 2.3885 11.1947 sec/batch\n",
      "Epoch 2/10  Iteration 276/1780 Training loss: 2.3871 11.1672 sec/batch\n",
      "Epoch 2/10  Iteration 277/1780 Training loss: 2.3858 11.1465 sec/batch\n",
      "Epoch 2/10  Iteration 278/1780 Training loss: 2.3844 11.1686 sec/batch\n",
      "Epoch 2/10  Iteration 279/1780 Training loss: 2.3834 11.2273 sec/batch\n",
      "Epoch 2/10  Iteration 280/1780 Training loss: 2.3822 11.1920 sec/batch\n",
      "Epoch 2/10  Iteration 281/1780 Training loss: 2.3808 11.1868 sec/batch\n",
      "Epoch 2/10  Iteration 282/1780 Training loss: 2.3795 11.2332 sec/batch\n",
      "Epoch 2/10  Iteration 283/1780 Training loss: 2.3782 11.1647 sec/batch\n",
      "Epoch 2/10  Iteration 284/1780 Training loss: 2.3770 11.2675 sec/batch\n",
      "Epoch 2/10  Iteration 285/1780 Training loss: 2.3758 11.2694 sec/batch\n",
      "Epoch 2/10  Iteration 286/1780 Training loss: 2.3749 11.2729 sec/batch\n",
      "Epoch 2/10  Iteration 287/1780 Training loss: 2.3738 11.1719 sec/batch\n",
      "Epoch 2/10  Iteration 288/1780 Training loss: 2.3725 11.1746 sec/batch\n",
      "Epoch 2/10  Iteration 289/1780 Training loss: 2.3714 11.2858 sec/batch\n",
      "Epoch 2/10  Iteration 290/1780 Training loss: 2.3704 11.2811 sec/batch\n",
      "Epoch 2/10  Iteration 291/1780 Training loss: 2.3692 11.2081 sec/batch\n",
      "Epoch 2/10  Iteration 292/1780 Training loss: 2.3679 11.2360 sec/batch\n",
      "Epoch 2/10  Iteration 293/1780 Training loss: 2.3666 11.2746 sec/batch\n",
      "Epoch 2/10  Iteration 294/1780 Training loss: 2.3652 11.2310 sec/batch\n",
      "Epoch 2/10  Iteration 295/1780 Training loss: 2.3640 11.1897 sec/batch\n",
      "Epoch 2/10  Iteration 296/1780 Training loss: 2.3628 11.2808 sec/batch\n",
      "Epoch 2/10  Iteration 297/1780 Training loss: 2.3618 11.1812 sec/batch\n",
      "Epoch 2/10  Iteration 298/1780 Training loss: 2.3607 11.2472 sec/batch\n",
      "Epoch 2/10  Iteration 299/1780 Training loss: 2.3596 11.2033 sec/batch\n",
      "Epoch 2/10  Iteration 300/1780 Training loss: 2.3584 11.1597 sec/batch\n",
      "Validation loss: 2.12974 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 301/1780 Training loss: 2.3574 11.1709 sec/batch\n",
      "Epoch 2/10  Iteration 302/1780 Training loss: 2.3564 11.1700 sec/batch\n",
      "Epoch 2/10  Iteration 303/1780 Training loss: 2.3552 11.2250 sec/batch\n",
      "Epoch 2/10  Iteration 304/1780 Training loss: 2.3539 11.2289 sec/batch\n",
      "Epoch 2/10  Iteration 305/1780 Training loss: 2.3529 11.1833 sec/batch\n",
      "Epoch 2/10  Iteration 306/1780 Training loss: 2.3519 11.1564 sec/batch\n",
      "Epoch 2/10  Iteration 307/1780 Training loss: 2.3509 11.2028 sec/batch\n",
      "Epoch 2/10  Iteration 308/1780 Training loss: 2.3498 11.2078 sec/batch\n",
      "Epoch 2/10  Iteration 309/1780 Training loss: 2.3486 11.2231 sec/batch\n",
      "Epoch 2/10  Iteration 310/1780 Training loss: 2.3473 11.1584 sec/batch\n",
      "Epoch 2/10  Iteration 311/1780 Training loss: 2.3463 11.3073 sec/batch\n",
      "Epoch 2/10  Iteration 312/1780 Training loss: 2.3453 11.1693 sec/batch\n",
      "Epoch 2/10  Iteration 313/1780 Training loss: 2.3442 11.2357 sec/batch\n",
      "Epoch 2/10  Iteration 314/1780 Training loss: 2.3432 11.2749 sec/batch\n",
      "Epoch 2/10  Iteration 315/1780 Training loss: 2.3421 11.2235 sec/batch\n",
      "Epoch 2/10  Iteration 316/1780 Training loss: 2.3410 11.2068 sec/batch\n",
      "Epoch 2/10  Iteration 317/1780 Training loss: 2.3402 11.1426 sec/batch\n",
      "Epoch 2/10  Iteration 318/1780 Training loss: 2.3390 11.2961 sec/batch\n",
      "Epoch 2/10  Iteration 319/1780 Training loss: 2.3381 11.2009 sec/batch\n",
      "Epoch 2/10  Iteration 320/1780 Training loss: 2.3370 11.1664 sec/batch\n",
      "Epoch 2/10  Iteration 321/1780 Training loss: 2.3359 11.2456 sec/batch\n",
      "Epoch 2/10  Iteration 322/1780 Training loss: 2.3349 11.2899 sec/batch\n",
      "Epoch 2/10  Iteration 323/1780 Training loss: 2.3338 11.2383 sec/batch\n",
      "Epoch 2/10  Iteration 324/1780 Training loss: 2.3330 11.2307 sec/batch\n",
      "Epoch 2/10  Iteration 325/1780 Training loss: 2.3319 11.1836 sec/batch\n",
      "Epoch 2/10  Iteration 326/1780 Training loss: 2.3311 11.2432 sec/batch\n",
      "Epoch 2/10  Iteration 327/1780 Training loss: 2.3300 11.2137 sec/batch\n",
      "Epoch 2/10  Iteration 328/1780 Training loss: 2.3289 11.2227 sec/batch\n",
      "Epoch 2/10  Iteration 329/1780 Training loss: 2.3279 11.1603 sec/batch\n",
      "Epoch 2/10  Iteration 330/1780 Training loss: 2.3271 11.2165 sec/batch\n",
      "Epoch 2/10  Iteration 331/1780 Training loss: 2.3262 11.2437 sec/batch\n",
      "Epoch 2/10  Iteration 332/1780 Training loss: 2.3253 11.1530 sec/batch\n",
      "Epoch 2/10  Iteration 333/1780 Training loss: 2.3242 11.2278 sec/batch\n",
      "Epoch 2/10  Iteration 334/1780 Training loss: 2.3232 11.2278 sec/batch\n",
      "Epoch 2/10  Iteration 335/1780 Training loss: 2.3222 11.2217 sec/batch\n",
      "Epoch 2/10  Iteration 336/1780 Training loss: 2.3211 11.2436 sec/batch\n",
      "Epoch 2/10  Iteration 337/1780 Training loss: 2.3200 11.2223 sec/batch\n",
      "Epoch 2/10  Iteration 338/1780 Training loss: 2.3192 11.1965 sec/batch\n",
      "Epoch 2/10  Iteration 339/1780 Training loss: 2.3183 11.2197 sec/batch\n",
      "Epoch 2/10  Iteration 340/1780 Training loss: 2.3173 11.1711 sec/batch\n",
      "Epoch 2/10  Iteration 341/1780 Training loss: 2.3162 11.2285 sec/batch\n",
      "Epoch 2/10  Iteration 342/1780 Training loss: 2.3153 11.1863 sec/batch\n",
      "Epoch 2/10  Iteration 343/1780 Training loss: 2.3144 11.3356 sec/batch\n",
      "Epoch 2/10  Iteration 344/1780 Training loss: 2.3134 11.2566 sec/batch\n",
      "Epoch 2/10  Iteration 345/1780 Training loss: 2.3125 11.2753 sec/batch\n",
      "Epoch 2/10  Iteration 346/1780 Training loss: 2.3117 11.2600 sec/batch\n",
      "Epoch 2/10  Iteration 347/1780 Training loss: 2.3108 11.3102 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10  Iteration 348/1780 Training loss: 2.3097 11.2025 sec/batch\n",
      "Epoch 2/10  Iteration 349/1780 Training loss: 2.3086 11.1708 sec/batch\n",
      "Epoch 2/10  Iteration 350/1780 Training loss: 2.3076 11.2172 sec/batch\n",
      "Epoch 2/10  Iteration 351/1780 Training loss: 2.3068 11.1469 sec/batch\n",
      "Epoch 2/10  Iteration 352/1780 Training loss: 2.3059 11.1627 sec/batch\n",
      "Epoch 2/10  Iteration 353/1780 Training loss: 2.3050 11.1968 sec/batch\n",
      "Epoch 2/10  Iteration 354/1780 Training loss: 2.3040 11.2171 sec/batch\n",
      "Epoch 2/10  Iteration 355/1780 Training loss: 2.3030 11.1398 sec/batch\n",
      "Epoch 2/10  Iteration 356/1780 Training loss: 2.3020 11.1335 sec/batch\n",
      "Epoch 3/10  Iteration 357/1780 Training loss: 2.2043 11.2139 sec/batch\n",
      "Epoch 3/10  Iteration 358/1780 Training loss: 2.1489 11.1358 sec/batch\n",
      "Epoch 3/10  Iteration 359/1780 Training loss: 2.1344 11.1822 sec/batch\n",
      "Epoch 3/10  Iteration 360/1780 Training loss: 2.1286 11.1412 sec/batch\n",
      "Epoch 3/10  Iteration 361/1780 Training loss: 2.1268 11.1707 sec/batch\n",
      "Epoch 3/10  Iteration 362/1780 Training loss: 2.1197 11.2319 sec/batch\n",
      "Epoch 3/10  Iteration 363/1780 Training loss: 2.1196 11.2116 sec/batch\n",
      "Epoch 3/10  Iteration 364/1780 Training loss: 2.1206 11.1005 sec/batch\n",
      "Epoch 3/10  Iteration 365/1780 Training loss: 2.1225 11.1682 sec/batch\n",
      "Epoch 3/10  Iteration 366/1780 Training loss: 2.1220 11.2217 sec/batch\n",
      "Epoch 3/10  Iteration 367/1780 Training loss: 2.1195 11.2216 sec/batch\n",
      "Epoch 3/10  Iteration 368/1780 Training loss: 2.1171 11.1642 sec/batch\n",
      "Epoch 3/10  Iteration 369/1780 Training loss: 2.1164 11.1620 sec/batch\n",
      "Epoch 3/10  Iteration 370/1780 Training loss: 2.1179 11.1798 sec/batch\n",
      "Epoch 3/10  Iteration 371/1780 Training loss: 2.1168 11.2134 sec/batch\n",
      "Epoch 3/10  Iteration 372/1780 Training loss: 2.1155 11.1791 sec/batch\n",
      "Epoch 3/10  Iteration 373/1780 Training loss: 2.1139 11.1353 sec/batch\n",
      "Epoch 3/10  Iteration 374/1780 Training loss: 2.1156 11.1774 sec/batch\n",
      "Epoch 3/10  Iteration 375/1780 Training loss: 2.1146 11.2884 sec/batch\n",
      "Epoch 3/10  Iteration 376/1780 Training loss: 2.1132 11.2678 sec/batch\n",
      "Epoch 3/10  Iteration 377/1780 Training loss: 2.1124 11.1463 sec/batch\n",
      "Epoch 3/10  Iteration 378/1780 Training loss: 2.1135 11.1595 sec/batch\n",
      "Epoch 3/10  Iteration 379/1780 Training loss: 2.1126 11.1406 sec/batch\n",
      "Epoch 3/10  Iteration 380/1780 Training loss: 2.1113 11.2889 sec/batch\n",
      "Epoch 3/10  Iteration 381/1780 Training loss: 2.1105 11.1852 sec/batch\n",
      "Epoch 3/10  Iteration 382/1780 Training loss: 2.1091 11.2275 sec/batch\n",
      "Epoch 3/10  Iteration 383/1780 Training loss: 2.1074 11.1623 sec/batch\n",
      "Epoch 3/10  Iteration 384/1780 Training loss: 2.1068 11.1983 sec/batch\n",
      "Epoch 3/10  Iteration 385/1780 Training loss: 2.1070 11.2197 sec/batch\n",
      "Epoch 3/10  Iteration 386/1780 Training loss: 2.1061 11.2478 sec/batch\n",
      "Epoch 3/10  Iteration 387/1780 Training loss: 2.1057 11.1186 sec/batch\n",
      "Epoch 3/10  Iteration 388/1780 Training loss: 2.1043 11.2629 sec/batch\n",
      "Epoch 3/10  Iteration 389/1780 Training loss: 2.1032 11.1420 sec/batch\n",
      "Epoch 3/10  Iteration 390/1780 Training loss: 2.1033 11.2689 sec/batch\n",
      "Epoch 3/10  Iteration 391/1780 Training loss: 2.1022 11.2660 sec/batch\n",
      "Epoch 3/10  Iteration 392/1780 Training loss: 2.1014 11.1493 sec/batch\n",
      "Epoch 3/10  Iteration 393/1780 Training loss: 2.1004 11.1864 sec/batch\n",
      "Epoch 3/10  Iteration 394/1780 Training loss: 2.0986 11.1827 sec/batch\n",
      "Epoch 3/10  Iteration 395/1780 Training loss: 2.0967 11.2078 sec/batch\n",
      "Epoch 3/10  Iteration 396/1780 Training loss: 2.0952 11.2068 sec/batch\n",
      "Epoch 3/10  Iteration 397/1780 Training loss: 2.0941 11.2793 sec/batch\n",
      "Epoch 3/10  Iteration 398/1780 Training loss: 2.0933 12.2663 sec/batch\n",
      "Epoch 3/10  Iteration 399/1780 Training loss: 2.0921 11.0983 sec/batch\n",
      "Epoch 3/10  Iteration 400/1780 Training loss: 2.0907 11.1366 sec/batch\n",
      "Validation loss: 1.96522 Saving checkpoint!\n",
      "Epoch 3/10  Iteration 401/1780 Training loss: 2.0909 11.2991 sec/batch\n",
      "Epoch 3/10  Iteration 402/1780 Training loss: 2.0889 11.1385 sec/batch\n",
      "Epoch 3/10  Iteration 403/1780 Training loss: 2.0885 11.1796 sec/batch\n",
      "Epoch 3/10  Iteration 404/1780 Training loss: 2.0874 11.1457 sec/batch\n",
      "Epoch 3/10  Iteration 405/1780 Training loss: 2.0864 11.2051 sec/batch\n",
      "Epoch 3/10  Iteration 406/1780 Training loss: 2.0865 11.1178 sec/batch\n",
      "Epoch 3/10  Iteration 407/1780 Training loss: 2.0851 11.1011 sec/batch\n",
      "Epoch 3/10  Iteration 408/1780 Training loss: 2.0853 11.1369 sec/batch\n",
      "Epoch 3/10  Iteration 409/1780 Training loss: 2.0842 11.1726 sec/batch\n",
      "Epoch 3/10  Iteration 410/1780 Training loss: 2.0836 11.1733 sec/batch\n",
      "Epoch 3/10  Iteration 411/1780 Training loss: 2.0827 11.2690 sec/batch\n",
      "Epoch 3/10  Iteration 412/1780 Training loss: 2.0822 11.2455 sec/batch\n",
      "Epoch 3/10  Iteration 413/1780 Training loss: 2.0818 11.1119 sec/batch\n",
      "Epoch 3/10  Iteration 414/1780 Training loss: 2.0810 11.2688 sec/batch\n",
      "Epoch 3/10  Iteration 415/1780 Training loss: 2.0800 11.2230 sec/batch\n",
      "Epoch 3/10  Iteration 416/1780 Training loss: 2.0799 11.1704 sec/batch\n",
      "Epoch 3/10  Iteration 417/1780 Training loss: 2.0791 11.1934 sec/batch\n",
      "Epoch 3/10  Iteration 418/1780 Training loss: 2.0789 11.1735 sec/batch\n",
      "Epoch 3/10  Iteration 419/1780 Training loss: 2.0788 11.1700 sec/batch\n",
      "Epoch 3/10  Iteration 420/1780 Training loss: 2.0783 11.2084 sec/batch\n",
      "Epoch 3/10  Iteration 421/1780 Training loss: 2.0774 11.1945 sec/batch\n",
      "Epoch 3/10  Iteration 422/1780 Training loss: 2.0770 11.1448 sec/batch\n",
      "Epoch 3/10  Iteration 423/1780 Training loss: 2.0765 11.1609 sec/batch\n",
      "Epoch 3/10  Iteration 424/1780 Training loss: 2.0755 11.1891 sec/batch\n",
      "Epoch 3/10  Iteration 425/1780 Training loss: 2.0745 11.2149 sec/batch\n",
      "Epoch 3/10  Iteration 426/1780 Training loss: 2.0739 11.2396 sec/batch\n",
      "Epoch 3/10  Iteration 427/1780 Training loss: 2.0734 11.1616 sec/batch\n",
      "Epoch 3/10  Iteration 428/1780 Training loss: 2.0727 11.2228 sec/batch\n",
      "Epoch 3/10  Iteration 429/1780 Training loss: 2.0722 11.1724 sec/batch\n",
      "Epoch 3/10  Iteration 430/1780 Training loss: 2.0712 11.1605 sec/batch\n",
      "Epoch 3/10  Iteration 431/1780 Training loss: 2.0704 11.2410 sec/batch\n",
      "Epoch 3/10  Iteration 432/1780 Training loss: 2.0701 11.0759 sec/batch\n",
      "Epoch 3/10  Iteration 433/1780 Training loss: 2.0693 11.2607 sec/batch\n",
      "Epoch 3/10  Iteration 434/1780 Training loss: 2.0688 11.1407 sec/batch\n",
      "Epoch 3/10  Iteration 435/1780 Training loss: 2.0677 11.2259 sec/batch\n",
      "Epoch 3/10  Iteration 436/1780 Training loss: 2.0669 11.1723 sec/batch\n",
      "Epoch 3/10  Iteration 437/1780 Training loss: 2.0659 11.2415 sec/batch\n",
      "Epoch 3/10  Iteration 438/1780 Training loss: 2.0653 11.2900 sec/batch\n",
      "Epoch 3/10  Iteration 439/1780 Training loss: 2.0641 11.2800 sec/batch\n",
      "Epoch 3/10  Iteration 440/1780 Training loss: 2.0634 11.2362 sec/batch\n",
      "Epoch 3/10  Iteration 441/1780 Training loss: 2.0624 11.1445 sec/batch\n",
      "Epoch 3/10  Iteration 442/1780 Training loss: 2.0615 11.2608 sec/batch\n",
      "Epoch 3/10  Iteration 443/1780 Training loss: 2.0608 11.2840 sec/batch\n",
      "Epoch 3/10  Iteration 444/1780 Training loss: 2.0599 11.2665 sec/batch\n",
      "Epoch 3/10  Iteration 445/1780 Training loss: 2.0589 11.2173 sec/batch\n",
      "Epoch 3/10  Iteration 446/1780 Training loss: 2.0583 11.1818 sec/batch\n",
      "Epoch 3/10  Iteration 447/1780 Training loss: 2.0573 11.2914 sec/batch\n",
      "Epoch 3/10  Iteration 448/1780 Training loss: 2.0567 11.2576 sec/batch\n",
      "Epoch 3/10  Iteration 449/1780 Training loss: 2.0555 11.2255 sec/batch\n",
      "Epoch 3/10  Iteration 450/1780 Training loss: 2.0546 11.2110 sec/batch\n",
      "Epoch 3/10  Iteration 451/1780 Training loss: 2.0535 11.1382 sec/batch\n",
      "Epoch 3/10  Iteration 452/1780 Training loss: 2.0527 11.1401 sec/batch\n",
      "Epoch 3/10  Iteration 453/1780 Training loss: 2.0519 11.2543 sec/batch\n",
      "Epoch 3/10  Iteration 454/1780 Training loss: 2.0510 11.3006 sec/batch\n",
      "Epoch 3/10  Iteration 455/1780 Training loss: 2.0500 11.2575 sec/batch\n",
      "Epoch 3/10  Iteration 456/1780 Training loss: 2.0489 11.1653 sec/batch\n",
      "Epoch 3/10  Iteration 457/1780 Training loss: 2.0482 11.1697 sec/batch\n",
      "Epoch 3/10  Iteration 458/1780 Training loss: 2.0476 11.2258 sec/batch\n",
      "Epoch 3/10  Iteration 459/1780 Training loss: 2.0467 11.2064 sec/batch\n",
      "Epoch 3/10  Iteration 460/1780 Training loss: 2.0459 11.0865 sec/batch\n",
      "Epoch 3/10  Iteration 461/1780 Training loss: 2.0450 11.0920 sec/batch\n",
      "Epoch 3/10  Iteration 462/1780 Training loss: 2.0443 11.2238 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10  Iteration 463/1780 Training loss: 2.0436 11.2055 sec/batch\n",
      "Epoch 3/10  Iteration 464/1780 Training loss: 2.0430 11.2056 sec/batch\n",
      "Epoch 3/10  Iteration 465/1780 Training loss: 2.0424 11.2392 sec/batch\n",
      "Epoch 3/10  Iteration 466/1780 Training loss: 2.0418 11.0696 sec/batch\n",
      "Epoch 3/10  Iteration 467/1780 Training loss: 2.0411 11.1347 sec/batch\n",
      "Epoch 3/10  Iteration 468/1780 Training loss: 2.0404 11.1286 sec/batch\n",
      "Epoch 3/10  Iteration 469/1780 Training loss: 2.0397 11.2194 sec/batch\n",
      "Epoch 3/10  Iteration 470/1780 Training loss: 2.0390 11.1435 sec/batch\n",
      "Epoch 3/10  Iteration 471/1780 Training loss: 2.0382 11.1461 sec/batch\n",
      "Epoch 3/10  Iteration 472/1780 Training loss: 2.0372 11.1583 sec/batch\n",
      "Epoch 3/10  Iteration 473/1780 Training loss: 2.0366 11.2216 sec/batch\n",
      "Epoch 3/10  Iteration 474/1780 Training loss: 2.0359 11.3673 sec/batch\n",
      "Epoch 3/10  Iteration 475/1780 Training loss: 2.0352 11.1979 sec/batch\n",
      "Epoch 3/10  Iteration 476/1780 Training loss: 2.0346 11.3509 sec/batch\n",
      "Epoch 3/10  Iteration 477/1780 Training loss: 2.0341 11.1303 sec/batch\n",
      "Epoch 3/10  Iteration 478/1780 Training loss: 2.0332 11.3065 sec/batch\n",
      "Epoch 3/10  Iteration 479/1780 Training loss: 2.0324 11.2089 sec/batch\n",
      "Epoch 3/10  Iteration 480/1780 Training loss: 2.0319 11.2144 sec/batch\n",
      "Epoch 3/10  Iteration 481/1780 Training loss: 2.0313 11.3404 sec/batch\n",
      "Epoch 3/10  Iteration 482/1780 Training loss: 2.0303 11.3828 sec/batch\n",
      "Epoch 3/10  Iteration 483/1780 Training loss: 2.0298 11.2509 sec/batch\n",
      "Epoch 3/10  Iteration 484/1780 Training loss: 2.0292 11.2331 sec/batch\n",
      "Epoch 3/10  Iteration 485/1780 Training loss: 2.0286 11.2499 sec/batch\n",
      "Epoch 3/10  Iteration 486/1780 Training loss: 2.0280 11.3004 sec/batch\n",
      "Epoch 3/10  Iteration 487/1780 Training loss: 2.0271 11.3708 sec/batch\n",
      "Epoch 3/10  Iteration 488/1780 Training loss: 2.0263 11.1672 sec/batch\n",
      "Epoch 3/10  Iteration 489/1780 Training loss: 2.0257 11.2099 sec/batch\n",
      "Epoch 3/10  Iteration 490/1780 Training loss: 2.0251 11.2821 sec/batch\n",
      "Epoch 3/10  Iteration 491/1780 Training loss: 2.0245 11.1900 sec/batch\n",
      "Epoch 3/10  Iteration 492/1780 Training loss: 2.0239 11.2596 sec/batch\n",
      "Epoch 3/10  Iteration 493/1780 Training loss: 2.0234 11.2115 sec/batch\n",
      "Epoch 3/10  Iteration 494/1780 Training loss: 2.0229 11.2255 sec/batch\n",
      "Epoch 3/10  Iteration 495/1780 Training loss: 2.0225 11.0870 sec/batch\n",
      "Epoch 3/10  Iteration 496/1780 Training loss: 2.0219 11.1737 sec/batch\n",
      "Epoch 3/10  Iteration 497/1780 Training loss: 2.0216 11.2288 sec/batch\n",
      "Epoch 3/10  Iteration 498/1780 Training loss: 2.0210 11.1448 sec/batch\n",
      "Epoch 3/10  Iteration 499/1780 Training loss: 2.0204 11.1221 sec/batch\n",
      "Epoch 3/10  Iteration 500/1780 Training loss: 2.0199 11.1230 sec/batch\n",
      "Validation loss: 1.83821 Saving checkpoint!\n",
      "Epoch 3/10  Iteration 501/1780 Training loss: 2.0194 11.2607 sec/batch\n",
      "Epoch 3/10  Iteration 502/1780 Training loss: 2.0190 11.1745 sec/batch\n",
      "Epoch 3/10  Iteration 503/1780 Training loss: 2.0186 11.0990 sec/batch\n",
      "Epoch 3/10  Iteration 504/1780 Training loss: 2.0182 11.2124 sec/batch\n",
      "Epoch 3/10  Iteration 505/1780 Training loss: 2.0177 11.3537 sec/batch\n",
      "Epoch 3/10  Iteration 506/1780 Training loss: 2.0171 11.2542 sec/batch\n",
      "Epoch 3/10  Iteration 507/1780 Training loss: 2.0164 11.2045 sec/batch\n",
      "Epoch 3/10  Iteration 508/1780 Training loss: 2.0161 11.2866 sec/batch\n",
      "Epoch 3/10  Iteration 509/1780 Training loss: 2.0157 11.2973 sec/batch\n",
      "Epoch 3/10  Iteration 510/1780 Training loss: 2.0152 11.1636 sec/batch\n",
      "Epoch 3/10  Iteration 511/1780 Training loss: 2.0146 11.2757 sec/batch\n",
      "Epoch 3/10  Iteration 512/1780 Training loss: 2.0140 11.2885 sec/batch\n",
      "Epoch 3/10  Iteration 513/1780 Training loss: 2.0135 11.2619 sec/batch\n",
      "Epoch 3/10  Iteration 514/1780 Training loss: 2.0129 11.1714 sec/batch\n",
      "Epoch 3/10  Iteration 515/1780 Training loss: 2.0122 11.2386 sec/batch\n",
      "Epoch 3/10  Iteration 516/1780 Training loss: 2.0119 11.1621 sec/batch\n",
      "Epoch 3/10  Iteration 517/1780 Training loss: 2.0114 11.1724 sec/batch\n",
      "Epoch 3/10  Iteration 518/1780 Training loss: 2.0108 11.2067 sec/batch\n",
      "Epoch 3/10  Iteration 519/1780 Training loss: 2.0103 11.1149 sec/batch\n",
      "Epoch 3/10  Iteration 520/1780 Training loss: 2.0097 11.2055 sec/batch\n",
      "Epoch 3/10  Iteration 521/1780 Training loss: 2.0092 11.2847 sec/batch\n",
      "Epoch 3/10  Iteration 522/1780 Training loss: 2.0086 11.1948 sec/batch\n",
      "Epoch 3/10  Iteration 523/1780 Training loss: 2.0081 11.1803 sec/batch\n",
      "Epoch 3/10  Iteration 524/1780 Training loss: 2.0078 11.1896 sec/batch\n",
      "Epoch 3/10  Iteration 525/1780 Training loss: 2.0072 11.1234 sec/batch\n",
      "Epoch 3/10  Iteration 526/1780 Training loss: 2.0067 11.2188 sec/batch\n",
      "Epoch 3/10  Iteration 527/1780 Training loss: 2.0060 11.1799 sec/batch\n",
      "Epoch 3/10  Iteration 528/1780 Training loss: 2.0054 11.1995 sec/batch\n",
      "Epoch 3/10  Iteration 529/1780 Training loss: 2.0049 11.2612 sec/batch\n",
      "Epoch 3/10  Iteration 530/1780 Training loss: 2.0045 11.2738 sec/batch\n",
      "Epoch 3/10  Iteration 531/1780 Training loss: 2.0040 11.2823 sec/batch\n",
      "Epoch 3/10  Iteration 532/1780 Training loss: 2.0034 11.2662 sec/batch\n",
      "Epoch 3/10  Iteration 533/1780 Training loss: 2.0027 11.2046 sec/batch\n",
      "Epoch 3/10  Iteration 534/1780 Training loss: 2.0023 11.3193 sec/batch\n",
      "Epoch 4/10  Iteration 535/1780 Training loss: 1.9754 11.1071 sec/batch\n",
      "Epoch 4/10  Iteration 536/1780 Training loss: 1.9261 11.3068 sec/batch\n",
      "Epoch 4/10  Iteration 537/1780 Training loss: 1.9099 11.2205 sec/batch\n",
      "Epoch 4/10  Iteration 538/1780 Training loss: 1.9041 11.1105 sec/batch\n",
      "Epoch 4/10  Iteration 539/1780 Training loss: 1.9027 11.1633 sec/batch\n",
      "Epoch 4/10  Iteration 540/1780 Training loss: 1.8942 11.1486 sec/batch\n",
      "Epoch 4/10  Iteration 541/1780 Training loss: 1.8944 11.1879 sec/batch\n",
      "Epoch 4/10  Iteration 542/1780 Training loss: 1.8922 11.2506 sec/batch\n",
      "Epoch 4/10  Iteration 543/1780 Training loss: 1.8950 11.1133 sec/batch\n",
      "Epoch 4/10  Iteration 544/1780 Training loss: 1.8949 11.3793 sec/batch\n",
      "Epoch 4/10  Iteration 545/1780 Training loss: 1.8915 11.2059 sec/batch\n",
      "Epoch 4/10  Iteration 546/1780 Training loss: 1.8908 11.2057 sec/batch\n",
      "Epoch 4/10  Iteration 547/1780 Training loss: 1.8905 11.2783 sec/batch\n",
      "Epoch 4/10  Iteration 548/1780 Training loss: 1.8925 11.2101 sec/batch\n",
      "Epoch 4/10  Iteration 549/1780 Training loss: 1.8912 11.3221 sec/batch\n",
      "Epoch 4/10  Iteration 550/1780 Training loss: 1.8897 11.9071 sec/batch\n",
      "Epoch 4/10  Iteration 551/1780 Training loss: 1.8890 11.5026 sec/batch\n",
      "Epoch 4/10  Iteration 552/1780 Training loss: 1.8907 11.1177 sec/batch\n",
      "Epoch 4/10  Iteration 553/1780 Training loss: 1.8904 11.1709 sec/batch\n",
      "Epoch 4/10  Iteration 554/1780 Training loss: 1.8904 11.2272 sec/batch\n",
      "Epoch 4/10  Iteration 555/1780 Training loss: 1.8897 11.2569 sec/batch\n",
      "Epoch 4/10  Iteration 556/1780 Training loss: 1.8911 11.2490 sec/batch\n",
      "Epoch 4/10  Iteration 557/1780 Training loss: 1.8897 11.1325 sec/batch\n",
      "Epoch 4/10  Iteration 558/1780 Training loss: 1.8887 11.3670 sec/batch\n",
      "Epoch 4/10  Iteration 559/1780 Training loss: 1.8881 11.1821 sec/batch\n",
      "Epoch 4/10  Iteration 560/1780 Training loss: 1.8866 11.1566 sec/batch\n",
      "Epoch 4/10  Iteration 561/1780 Training loss: 1.8854 11.2721 sec/batch\n",
      "Epoch 4/10  Iteration 562/1780 Training loss: 1.8851 11.3073 sec/batch\n",
      "Epoch 4/10  Iteration 563/1780 Training loss: 1.8857 11.2542 sec/batch\n",
      "Epoch 4/10  Iteration 564/1780 Training loss: 1.8856 11.3297 sec/batch\n",
      "Epoch 4/10  Iteration 565/1780 Training loss: 1.8852 11.3316 sec/batch\n",
      "Epoch 4/10  Iteration 566/1780 Training loss: 1.8842 11.3161 sec/batch\n",
      "Epoch 4/10  Iteration 567/1780 Training loss: 1.8838 11.2801 sec/batch\n",
      "Epoch 4/10  Iteration 568/1780 Training loss: 1.8841 11.1485 sec/batch\n",
      "Epoch 4/10  Iteration 569/1780 Training loss: 1.8832 11.3235 sec/batch\n",
      "Epoch 4/10  Iteration 570/1780 Training loss: 1.8828 11.2091 sec/batch\n",
      "Epoch 4/10  Iteration 571/1780 Training loss: 1.8817 11.2653 sec/batch\n",
      "Epoch 4/10  Iteration 572/1780 Training loss: 1.8803 11.1757 sec/batch\n",
      "Epoch 4/10  Iteration 573/1780 Training loss: 1.8789 11.1889 sec/batch\n",
      "Epoch 4/10  Iteration 574/1780 Training loss: 1.8777 11.0859 sec/batch\n",
      "Epoch 4/10  Iteration 575/1780 Training loss: 1.8768 11.2563 sec/batch\n",
      "Epoch 4/10  Iteration 576/1780 Training loss: 1.8765 11.1976 sec/batch\n",
      "Epoch 4/10  Iteration 577/1780 Training loss: 1.8755 11.0860 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10  Iteration 578/1780 Training loss: 1.8743 11.1993 sec/batch\n",
      "Epoch 4/10  Iteration 579/1780 Training loss: 1.8740 11.2876 sec/batch\n",
      "Epoch 4/10  Iteration 580/1780 Training loss: 1.8727 11.2934 sec/batch\n",
      "Epoch 4/10  Iteration 581/1780 Training loss: 1.8721 11.1821 sec/batch\n",
      "Epoch 4/10  Iteration 582/1780 Training loss: 1.8712 11.1947 sec/batch\n",
      "Epoch 4/10  Iteration 583/1780 Training loss: 1.8706 11.1150 sec/batch\n",
      "Epoch 4/10  Iteration 584/1780 Training loss: 1.8708 11.1868 sec/batch\n",
      "Epoch 4/10  Iteration 585/1780 Training loss: 1.8696 11.2933 sec/batch\n",
      "Epoch 4/10  Iteration 586/1780 Training loss: 1.8700 11.2534 sec/batch\n",
      "Epoch 4/10  Iteration 587/1780 Training loss: 1.8693 11.4094 sec/batch\n",
      "Epoch 4/10  Iteration 588/1780 Training loss: 1.8689 11.2936 sec/batch\n",
      "Epoch 4/10  Iteration 589/1780 Training loss: 1.8684 11.3324 sec/batch\n",
      "Epoch 4/10  Iteration 590/1780 Training loss: 1.8681 11.1549 sec/batch\n",
      "Epoch 4/10  Iteration 591/1780 Training loss: 1.8682 11.1539 sec/batch\n",
      "Epoch 4/10  Iteration 592/1780 Training loss: 1.8675 11.1104 sec/batch\n",
      "Epoch 4/10  Iteration 593/1780 Training loss: 1.8667 11.1065 sec/batch\n",
      "Epoch 4/10  Iteration 594/1780 Training loss: 1.8667 11.2239 sec/batch\n",
      "Epoch 4/10  Iteration 595/1780 Training loss: 1.8662 11.2513 sec/batch\n",
      "Epoch 4/10  Iteration 596/1780 Training loss: 1.8664 11.2046 sec/batch\n",
      "Epoch 4/10  Iteration 597/1780 Training loss: 1.8664 11.1878 sec/batch\n",
      "Epoch 4/10  Iteration 598/1780 Training loss: 1.8663 11.1227 sec/batch\n",
      "Epoch 4/10  Iteration 599/1780 Training loss: 1.8659 11.2769 sec/batch\n",
      "Epoch 4/10  Iteration 600/1780 Training loss: 1.8659 11.3233 sec/batch\n",
      "Validation loss: 1.7309 Saving checkpoint!\n",
      "Epoch 4/10  Iteration 601/1780 Training loss: 1.8663 11.2532 sec/batch\n",
      "Epoch 4/10  Iteration 602/1780 Training loss: 1.8656 11.2013 sec/batch\n",
      "Epoch 4/10  Iteration 603/1780 Training loss: 1.8649 11.2250 sec/batch\n",
      "Epoch 4/10  Iteration 604/1780 Training loss: 1.8645 11.2357 sec/batch\n",
      "Epoch 4/10  Iteration 605/1780 Training loss: 1.8646 11.1392 sec/batch\n",
      "Epoch 4/10  Iteration 606/1780 Training loss: 1.8644 11.3283 sec/batch\n",
      "Epoch 4/10  Iteration 607/1780 Training loss: 1.8643 11.1996 sec/batch\n",
      "Epoch 4/10  Iteration 608/1780 Training loss: 1.8636 11.1570 sec/batch\n",
      "Epoch 4/10  Iteration 609/1780 Training loss: 1.8631 11.3157 sec/batch\n",
      "Epoch 4/10  Iteration 610/1780 Training loss: 1.8630 11.2040 sec/batch\n",
      "Epoch 4/10  Iteration 611/1780 Training loss: 1.8625 11.2647 sec/batch\n",
      "Epoch 4/10  Iteration 612/1780 Training loss: 1.8621 11.4741 sec/batch\n",
      "Epoch 4/10  Iteration 613/1780 Training loss: 1.8613 11.2193 sec/batch\n",
      "Epoch 4/10  Iteration 614/1780 Training loss: 1.8607 11.3179 sec/batch\n",
      "Epoch 4/10  Iteration 615/1780 Training loss: 1.8599 11.1144 sec/batch\n",
      "Epoch 4/10  Iteration 616/1780 Training loss: 1.8595 11.3310 sec/batch\n",
      "Epoch 4/10  Iteration 617/1780 Training loss: 1.8585 11.1678 sec/batch\n",
      "Epoch 4/10  Iteration 618/1780 Training loss: 1.8581 11.1589 sec/batch\n",
      "Epoch 4/10  Iteration 619/1780 Training loss: 1.8572 11.1457 sec/batch\n",
      "Epoch 4/10  Iteration 620/1780 Training loss: 1.8565 11.2191 sec/batch\n",
      "Epoch 4/10  Iteration 621/1780 Training loss: 1.8559 11.1964 sec/batch\n",
      "Epoch 4/10  Iteration 622/1780 Training loss: 1.8554 11.2160 sec/batch\n",
      "Epoch 4/10  Iteration 623/1780 Training loss: 1.8546 11.2947 sec/batch\n",
      "Epoch 4/10  Iteration 624/1780 Training loss: 1.8544 11.1289 sec/batch\n",
      "Epoch 4/10  Iteration 625/1780 Training loss: 1.8537 11.1660 sec/batch\n",
      "Epoch 4/10  Iteration 626/1780 Training loss: 1.8531 11.3003 sec/batch\n",
      "Epoch 4/10  Iteration 627/1780 Training loss: 1.8522 11.1649 sec/batch\n",
      "Epoch 4/10  Iteration 628/1780 Training loss: 1.8516 11.1564 sec/batch\n",
      "Epoch 4/10  Iteration 629/1780 Training loss: 1.8509 11.1500 sec/batch\n",
      "Epoch 4/10  Iteration 630/1780 Training loss: 1.8504 11.2868 sec/batch\n",
      "Epoch 4/10  Iteration 631/1780 Training loss: 1.8500 11.3725 sec/batch\n",
      "Epoch 4/10  Iteration 632/1780 Training loss: 1.8493 11.2923 sec/batch\n",
      "Epoch 4/10  Iteration 633/1780 Training loss: 1.8485 11.4045 sec/batch\n",
      "Epoch 4/10  Iteration 634/1780 Training loss: 1.8477 11.1682 sec/batch\n",
      "Epoch 4/10  Iteration 635/1780 Training loss: 1.8473 11.2161 sec/batch\n",
      "Epoch 4/10  Iteration 636/1780 Training loss: 1.8468 11.1351 sec/batch\n",
      "Epoch 4/10  Iteration 637/1780 Training loss: 1.8462 11.4000 sec/batch\n",
      "Epoch 4/10  Iteration 638/1780 Training loss: 1.8456 11.3713 sec/batch\n",
      "Epoch 4/10  Iteration 639/1780 Training loss: 1.8449 13.4175 sec/batch\n",
      "Epoch 4/10  Iteration 640/1780 Training loss: 1.8444 17.5834 sec/batch\n",
      "Epoch 4/10  Iteration 641/1780 Training loss: 1.8440 15.8476 sec/batch\n",
      "Epoch 4/10  Iteration 642/1780 Training loss: 1.8435 15.8928 sec/batch\n",
      "Epoch 4/10  Iteration 643/1780 Training loss: 1.8432 18.5924 sec/batch\n",
      "Epoch 4/10  Iteration 644/1780 Training loss: 1.8427 14.5592 sec/batch\n",
      "Epoch 4/10  Iteration 645/1780 Training loss: 1.8423 17.3801 sec/batch\n",
      "Epoch 4/10  Iteration 646/1780 Training loss: 1.8417 17.6174 sec/batch\n",
      "Epoch 4/10  Iteration 647/1780 Training loss: 1.8412 15.7759 sec/batch\n",
      "Epoch 4/10  Iteration 648/1780 Training loss: 1.8407 16.7473 sec/batch\n",
      "Epoch 4/10  Iteration 649/1780 Training loss: 1.8401 17.4424 sec/batch\n",
      "Epoch 4/10  Iteration 650/1780 Training loss: 1.8393 13.1132 sec/batch\n",
      "Epoch 4/10  Iteration 651/1780 Training loss: 1.8389 16.4569 sec/batch\n",
      "Epoch 4/10  Iteration 652/1780 Training loss: 1.8384 14.4177 sec/batch\n",
      "Epoch 4/10  Iteration 653/1780 Training loss: 1.8379 16.1755 sec/batch\n",
      "Epoch 4/10  Iteration 654/1780 Training loss: 1.8374 15.2168 sec/batch\n",
      "Epoch 4/10  Iteration 655/1780 Training loss: 1.8371 16.2685 sec/batch\n",
      "Epoch 4/10  Iteration 656/1780 Training loss: 1.8364 16.8394 sec/batch\n",
      "Epoch 4/10  Iteration 657/1780 Training loss: 1.8357 16.6740 sec/batch\n",
      "Epoch 4/10  Iteration 658/1780 Training loss: 1.8354 14.4873 sec/batch\n",
      "Epoch 4/10  Iteration 659/1780 Training loss: 1.8349 16.3877 sec/batch\n",
      "Epoch 4/10  Iteration 660/1780 Training loss: 1.8342 17.7085 sec/batch\n",
      "Epoch 4/10  Iteration 661/1780 Training loss: 1.8338 16.7413 sec/batch\n",
      "Epoch 4/10  Iteration 662/1780 Training loss: 1.8335 16.9053 sec/batch\n",
      "Epoch 4/10  Iteration 663/1780 Training loss: 1.8331 15.9857 sec/batch\n",
      "Epoch 4/10  Iteration 664/1780 Training loss: 1.8327 18.3058 sec/batch\n",
      "Epoch 4/10  Iteration 665/1780 Training loss: 1.8320 16.8464 sec/batch\n",
      "Epoch 4/10  Iteration 666/1780 Training loss: 1.8313 17.4213 sec/batch\n",
      "Epoch 4/10  Iteration 667/1780 Training loss: 1.8311 17.0566 sec/batch\n",
      "Epoch 4/10  Iteration 668/1780 Training loss: 1.8307 18.1082 sec/batch\n",
      "Epoch 4/10  Iteration 669/1780 Training loss: 1.8303 15.6889 sec/batch\n",
      "Epoch 4/10  Iteration 670/1780 Training loss: 1.8300 17.6854 sec/batch\n",
      "Epoch 4/10  Iteration 671/1780 Training loss: 1.8297 17.6184 sec/batch\n",
      "Epoch 4/10  Iteration 672/1780 Training loss: 1.8294 17.3799 sec/batch\n",
      "Epoch 4/10  Iteration 673/1780 Training loss: 1.8292 16.7985 sec/batch\n",
      "Epoch 4/10  Iteration 674/1780 Training loss: 1.8289 16.8946 sec/batch\n",
      "Epoch 4/10  Iteration 675/1780 Training loss: 1.8288 14.6929 sec/batch\n",
      "Epoch 4/10  Iteration 676/1780 Training loss: 1.8283 14.7490 sec/batch\n",
      "Epoch 4/10  Iteration 677/1780 Training loss: 1.8279 15.5207 sec/batch\n",
      "Epoch 4/10  Iteration 678/1780 Training loss: 1.8275 18.1050 sec/batch\n",
      "Epoch 4/10  Iteration 679/1780 Training loss: 1.8270 16.0347 sec/batch\n",
      "Epoch 4/10  Iteration 680/1780 Training loss: 1.8267 17.3830 sec/batch\n",
      "Epoch 4/10  Iteration 681/1780 Training loss: 1.8264 18.8735 sec/batch\n",
      "Epoch 4/10  Iteration 682/1780 Training loss: 1.8262 16.8081 sec/batch\n",
      "Epoch 4/10  Iteration 683/1780 Training loss: 1.8259 17.4324 sec/batch\n",
      "Epoch 4/10  Iteration 684/1780 Training loss: 1.8254 16.6556 sec/batch\n",
      "Epoch 4/10  Iteration 685/1780 Training loss: 1.8248 14.2036 sec/batch\n",
      "Epoch 4/10  Iteration 686/1780 Training loss: 1.8247 17.3239 sec/batch\n",
      "Epoch 4/10  Iteration 687/1780 Training loss: 1.8243 14.7923 sec/batch\n",
      "Epoch 4/10  Iteration 688/1780 Training loss: 1.8240 14.9969 sec/batch\n",
      "Epoch 4/10  Iteration 689/1780 Training loss: 1.8236 14.6231 sec/batch\n",
      "Epoch 4/10  Iteration 690/1780 Training loss: 1.8232 14.2144 sec/batch\n",
      "Epoch 4/10  Iteration 691/1780 Training loss: 1.8229 14.7349 sec/batch\n",
      "Epoch 4/10  Iteration 692/1780 Training loss: 1.8225 15.3797 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10  Iteration 693/1780 Training loss: 1.8219 14.4908 sec/batch\n",
      "Epoch 4/10  Iteration 694/1780 Training loss: 1.8217 14.2156 sec/batch\n",
      "Epoch 4/10  Iteration 695/1780 Training loss: 1.8215 15.7206 sec/batch\n",
      "Epoch 4/10  Iteration 696/1780 Training loss: 1.8211 17.3379 sec/batch\n",
      "Epoch 4/10  Iteration 697/1780 Training loss: 1.8208 17.8525 sec/batch\n",
      "Epoch 4/10  Iteration 698/1780 Training loss: 1.8205 19.6999 sec/batch\n",
      "Epoch 4/10  Iteration 699/1780 Training loss: 1.8201 18.0227 sec/batch\n",
      "Epoch 4/10  Iteration 700/1780 Training loss: 1.8196 18.3051 sec/batch\n",
      "Validation loss: 1.64238 Saving checkpoint!\n",
      "Epoch 4/10  Iteration 701/1780 Training loss: 1.8196 15.6898 sec/batch\n",
      "Epoch 4/10  Iteration 702/1780 Training loss: 1.8197 17.0044 sec/batch\n",
      "Epoch 4/10  Iteration 703/1780 Training loss: 1.8193 16.3866 sec/batch\n",
      "Epoch 4/10  Iteration 704/1780 Training loss: 1.8190 16.1168 sec/batch\n",
      "Epoch 4/10  Iteration 705/1780 Training loss: 1.8185 17.6150 sec/batch\n",
      "Epoch 4/10  Iteration 706/1780 Training loss: 1.8179 18.5737 sec/batch\n",
      "Epoch 4/10  Iteration 707/1780 Training loss: 1.8177 15.9998 sec/batch\n",
      "Epoch 4/10  Iteration 708/1780 Training loss: 1.8174 17.8216 sec/batch\n",
      "Epoch 4/10  Iteration 709/1780 Training loss: 1.8170 18.9251 sec/batch\n",
      "Epoch 4/10  Iteration 710/1780 Training loss: 1.8166 18.7899 sec/batch\n",
      "Epoch 4/10  Iteration 711/1780 Training loss: 1.8161 18.1641 sec/batch\n",
      "Epoch 4/10  Iteration 712/1780 Training loss: 1.8158 15.6540 sec/batch\n",
      "Epoch 5/10  Iteration 713/1780 Training loss: 1.8352 16.5420 sec/batch\n",
      "Epoch 5/10  Iteration 714/1780 Training loss: 1.7870 19.0489 sec/batch\n",
      "Epoch 5/10  Iteration 715/1780 Training loss: 1.7715 14.0428 sec/batch\n",
      "Epoch 5/10  Iteration 716/1780 Training loss: 1.7629 16.3313 sec/batch\n",
      "Epoch 5/10  Iteration 717/1780 Training loss: 1.7572 15.6984 sec/batch\n",
      "Epoch 5/10  Iteration 718/1780 Training loss: 1.7474 13.0766 sec/batch\n",
      "Epoch 5/10  Iteration 719/1780 Training loss: 1.7476 13.7313 sec/batch\n",
      "Epoch 5/10  Iteration 720/1780 Training loss: 1.7462 13.4559 sec/batch\n",
      "Epoch 5/10  Iteration 721/1780 Training loss: 1.7482 16.2008 sec/batch\n",
      "Epoch 5/10  Iteration 722/1780 Training loss: 1.7474 17.0968 sec/batch\n",
      "Epoch 5/10  Iteration 723/1780 Training loss: 1.7438 17.0922 sec/batch\n",
      "Epoch 5/10  Iteration 724/1780 Training loss: 1.7420 15.3416 sec/batch\n",
      "Epoch 5/10  Iteration 725/1780 Training loss: 1.7414 17.9112 sec/batch\n",
      "Epoch 5/10  Iteration 726/1780 Training loss: 1.7435 16.0868 sec/batch\n",
      "Epoch 5/10  Iteration 727/1780 Training loss: 1.7422 13.3459 sec/batch\n",
      "Epoch 5/10  Iteration 728/1780 Training loss: 1.7405 11.9909 sec/batch\n",
      "Epoch 5/10  Iteration 729/1780 Training loss: 1.7399 13.6048 sec/batch\n",
      "Epoch 5/10  Iteration 730/1780 Training loss: 1.7412 13.2938 sec/batch\n",
      "Epoch 5/10  Iteration 731/1780 Training loss: 1.7416 16.4436 sec/batch\n",
      "Epoch 5/10  Iteration 732/1780 Training loss: 1.7419 13.8568 sec/batch\n",
      "Epoch 5/10  Iteration 733/1780 Training loss: 1.7410 12.7232 sec/batch\n",
      "Epoch 5/10  Iteration 734/1780 Training loss: 1.7421 13.2452 sec/batch\n",
      "Epoch 5/10  Iteration 735/1780 Training loss: 1.7409 14.3060 sec/batch\n",
      "Epoch 5/10  Iteration 736/1780 Training loss: 1.7399 14.9548 sec/batch\n",
      "Epoch 5/10  Iteration 737/1780 Training loss: 1.7396 14.5947 sec/batch\n",
      "Epoch 5/10  Iteration 738/1780 Training loss: 1.7378 14.6782 sec/batch\n",
      "Epoch 5/10  Iteration 739/1780 Training loss: 1.7365 16.4124 sec/batch\n",
      "Epoch 5/10  Iteration 740/1780 Training loss: 1.7365 18.0611 sec/batch\n",
      "Epoch 5/10  Iteration 741/1780 Training loss: 1.7371 18.7324 sec/batch\n",
      "Epoch 5/10  Iteration 742/1780 Training loss: 1.7371 14.9594 sec/batch\n",
      "Epoch 5/10  Iteration 743/1780 Training loss: 1.7368 14.6743 sec/batch\n",
      "Epoch 5/10  Iteration 744/1780 Training loss: 1.7357 13.9629 sec/batch\n",
      "Epoch 5/10  Iteration 745/1780 Training loss: 1.7356 16.5992 sec/batch\n",
      "Epoch 5/10  Iteration 746/1780 Training loss: 1.7362 13.4256 sec/batch\n",
      "Epoch 5/10  Iteration 747/1780 Training loss: 1.7356 12.1395 sec/batch\n",
      "Epoch 5/10  Iteration 748/1780 Training loss: 1.7350 11.8590 sec/batch\n",
      "Epoch 5/10  Iteration 749/1780 Training loss: 1.7342 14.3143 sec/batch\n",
      "Epoch 5/10  Iteration 750/1780 Training loss: 1.7328 17.6694 sec/batch\n",
      "Epoch 5/10  Iteration 751/1780 Training loss: 1.7314 16.2152 sec/batch\n",
      "Epoch 5/10  Iteration 752/1780 Training loss: 1.7304 14.9686 sec/batch\n",
      "Epoch 5/10  Iteration 753/1780 Training loss: 1.7296 17.2560 sec/batch\n",
      "Epoch 5/10  Iteration 754/1780 Training loss: 1.7298 19.0547 sec/batch\n",
      "Epoch 5/10  Iteration 755/1780 Training loss: 1.7290 18.4767 sec/batch\n",
      "Epoch 5/10  Iteration 756/1780 Training loss: 1.7278 19.7937 sec/batch\n",
      "Epoch 5/10  Iteration 757/1780 Training loss: 1.7279 17.0767 sec/batch\n",
      "Epoch 5/10  Iteration 758/1780 Training loss: 1.7267 15.9954 sec/batch\n",
      "Epoch 5/10  Iteration 759/1780 Training loss: 1.7261 14.5987 sec/batch\n",
      "Epoch 5/10  Iteration 760/1780 Training loss: 1.7253 13.2225 sec/batch\n",
      "Epoch 5/10  Iteration 761/1780 Training loss: 1.7248 17.1716 sec/batch\n",
      "Epoch 5/10  Iteration 762/1780 Training loss: 1.7253 15.1540 sec/batch\n",
      "Epoch 5/10  Iteration 763/1780 Training loss: 1.7245 17.9201 sec/batch\n",
      "Epoch 5/10  Iteration 764/1780 Training loss: 1.7252 15.1454 sec/batch\n",
      "Epoch 5/10  Iteration 765/1780 Training loss: 1.7247 14.5997 sec/batch\n",
      "Epoch 5/10  Iteration 766/1780 Training loss: 1.7245 15.2313 sec/batch\n",
      "Epoch 5/10  Iteration 767/1780 Training loss: 1.7239 14.5217 sec/batch\n",
      "Epoch 5/10  Iteration 768/1780 Training loss: 1.7236 14.2317 sec/batch\n",
      "Epoch 5/10  Iteration 769/1780 Training loss: 1.7237 13.6342 sec/batch\n",
      "Epoch 5/10  Iteration 770/1780 Training loss: 1.7231 15.3443 sec/batch\n",
      "Epoch 5/10  Iteration 771/1780 Training loss: 1.7225 13.9177 sec/batch\n",
      "Epoch 5/10  Iteration 772/1780 Training loss: 1.7227 16.6492 sec/batch\n",
      "Epoch 5/10  Iteration 773/1780 Training loss: 1.7224 13.0664 sec/batch\n",
      "Epoch 5/10  Iteration 774/1780 Training loss: 1.7229 17.7389 sec/batch\n",
      "Epoch 5/10  Iteration 775/1780 Training loss: 1.7230 17.9653 sec/batch\n",
      "Epoch 5/10  Iteration 776/1780 Training loss: 1.7229 14.5947 sec/batch\n",
      "Epoch 5/10  Iteration 777/1780 Training loss: 1.7226 15.4463 sec/batch\n",
      "Epoch 5/10  Iteration 778/1780 Training loss: 1.7226 14.5872 sec/batch\n",
      "Epoch 5/10  Iteration 779/1780 Training loss: 1.7225 16.4577 sec/batch\n",
      "Epoch 5/10  Iteration 780/1780 Training loss: 1.7218 15.0086 sec/batch\n",
      "Epoch 5/10  Iteration 781/1780 Training loss: 1.7215 13.8929 sec/batch\n",
      "Epoch 5/10  Iteration 782/1780 Training loss: 1.7212 11.9135 sec/batch\n",
      "Epoch 5/10  Iteration 783/1780 Training loss: 1.7215 14.9213 sec/batch\n",
      "Epoch 5/10  Iteration 784/1780 Training loss: 1.7215 14.0115 sec/batch\n",
      "Epoch 5/10  Iteration 785/1780 Training loss: 1.7217 12.9477 sec/batch\n",
      "Epoch 5/10  Iteration 786/1780 Training loss: 1.7211 15.4928 sec/batch\n",
      "Epoch 5/10  Iteration 787/1780 Training loss: 1.7207 16.3244 sec/batch\n",
      "Epoch 5/10  Iteration 788/1780 Training loss: 1.7206 14.3274 sec/batch\n",
      "Epoch 5/10  Iteration 789/1780 Training loss: 1.7203 15.4434 sec/batch\n",
      "Epoch 5/10  Iteration 790/1780 Training loss: 1.7201 15.6707 sec/batch\n",
      "Epoch 5/10  Iteration 791/1780 Training loss: 1.7193 15.3017 sec/batch\n",
      "Epoch 5/10  Iteration 792/1780 Training loss: 1.7190 17.8874 sec/batch\n",
      "Epoch 5/10  Iteration 793/1780 Training loss: 1.7182 16.1065 sec/batch\n",
      "Epoch 5/10  Iteration 794/1780 Training loss: 1.7181 16.2166 sec/batch\n",
      "Epoch 5/10  Iteration 795/1780 Training loss: 1.7173 14.0325 sec/batch\n",
      "Epoch 5/10  Iteration 796/1780 Training loss: 1.7171 15.8694 sec/batch\n",
      "Epoch 5/10  Iteration 797/1780 Training loss: 1.7166 14.7026 sec/batch\n",
      "Epoch 5/10  Iteration 798/1780 Training loss: 1.7162 12.7439 sec/batch\n",
      "Epoch 5/10  Iteration 799/1780 Training loss: 1.7158 17.3922 sec/batch\n",
      "Epoch 5/10  Iteration 800/1780 Training loss: 1.7155 17.4677 sec/batch\n",
      "Validation loss: 1.57685 Saving checkpoint!\n",
      "Epoch 5/10  Iteration 801/1780 Training loss: 1.7155 13.8956 sec/batch\n",
      "Epoch 5/10  Iteration 802/1780 Training loss: 1.7155 13.4298 sec/batch\n",
      "Epoch 5/10  Iteration 803/1780 Training loss: 1.7150 14.7602 sec/batch\n",
      "Epoch 5/10  Iteration 804/1780 Training loss: 1.7146 17.1144 sec/batch\n",
      "Epoch 5/10  Iteration 805/1780 Training loss: 1.7139 16.3574 sec/batch\n",
      "Epoch 5/10  Iteration 806/1780 Training loss: 1.7134 19.7540 sec/batch\n",
      "Epoch 5/10  Iteration 807/1780 Training loss: 1.7128 15.3467 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10  Iteration 808/1780 Training loss: 1.7125 17.6544 sec/batch\n",
      "Epoch 5/10  Iteration 809/1780 Training loss: 1.7122 15.2909 sec/batch\n",
      "Epoch 5/10  Iteration 810/1780 Training loss: 1.7116 19.0527 sec/batch\n",
      "Epoch 5/10  Iteration 811/1780 Training loss: 1.7111 15.2456 sec/batch\n",
      "Epoch 5/10  Iteration 812/1780 Training loss: 1.7103 16.3975 sec/batch\n",
      "Epoch 5/10  Iteration 813/1780 Training loss: 1.7100 17.8573 sec/batch\n",
      "Epoch 5/10  Iteration 814/1780 Training loss: 1.7096 16.6621 sec/batch\n",
      "Epoch 5/10  Iteration 815/1780 Training loss: 1.7092 18.9152 sec/batch\n",
      "Epoch 5/10  Iteration 816/1780 Training loss: 1.7086 17.5868 sec/batch\n",
      "Epoch 5/10  Iteration 817/1780 Training loss: 1.7081 18.6115 sec/batch\n",
      "Epoch 5/10  Iteration 818/1780 Training loss: 1.7077 20.2528 sec/batch\n",
      "Epoch 5/10  Iteration 819/1780 Training loss: 1.7074 20.2837 sec/batch\n",
      "Epoch 5/10  Iteration 820/1780 Training loss: 1.7070 14.5191 sec/batch\n",
      "Epoch 5/10  Iteration 821/1780 Training loss: 1.7068 14.4185 sec/batch\n",
      "Epoch 5/10  Iteration 822/1780 Training loss: 1.7065 15.8592 sec/batch\n",
      "Epoch 5/10  Iteration 823/1780 Training loss: 1.7061 14.8018 sec/batch\n",
      "Epoch 5/10  Iteration 824/1780 Training loss: 1.7057 15.0661 sec/batch\n",
      "Epoch 5/10  Iteration 825/1780 Training loss: 1.7054 13.7926 sec/batch\n",
      "Epoch 5/10  Iteration 826/1780 Training loss: 1.7051 16.8399 sec/batch\n",
      "Epoch 5/10  Iteration 827/1780 Training loss: 1.7045 18.8938 sec/batch\n",
      "Epoch 5/10  Iteration 828/1780 Training loss: 1.7039 16.3434 sec/batch\n",
      "Epoch 5/10  Iteration 829/1780 Training loss: 1.7035 14.4542 sec/batch\n",
      "Epoch 5/10  Iteration 830/1780 Training loss: 1.7031 16.1591 sec/batch\n",
      "Epoch 5/10  Iteration 831/1780 Training loss: 1.7026 16.9271 sec/batch\n",
      "Epoch 5/10  Iteration 832/1780 Training loss: 1.7023 16.4700 sec/batch\n",
      "Epoch 5/10  Iteration 833/1780 Training loss: 1.7020 14.0670 sec/batch\n",
      "Epoch 5/10  Iteration 834/1780 Training loss: 1.7013 18.1048 sec/batch\n",
      "Epoch 5/10  Iteration 835/1780 Training loss: 1.7007 17.3893 sec/batch\n",
      "Epoch 5/10  Iteration 836/1780 Training loss: 1.7004 18.0214 sec/batch\n",
      "Epoch 5/10  Iteration 837/1780 Training loss: 1.7001 17.6251 sec/batch\n",
      "Epoch 5/10  Iteration 838/1780 Training loss: 1.6995 15.0366 sec/batch\n",
      "Epoch 5/10  Iteration 839/1780 Training loss: 1.6993 15.6913 sec/batch\n",
      "Epoch 5/10  Iteration 840/1780 Training loss: 1.6991 18.8025 sec/batch\n",
      "Epoch 5/10  Iteration 841/1780 Training loss: 1.6987 16.5080 sec/batch\n",
      "Epoch 5/10  Iteration 842/1780 Training loss: 1.6983 13.9703 sec/batch\n",
      "Epoch 5/10  Iteration 843/1780 Training loss: 1.6977 14.2997 sec/batch\n",
      "Epoch 5/10  Iteration 844/1780 Training loss: 1.6972 15.2350 sec/batch\n",
      "Epoch 5/10  Iteration 845/1780 Training loss: 1.6970 13.6788 sec/batch\n",
      "Epoch 5/10  Iteration 846/1780 Training loss: 1.6967 14.3665 sec/batch\n",
      "Epoch 5/10  Iteration 847/1780 Training loss: 1.6964 15.2477 sec/batch\n",
      "Epoch 5/10  Iteration 848/1780 Training loss: 1.6962 14.4931 sec/batch\n",
      "Epoch 5/10  Iteration 849/1780 Training loss: 1.6960 15.9280 sec/batch\n",
      "Epoch 5/10  Iteration 850/1780 Training loss: 1.6957 17.7165 sec/batch\n",
      "Epoch 5/10  Iteration 851/1780 Training loss: 1.6956 16.1323 sec/batch\n",
      "Epoch 5/10  Iteration 852/1780 Training loss: 1.6952 16.8684 sec/batch\n",
      "Epoch 5/10  Iteration 853/1780 Training loss: 1.6952 17.3407 sec/batch\n",
      "Epoch 5/10  Iteration 854/1780 Training loss: 1.6949 15.1229 sec/batch\n",
      "Epoch 5/10  Iteration 855/1780 Training loss: 1.6946 15.9383 sec/batch\n",
      "Epoch 5/10  Iteration 856/1780 Training loss: 1.6944 15.8419 sec/batch\n",
      "Epoch 5/10  Iteration 857/1780 Training loss: 1.6940 16.6822 sec/batch\n",
      "Epoch 5/10  Iteration 858/1780 Training loss: 1.6938 17.8481 sec/batch\n",
      "Epoch 5/10  Iteration 859/1780 Training loss: 1.6936 18.0454 sec/batch\n",
      "Epoch 5/10  Iteration 860/1780 Training loss: 1.6936 13.6039 sec/batch\n",
      "Epoch 5/10  Iteration 861/1780 Training loss: 1.6933 13.7077 sec/batch\n",
      "Epoch 5/10  Iteration 862/1780 Training loss: 1.6930 13.9576 sec/batch\n",
      "Epoch 5/10  Iteration 863/1780 Training loss: 1.6925 12.1838 sec/batch\n",
      "Epoch 5/10  Iteration 864/1780 Training loss: 1.6924 12.6317 sec/batch\n",
      "Epoch 5/10  Iteration 865/1780 Training loss: 1.6922 11.9634 sec/batch\n",
      "Epoch 5/10  Iteration 866/1780 Training loss: 1.6920 12.5455 sec/batch\n",
      "Epoch 5/10  Iteration 867/1780 Training loss: 1.6918 12.0361 sec/batch\n",
      "Epoch 5/10  Iteration 868/1780 Training loss: 1.6915 13.0988 sec/batch\n",
      "Epoch 5/10  Iteration 869/1780 Training loss: 1.6913 12.1746 sec/batch\n",
      "Epoch 5/10  Iteration 870/1780 Training loss: 1.6910 13.1229 sec/batch\n",
      "Epoch 5/10  Iteration 871/1780 Training loss: 1.6905 11.8367 sec/batch\n",
      "Epoch 5/10  Iteration 872/1780 Training loss: 1.6903 11.7648 sec/batch\n",
      "Epoch 5/10  Iteration 873/1780 Training loss: 1.6903 11.7215 sec/batch\n",
      "Epoch 5/10  Iteration 874/1780 Training loss: 1.6901 12.2341 sec/batch\n",
      "Epoch 5/10  Iteration 875/1780 Training loss: 1.6898 14.0620 sec/batch\n",
      "Epoch 5/10  Iteration 876/1780 Training loss: 1.6896 12.6316 sec/batch\n",
      "Epoch 5/10  Iteration 877/1780 Training loss: 1.6893 11.3521 sec/batch\n",
      "Epoch 5/10  Iteration 878/1780 Training loss: 1.6890 11.9613 sec/batch\n",
      "Epoch 5/10  Iteration 879/1780 Training loss: 1.6889 14.7263 sec/batch\n",
      "Epoch 5/10  Iteration 880/1780 Training loss: 1.6890 13.2966 sec/batch\n",
      "Epoch 5/10  Iteration 881/1780 Training loss: 1.6887 15.8090 sec/batch\n",
      "Epoch 5/10  Iteration 882/1780 Training loss: 1.6885 14.5303 sec/batch\n",
      "Epoch 5/10  Iteration 883/1780 Training loss: 1.6881 16.6898 sec/batch\n",
      "Epoch 5/10  Iteration 884/1780 Training loss: 1.6877 14.6186 sec/batch\n",
      "Epoch 5/10  Iteration 885/1780 Training loss: 1.6875 14.8825 sec/batch\n",
      "Epoch 5/10  Iteration 886/1780 Training loss: 1.6873 18.6048 sec/batch\n",
      "Epoch 5/10  Iteration 887/1780 Training loss: 1.6871 18.1455 sec/batch\n",
      "Epoch 5/10  Iteration 888/1780 Training loss: 1.6868 16.6802 sec/batch\n",
      "Epoch 5/10  Iteration 889/1780 Training loss: 1.6864 18.2724 sec/batch\n",
      "Epoch 5/10  Iteration 890/1780 Training loss: 1.6863 17.3715 sec/batch\n",
      "Epoch 6/10  Iteration 891/1780 Training loss: 1.7288 16.0840 sec/batch\n",
      "Epoch 6/10  Iteration 892/1780 Training loss: 1.6841 15.5584 sec/batch\n",
      "Epoch 6/10  Iteration 893/1780 Training loss: 1.6655 13.2942 sec/batch\n",
      "Epoch 6/10  Iteration 894/1780 Training loss: 1.6578 14.2138 sec/batch\n",
      "Epoch 6/10  Iteration 895/1780 Training loss: 1.6513 14.8119 sec/batch\n",
      "Epoch 6/10  Iteration 896/1780 Training loss: 1.6404 14.1736 sec/batch\n",
      "Epoch 6/10  Iteration 897/1780 Training loss: 1.6414 16.6120 sec/batch\n",
      "Epoch 6/10  Iteration 898/1780 Training loss: 1.6400 16.5423 sec/batch\n",
      "Epoch 6/10  Iteration 899/1780 Training loss: 1.6414 18.5034 sec/batch\n",
      "Epoch 6/10  Iteration 900/1780 Training loss: 1.6401 17.3990 sec/batch\n",
      "Validation loss: 1.51153 Saving checkpoint!\n",
      "Epoch 6/10  Iteration 901/1780 Training loss: 1.6425 14.4163 sec/batch\n",
      "Epoch 6/10  Iteration 902/1780 Training loss: 1.6408 17.0737 sec/batch\n",
      "Epoch 6/10  Iteration 903/1780 Training loss: 1.6395 16.9357 sec/batch\n",
      "Epoch 6/10  Iteration 904/1780 Training loss: 1.6408 16.6736 sec/batch\n",
      "Epoch 6/10  Iteration 905/1780 Training loss: 1.6392 16.7602 sec/batch\n",
      "Epoch 6/10  Iteration 906/1780 Training loss: 1.6374 11.8703 sec/batch\n",
      "Epoch 6/10  Iteration 907/1780 Training loss: 1.6368 11.8929 sec/batch\n",
      "Epoch 6/10  Iteration 908/1780 Training loss: 1.6383 12.0588 sec/batch\n",
      "Epoch 6/10  Iteration 909/1780 Training loss: 1.6382 13.9990 sec/batch\n",
      "Epoch 6/10  Iteration 910/1780 Training loss: 1.6389 13.9779 sec/batch\n",
      "Epoch 6/10  Iteration 911/1780 Training loss: 1.6380 15.3069 sec/batch\n",
      "Epoch 6/10  Iteration 912/1780 Training loss: 1.6386 13.2323 sec/batch\n",
      "Epoch 6/10  Iteration 913/1780 Training loss: 1.6375 13.4641 sec/batch\n",
      "Epoch 6/10  Iteration 914/1780 Training loss: 1.6370 13.1556 sec/batch\n",
      "Epoch 6/10  Iteration 915/1780 Training loss: 1.6364 13.4335 sec/batch\n",
      "Epoch 6/10  Iteration 916/1780 Training loss: 1.6350 13.9341 sec/batch\n",
      "Epoch 6/10  Iteration 917/1780 Training loss: 1.6335 12.8242 sec/batch\n",
      "Epoch 6/10  Iteration 918/1780 Training loss: 1.6333 13.4042 sec/batch\n",
      "Epoch 6/10  Iteration 919/1780 Training loss: 1.6336 17.6377 sec/batch\n",
      "Epoch 6/10  Iteration 920/1780 Training loss: 1.6334 19.2812 sec/batch\n",
      "Epoch 6/10  Iteration 921/1780 Training loss: 1.6330 13.6341 sec/batch\n",
      "Epoch 6/10  Iteration 922/1780 Training loss: 1.6317 15.1378 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10  Iteration 923/1780 Training loss: 1.6317 13.1841 sec/batch\n",
      "Epoch 6/10  Iteration 924/1780 Training loss: 1.6318 18.2084 sec/batch\n",
      "Epoch 6/10  Iteration 925/1780 Training loss: 1.6312 13.7539 sec/batch\n",
      "Epoch 6/10  Iteration 926/1780 Training loss: 1.6309 16.6957 sec/batch\n",
      "Epoch 6/10  Iteration 927/1780 Training loss: 1.6298 15.0020 sec/batch\n",
      "Epoch 6/10  Iteration 928/1780 Training loss: 1.6284 13.7993 sec/batch\n",
      "Epoch 6/10  Iteration 929/1780 Training loss: 1.6268 12.9770 sec/batch\n",
      "Epoch 6/10  Iteration 930/1780 Training loss: 1.6261 15.3195 sec/batch\n",
      "Epoch 6/10  Iteration 931/1780 Training loss: 1.6254 15.3128 sec/batch\n",
      "Epoch 6/10  Iteration 932/1780 Training loss: 1.6256 14.5837 sec/batch\n",
      "Epoch 6/10  Iteration 933/1780 Training loss: 1.6248 16.2066 sec/batch\n",
      "Epoch 6/10  Iteration 934/1780 Training loss: 1.6235 13.2182 sec/batch\n",
      "Epoch 6/10  Iteration 935/1780 Training loss: 1.6234 13.8582 sec/batch\n",
      "Epoch 6/10  Iteration 936/1780 Training loss: 1.6222 17.9208 sec/batch\n",
      "Epoch 6/10  Iteration 937/1780 Training loss: 1.6216 12.9020 sec/batch\n",
      "Epoch 6/10  Iteration 938/1780 Training loss: 1.6209 12.5826 sec/batch\n",
      "Epoch 6/10  Iteration 939/1780 Training loss: 1.6205 15.0775 sec/batch\n",
      "Epoch 6/10  Iteration 940/1780 Training loss: 1.6207 13.8149 sec/batch\n",
      "Epoch 6/10  Iteration 941/1780 Training loss: 1.6199 13.0070 sec/batch\n",
      "Epoch 6/10  Iteration 942/1780 Training loss: 1.6205 12.8961 sec/batch\n",
      "Epoch 6/10  Iteration 943/1780 Training loss: 1.6203 12.1851 sec/batch\n",
      "Epoch 6/10  Iteration 944/1780 Training loss: 1.6200 12.2505 sec/batch\n",
      "Epoch 6/10  Iteration 945/1780 Training loss: 1.6196 12.3938 sec/batch\n",
      "Epoch 6/10  Iteration 946/1780 Training loss: 1.6195 12.2166 sec/batch\n",
      "Epoch 6/10  Iteration 947/1780 Training loss: 1.6198 12.3627 sec/batch\n",
      "Epoch 6/10  Iteration 948/1780 Training loss: 1.6194 12.1386 sec/batch\n",
      "Epoch 6/10  Iteration 949/1780 Training loss: 1.6187 12.3768 sec/batch\n",
      "Epoch 6/10  Iteration 950/1780 Training loss: 1.6191 12.5347 sec/batch\n",
      "Epoch 6/10  Iteration 951/1780 Training loss: 1.6187 11.9829 sec/batch\n",
      "Epoch 6/10  Iteration 952/1780 Training loss: 1.6196 11.8389 sec/batch\n",
      "Epoch 6/10  Iteration 953/1780 Training loss: 1.6198 12.0902 sec/batch\n",
      "Epoch 6/10  Iteration 954/1780 Training loss: 1.6199 12.1583 sec/batch\n",
      "Epoch 6/10  Iteration 955/1780 Training loss: 1.6197 12.1359 sec/batch\n",
      "Epoch 6/10  Iteration 956/1780 Training loss: 1.6197 12.3118 sec/batch\n",
      "Epoch 6/10  Iteration 957/1780 Training loss: 1.6198 12.2309 sec/batch\n",
      "Epoch 6/10  Iteration 958/1780 Training loss: 1.6192 12.1578 sec/batch\n",
      "Epoch 6/10  Iteration 959/1780 Training loss: 1.6191 12.2610 sec/batch\n",
      "Epoch 6/10  Iteration 960/1780 Training loss: 1.6188 11.8501 sec/batch\n",
      "Epoch 6/10  Iteration 961/1780 Training loss: 1.6191 11.6656 sec/batch\n",
      "Epoch 6/10  Iteration 962/1780 Training loss: 1.6190 11.8520 sec/batch\n",
      "Epoch 6/10  Iteration 963/1780 Training loss: 1.6192 12.2148 sec/batch\n",
      "Epoch 6/10  Iteration 964/1780 Training loss: 1.6188 12.1317 sec/batch\n",
      "Epoch 6/10  Iteration 965/1780 Training loss: 1.6185 12.2506 sec/batch\n",
      "Epoch 6/10  Iteration 966/1780 Training loss: 1.6186 12.0759 sec/batch\n",
      "Epoch 6/10  Iteration 967/1780 Training loss: 1.6183 12.2741 sec/batch\n",
      "Epoch 6/10  Iteration 968/1780 Training loss: 1.6181 11.9726 sec/batch\n",
      "Epoch 6/10  Iteration 969/1780 Training loss: 1.6173 12.0068 sec/batch\n",
      "Epoch 6/10  Iteration 970/1780 Training loss: 1.6170 12.0067 sec/batch\n",
      "Epoch 6/10  Iteration 971/1780 Training loss: 1.6163 11.9515 sec/batch\n",
      "Epoch 6/10  Iteration 972/1780 Training loss: 1.6163 12.2319 sec/batch\n",
      "Epoch 6/10  Iteration 973/1780 Training loss: 1.6156 12.0937 sec/batch\n",
      "Epoch 6/10  Iteration 974/1780 Training loss: 1.6154 12.1309 sec/batch\n",
      "Epoch 6/10  Iteration 975/1780 Training loss: 1.6149 12.1074 sec/batch\n",
      "Epoch 6/10  Iteration 976/1780 Training loss: 1.6145 12.0808 sec/batch\n",
      "Epoch 6/10  Iteration 977/1780 Training loss: 1.6140 12.2511 sec/batch\n",
      "Epoch 6/10  Iteration 978/1780 Training loss: 1.6136 12.2423 sec/batch\n",
      "Epoch 6/10  Iteration 979/1780 Training loss: 1.6129 12.2139 sec/batch\n",
      "Epoch 6/10  Iteration 980/1780 Training loss: 1.6128 12.1269 sec/batch\n",
      "Epoch 6/10  Iteration 981/1780 Training loss: 1.6123 12.2259 sec/batch\n",
      "Epoch 6/10  Iteration 982/1780 Training loss: 1.6120 12.1906 sec/batch\n",
      "Epoch 6/10  Iteration 983/1780 Training loss: 1.6114 12.0836 sec/batch\n",
      "Epoch 6/10  Iteration 984/1780 Training loss: 1.6109 12.2973 sec/batch\n",
      "Epoch 6/10  Iteration 985/1780 Training loss: 1.6104 12.1246 sec/batch\n",
      "Epoch 6/10  Iteration 986/1780 Training loss: 1.6102 12.1985 sec/batch\n",
      "Epoch 6/10  Iteration 987/1780 Training loss: 1.6099 12.2866 sec/batch\n",
      "Epoch 6/10  Iteration 988/1780 Training loss: 1.6092 12.0949 sec/batch\n",
      "Epoch 6/10  Iteration 989/1780 Training loss: 1.6087 12.0430 sec/batch\n",
      "Epoch 6/10  Iteration 990/1780 Training loss: 1.6080 12.1522 sec/batch\n",
      "Epoch 6/10  Iteration 991/1780 Training loss: 1.6078 12.0675 sec/batch\n",
      "Epoch 6/10  Iteration 992/1780 Training loss: 1.6075 12.2054 sec/batch\n",
      "Epoch 6/10  Iteration 993/1780 Training loss: 1.6071 12.1875 sec/batch\n",
      "Epoch 6/10  Iteration 994/1780 Training loss: 1.6068 12.0809 sec/batch\n",
      "Epoch 6/10  Iteration 995/1780 Training loss: 1.6064 11.9894 sec/batch\n",
      "Epoch 6/10  Iteration 996/1780 Training loss: 1.6061 12.0986 sec/batch\n",
      "Epoch 6/10  Iteration 997/1780 Training loss: 1.6059 12.0937 sec/batch\n",
      "Epoch 6/10  Iteration 998/1780 Training loss: 1.6057 12.0972 sec/batch\n",
      "Epoch 6/10  Iteration 999/1780 Training loss: 1.6055 12.0725 sec/batch\n",
      "Epoch 6/10  Iteration 1000/1780 Training loss: 1.6053 11.9805 sec/batch\n",
      "Validation loss: 1.46968 Saving checkpoint!\n",
      "Epoch 6/10  Iteration 1001/1780 Training loss: 1.6057 12.1083 sec/batch\n",
      "Epoch 6/10  Iteration 1002/1780 Training loss: 1.6054 12.0778 sec/batch\n",
      "Epoch 6/10  Iteration 1003/1780 Training loss: 1.6051 12.1821 sec/batch\n",
      "Epoch 6/10  Iteration 1004/1780 Training loss: 1.6047 12.1192 sec/batch\n",
      "Epoch 6/10  Iteration 1005/1780 Training loss: 1.6043 12.0958 sec/batch\n",
      "Epoch 6/10  Iteration 1006/1780 Training loss: 1.6038 12.2311 sec/batch\n",
      "Epoch 6/10  Iteration 1007/1780 Training loss: 1.6035 12.2646 sec/batch\n",
      "Epoch 6/10  Iteration 1008/1780 Training loss: 1.6032 12.2380 sec/batch\n",
      "Epoch 6/10  Iteration 1009/1780 Training loss: 1.6029 12.2491 sec/batch\n",
      "Epoch 6/10  Iteration 1010/1780 Training loss: 1.6026 12.1551 sec/batch\n",
      "Epoch 6/10  Iteration 1011/1780 Training loss: 1.6024 11.9537 sec/batch\n",
      "Epoch 6/10  Iteration 1012/1780 Training loss: 1.6020 12.2110 sec/batch\n",
      "Epoch 6/10  Iteration 1013/1780 Training loss: 1.6015 12.1444 sec/batch\n",
      "Epoch 6/10  Iteration 1014/1780 Training loss: 1.6014 12.1118 sec/batch\n",
      "Epoch 6/10  Iteration 1015/1780 Training loss: 1.6011 12.1637 sec/batch\n",
      "Epoch 6/10  Iteration 1016/1780 Training loss: 1.6005 12.2236 sec/batch\n",
      "Epoch 6/10  Iteration 1017/1780 Training loss: 1.6004 12.2659 sec/batch\n",
      "Epoch 6/10  Iteration 1018/1780 Training loss: 1.6003 12.1170 sec/batch\n",
      "Epoch 6/10  Iteration 1019/1780 Training loss: 1.5999 13.0914 sec/batch\n",
      "Epoch 6/10  Iteration 1020/1780 Training loss: 1.5995 12.1511 sec/batch\n",
      "Epoch 6/10  Iteration 1021/1780 Training loss: 1.5990 12.2605 sec/batch\n",
      "Epoch 6/10  Iteration 1022/1780 Training loss: 1.5985 12.1432 sec/batch\n",
      "Epoch 6/10  Iteration 1023/1780 Training loss: 1.5985 12.2245 sec/batch\n",
      "Epoch 6/10  Iteration 1024/1780 Training loss: 1.5983 12.0775 sec/batch\n",
      "Epoch 6/10  Iteration 1025/1780 Training loss: 1.5981 12.2521 sec/batch\n",
      "Epoch 6/10  Iteration 1026/1780 Training loss: 1.5980 11.9644 sec/batch\n",
      "Epoch 6/10  Iteration 1027/1780 Training loss: 1.5979 12.1324 sec/batch\n",
      "Epoch 6/10  Iteration 1028/1780 Training loss: 1.5977 11.9914 sec/batch\n",
      "Epoch 6/10  Iteration 1029/1780 Training loss: 1.5976 11.9990 sec/batch\n",
      "Epoch 6/10  Iteration 1030/1780 Training loss: 1.5973 12.2262 sec/batch\n",
      "Epoch 6/10  Iteration 1031/1780 Training loss: 1.5975 12.1659 sec/batch\n",
      "Epoch 6/10  Iteration 1032/1780 Training loss: 1.5973 11.8635 sec/batch\n",
      "Epoch 6/10  Iteration 1033/1780 Training loss: 1.5970 11.6594 sec/batch\n",
      "Epoch 6/10  Iteration 1034/1780 Training loss: 1.5970 11.7046 sec/batch\n",
      "Epoch 6/10  Iteration 1035/1780 Training loss: 1.5967 11.8946 sec/batch\n",
      "Epoch 6/10  Iteration 1036/1780 Training loss: 1.5966 11.9011 sec/batch\n",
      "Epoch 6/10  Iteration 1037/1780 Training loss: 1.5965 12.0230 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10  Iteration 1038/1780 Training loss: 1.5965 11.7159 sec/batch\n",
      "Epoch 6/10  Iteration 1039/1780 Training loss: 1.5964 11.7980 sec/batch\n",
      "Epoch 6/10  Iteration 1040/1780 Training loss: 1.5961 11.7983 sec/batch\n",
      "Epoch 6/10  Iteration 1041/1780 Training loss: 1.5956 11.8575 sec/batch\n",
      "Epoch 6/10  Iteration 1042/1780 Training loss: 1.5955 11.9526 sec/batch\n",
      "Epoch 6/10  Iteration 1043/1780 Training loss: 1.5953 11.8460 sec/batch\n",
      "Epoch 6/10  Iteration 1044/1780 Training loss: 1.5951 11.9011 sec/batch\n",
      "Epoch 6/10  Iteration 1045/1780 Training loss: 1.5949 12.1588 sec/batch\n",
      "Epoch 6/10  Iteration 1046/1780 Training loss: 1.5947 12.1617 sec/batch\n",
      "Epoch 6/10  Iteration 1047/1780 Training loss: 1.5946 12.2505 sec/batch\n",
      "Epoch 6/10  Iteration 1048/1780 Training loss: 1.5944 11.7990 sec/batch\n",
      "Epoch 6/10  Iteration 1049/1780 Training loss: 1.5939 11.8103 sec/batch\n",
      "Epoch 6/10  Iteration 1050/1780 Training loss: 1.5938 12.1580 sec/batch\n",
      "Epoch 6/10  Iteration 1051/1780 Training loss: 1.5938 11.8137 sec/batch\n",
      "Epoch 6/10  Iteration 1052/1780 Training loss: 1.5935 11.7879 sec/batch\n",
      "Epoch 6/10  Iteration 1053/1780 Training loss: 1.5934 11.7724 sec/batch\n",
      "Epoch 6/10  Iteration 1054/1780 Training loss: 1.5931 11.8607 sec/batch\n",
      "Epoch 6/10  Iteration 1055/1780 Training loss: 1.5930 12.2413 sec/batch\n",
      "Epoch 6/10  Iteration 1056/1780 Training loss: 1.5928 12.0195 sec/batch\n",
      "Epoch 6/10  Iteration 1057/1780 Training loss: 1.5928 12.1028 sec/batch\n",
      "Epoch 6/10  Iteration 1058/1780 Training loss: 1.5930 12.2465 sec/batch\n",
      "Epoch 6/10  Iteration 1059/1780 Training loss: 1.5928 12.2071 sec/batch\n",
      "Epoch 6/10  Iteration 1060/1780 Training loss: 1.5926 12.0420 sec/batch\n",
      "Epoch 6/10  Iteration 1061/1780 Training loss: 1.5923 12.0559 sec/batch\n",
      "Epoch 6/10  Iteration 1062/1780 Training loss: 1.5920 12.2357 sec/batch\n",
      "Epoch 6/10  Iteration 1063/1780 Training loss: 1.5919 12.0702 sec/batch\n",
      "Epoch 6/10  Iteration 1064/1780 Training loss: 1.5918 12.1396 sec/batch\n",
      "Epoch 6/10  Iteration 1065/1780 Training loss: 1.5918 12.0274 sec/batch\n",
      "Epoch 6/10  Iteration 1066/1780 Training loss: 1.5915 12.0395 sec/batch\n",
      "Epoch 6/10  Iteration 1067/1780 Training loss: 1.5912 12.1840 sec/batch\n",
      "Epoch 6/10  Iteration 1068/1780 Training loss: 1.5911 12.1669 sec/batch\n",
      "Epoch 7/10  Iteration 1069/1780 Training loss: 1.6499 12.1927 sec/batch\n",
      "Epoch 7/10  Iteration 1070/1780 Training loss: 1.6084 12.2222 sec/batch\n",
      "Epoch 7/10  Iteration 1071/1780 Training loss: 1.5911 12.1419 sec/batch\n",
      "Epoch 7/10  Iteration 1072/1780 Training loss: 1.5822 12.1264 sec/batch\n",
      "Epoch 7/10  Iteration 1073/1780 Training loss: 1.5736 12.0377 sec/batch\n",
      "Epoch 7/10  Iteration 1074/1780 Training loss: 1.5604 12.0692 sec/batch\n",
      "Epoch 7/10  Iteration 1075/1780 Training loss: 1.5608 12.0654 sec/batch\n",
      "Epoch 7/10  Iteration 1076/1780 Training loss: 1.5586 12.0455 sec/batch\n",
      "Epoch 7/10  Iteration 1077/1780 Training loss: 1.5588 12.1280 sec/batch\n",
      "Epoch 7/10  Iteration 1078/1780 Training loss: 1.5579 12.2196 sec/batch\n",
      "Epoch 7/10  Iteration 1079/1780 Training loss: 1.5549 12.2482 sec/batch\n",
      "Epoch 7/10  Iteration 1080/1780 Training loss: 1.5539 12.1517 sec/batch\n",
      "Epoch 7/10  Iteration 1081/1780 Training loss: 1.5529 12.1895 sec/batch\n",
      "Epoch 7/10  Iteration 1082/1780 Training loss: 1.5547 12.2450 sec/batch\n",
      "Epoch 7/10  Iteration 1083/1780 Training loss: 1.5535 12.2033 sec/batch\n",
      "Epoch 7/10  Iteration 1084/1780 Training loss: 1.5518 12.1306 sec/batch\n",
      "Epoch 7/10  Iteration 1085/1780 Training loss: 1.5518 12.0521 sec/batch\n",
      "Epoch 7/10  Iteration 1086/1780 Training loss: 1.5527 12.0643 sec/batch\n",
      "Epoch 7/10  Iteration 1087/1780 Training loss: 1.5526 12.2220 sec/batch\n",
      "Epoch 7/10  Iteration 1088/1780 Training loss: 1.5534 12.1792 sec/batch\n",
      "Epoch 7/10  Iteration 1089/1780 Training loss: 1.5524 12.0225 sec/batch\n",
      "Epoch 7/10  Iteration 1090/1780 Training loss: 1.5529 11.9543 sec/batch\n",
      "Epoch 7/10  Iteration 1091/1780 Training loss: 1.5515 12.0269 sec/batch\n",
      "Epoch 7/10  Iteration 1092/1780 Training loss: 1.5517 11.9798 sec/batch\n",
      "Epoch 7/10  Iteration 1093/1780 Training loss: 1.5516 12.0017 sec/batch\n",
      "Epoch 7/10  Iteration 1094/1780 Training loss: 1.5500 12.0340 sec/batch\n",
      "Epoch 7/10  Iteration 1095/1780 Training loss: 1.5486 12.1520 sec/batch\n",
      "Epoch 7/10  Iteration 1096/1780 Training loss: 1.5488 12.1982 sec/batch\n",
      "Epoch 7/10  Iteration 1097/1780 Training loss: 1.5491 12.0773 sec/batch\n",
      "Epoch 7/10  Iteration 1098/1780 Training loss: 1.5494 12.0495 sec/batch\n",
      "Epoch 7/10  Iteration 1099/1780 Training loss: 1.5490 12.0147 sec/batch\n",
      "Epoch 7/10  Iteration 1100/1780 Training loss: 1.5478 12.0826 sec/batch\n",
      "Validation loss: 1.42471 Saving checkpoint!\n",
      "Epoch 7/10  Iteration 1101/1780 Training loss: 1.5508 11.7414 sec/batch\n",
      "Epoch 7/10  Iteration 1102/1780 Training loss: 1.5512 11.8074 sec/batch\n",
      "Epoch 7/10  Iteration 1103/1780 Training loss: 1.5508 12.0672 sec/batch\n",
      "Epoch 7/10  Iteration 1104/1780 Training loss: 1.5508 12.0775 sec/batch\n",
      "Epoch 7/10  Iteration 1105/1780 Training loss: 1.5498 12.1572 sec/batch\n",
      "Epoch 7/10  Iteration 1106/1780 Training loss: 1.5486 12.2028 sec/batch\n",
      "Epoch 7/10  Iteration 1107/1780 Training loss: 1.5471 12.0499 sec/batch\n",
      "Epoch 7/10  Iteration 1108/1780 Training loss: 1.5465 12.0968 sec/batch\n",
      "Epoch 7/10  Iteration 1109/1780 Training loss: 1.5458 11.9413 sec/batch\n",
      "Epoch 7/10  Iteration 1110/1780 Training loss: 1.5465 12.1937 sec/batch\n",
      "Epoch 7/10  Iteration 1111/1780 Training loss: 1.5459 11.9422 sec/batch\n",
      "Epoch 7/10  Iteration 1112/1780 Training loss: 1.5451 12.0746 sec/batch\n",
      "Epoch 7/10  Iteration 1113/1780 Training loss: 1.5452 11.9543 sec/batch\n",
      "Epoch 7/10  Iteration 1114/1780 Training loss: 1.5441 12.0590 sec/batch\n",
      "Epoch 7/10  Iteration 1115/1780 Training loss: 1.5438 12.1290 sec/batch\n",
      "Epoch 7/10  Iteration 1116/1780 Training loss: 1.5432 12.0562 sec/batch\n",
      "Epoch 7/10  Iteration 1117/1780 Training loss: 1.5429 12.1275 sec/batch\n",
      "Epoch 7/10  Iteration 1118/1780 Training loss: 1.5432 12.1632 sec/batch\n",
      "Epoch 7/10  Iteration 1119/1780 Training loss: 1.5425 12.1119 sec/batch\n",
      "Epoch 7/10  Iteration 1120/1780 Training loss: 1.5432 12.0949 sec/batch\n",
      "Epoch 7/10  Iteration 1121/1780 Training loss: 1.5430 12.1596 sec/batch\n",
      "Epoch 7/10  Iteration 1122/1780 Training loss: 1.5430 13.1284 sec/batch\n",
      "Epoch 7/10  Iteration 1123/1780 Training loss: 1.5426 16.0775 sec/batch\n",
      "Epoch 7/10  Iteration 1124/1780 Training loss: 1.5425 13.7352 sec/batch\n",
      "Epoch 7/10  Iteration 1125/1780 Training loss: 1.5427 12.9235 sec/batch\n",
      "Epoch 7/10  Iteration 1126/1780 Training loss: 1.5422 13.3122 sec/batch\n",
      "Epoch 7/10  Iteration 1127/1780 Training loss: 1.5415 13.0409 sec/batch\n",
      "Epoch 7/10  Iteration 1128/1780 Training loss: 1.5418 12.9174 sec/batch\n",
      "Epoch 7/10  Iteration 1129/1780 Training loss: 1.5416 13.3567 sec/batch\n",
      "Epoch 7/10  Iteration 1130/1780 Training loss: 1.5425 13.7577 sec/batch\n",
      "Epoch 7/10  Iteration 1131/1780 Training loss: 1.5427 13.4422 sec/batch\n",
      "Epoch 7/10  Iteration 1132/1780 Training loss: 1.5428 13.8334 sec/batch\n",
      "Epoch 7/10  Iteration 1133/1780 Training loss: 1.5426 13.6744 sec/batch\n",
      "Epoch 7/10  Iteration 1134/1780 Training loss: 1.5426 12.2573 sec/batch\n",
      "Epoch 7/10  Iteration 1135/1780 Training loss: 1.5426 15.3917 sec/batch\n",
      "Epoch 7/10  Iteration 1136/1780 Training loss: 1.5421 15.3401 sec/batch\n",
      "Epoch 7/10  Iteration 1137/1780 Training loss: 1.5420 15.1077 sec/batch\n",
      "Epoch 7/10  Iteration 1138/1780 Training loss: 1.5417 14.9263 sec/batch\n",
      "Epoch 7/10  Iteration 1139/1780 Training loss: 1.5421 14.9710 sec/batch\n",
      "Epoch 7/10  Iteration 1140/1780 Training loss: 1.5421 14.9709 sec/batch\n",
      "Epoch 7/10  Iteration 1141/1780 Training loss: 1.5424 15.2461 sec/batch\n",
      "Epoch 7/10  Iteration 1142/1780 Training loss: 1.5420 14.9072 sec/batch\n",
      "Epoch 7/10  Iteration 1143/1780 Training loss: 1.5418 15.9482 sec/batch\n",
      "Epoch 7/10  Iteration 1144/1780 Training loss: 1.5418 14.7398 sec/batch\n",
      "Epoch 7/10  Iteration 1145/1780 Training loss: 1.5415 15.6168 sec/batch\n",
      "Epoch 7/10  Iteration 1146/1780 Training loss: 1.5415 15.7364 sec/batch\n",
      "Epoch 7/10  Iteration 1147/1780 Training loss: 1.5408 16.9995 sec/batch\n",
      "Epoch 7/10  Iteration 1148/1780 Training loss: 1.5405 15.5978 sec/batch\n",
      "Epoch 7/10  Iteration 1149/1780 Training loss: 1.5398 14.3998 sec/batch\n",
      "Epoch 7/10  Iteration 1150/1780 Training loss: 1.5397 14.9686 sec/batch\n",
      "Epoch 7/10  Iteration 1151/1780 Training loss: 1.5391 15.0051 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10  Iteration 1152/1780 Training loss: 1.5389 16.4911 sec/batch\n",
      "Epoch 7/10  Iteration 1153/1780 Training loss: 1.5385 19.6034 sec/batch\n",
      "Epoch 7/10  Iteration 1154/1780 Training loss: 1.5381 17.6582 sec/batch\n",
      "Epoch 7/10  Iteration 1155/1780 Training loss: 1.5378 18.2780 sec/batch\n",
      "Epoch 7/10  Iteration 1156/1780 Training loss: 1.5374 17.6941 sec/batch\n",
      "Epoch 7/10  Iteration 1157/1780 Training loss: 1.5368 15.3814 sec/batch\n",
      "Epoch 7/10  Iteration 1158/1780 Training loss: 1.5368 17.7352 sec/batch\n",
      "Epoch 7/10  Iteration 1159/1780 Training loss: 1.5365 16.3193 sec/batch\n",
      "Epoch 7/10  Iteration 1160/1780 Training loss: 1.5362 17.5200 sec/batch\n",
      "Epoch 7/10  Iteration 1161/1780 Training loss: 1.5357 18.4273 sec/batch\n",
      "Epoch 7/10  Iteration 1162/1780 Training loss: 1.5353 19.8708 sec/batch\n",
      "Epoch 7/10  Iteration 1163/1780 Training loss: 1.5349 20.1008 sec/batch\n",
      "Epoch 7/10  Iteration 1164/1780 Training loss: 1.5349 18.7514 sec/batch\n",
      "Epoch 7/10  Iteration 1165/1780 Training loss: 1.5348 18.0782 sec/batch\n",
      "Epoch 7/10  Iteration 1166/1780 Training loss: 1.5343 16.8786 sec/batch\n",
      "Epoch 7/10  Iteration 1167/1780 Training loss: 1.5336 18.3620 sec/batch\n",
      "Epoch 7/10  Iteration 1168/1780 Training loss: 1.5331 16.9552 sec/batch\n",
      "Epoch 7/10  Iteration 1169/1780 Training loss: 1.5330 17.6930 sec/batch\n",
      "Epoch 7/10  Iteration 1170/1780 Training loss: 1.5327 16.2240 sec/batch\n",
      "Epoch 7/10  Iteration 1171/1780 Training loss: 1.5325 14.9121 sec/batch\n",
      "Epoch 7/10  Iteration 1172/1780 Training loss: 1.5322 14.8865 sec/batch\n",
      "Epoch 7/10  Iteration 1173/1780 Training loss: 1.5319 14.9438 sec/batch\n",
      "Epoch 7/10  Iteration 1174/1780 Training loss: 1.5317 14.2250 sec/batch\n",
      "Epoch 7/10  Iteration 1175/1780 Training loss: 1.5315 13.2511 sec/batch\n",
      "Epoch 7/10  Iteration 1176/1780 Training loss: 1.5313 13.7200 sec/batch\n",
      "Epoch 7/10  Iteration 1177/1780 Training loss: 1.5312 14.1751 sec/batch\n",
      "Epoch 7/10  Iteration 1178/1780 Training loss: 1.5311 13.6259 sec/batch\n",
      "Epoch 7/10  Iteration 1179/1780 Training loss: 1.5307 14.2993 sec/batch\n",
      "Epoch 7/10  Iteration 1180/1780 Training loss: 1.5305 15.5723 sec/batch\n",
      "Epoch 7/10  Iteration 1181/1780 Training loss: 1.5303 16.3256 sec/batch\n",
      "Epoch 7/10  Iteration 1182/1780 Training loss: 1.5300 15.6681 sec/batch\n",
      "Epoch 7/10  Iteration 1183/1780 Training loss: 1.5296 15.8101 sec/batch\n",
      "Epoch 7/10  Iteration 1184/1780 Training loss: 1.5291 14.6723 sec/batch\n",
      "Epoch 7/10  Iteration 1185/1780 Training loss: 1.5289 12.6257 sec/batch\n",
      "Epoch 7/10  Iteration 1186/1780 Training loss: 1.5287 11.8338 sec/batch\n",
      "Epoch 7/10  Iteration 1187/1780 Training loss: 1.5285 14.0293 sec/batch\n",
      "Epoch 7/10  Iteration 1188/1780 Training loss: 1.5283 13.6414 sec/batch\n",
      "Epoch 7/10  Iteration 1189/1780 Training loss: 1.5280 13.6399 sec/batch\n",
      "Epoch 7/10  Iteration 1190/1780 Training loss: 1.5275 14.5109 sec/batch\n",
      "Epoch 7/10  Iteration 1191/1780 Training loss: 1.5271 13.4673 sec/batch\n",
      "Epoch 7/10  Iteration 1192/1780 Training loss: 1.5270 12.7573 sec/batch\n",
      "Epoch 7/10  Iteration 1193/1780 Training loss: 1.5268 11.2428 sec/batch\n",
      "Epoch 7/10  Iteration 1194/1780 Training loss: 1.5263 12.3220 sec/batch\n",
      "Epoch 7/10  Iteration 1195/1780 Training loss: 1.5263 14.8533 sec/batch\n",
      "Epoch 7/10  Iteration 1196/1780 Training loss: 1.5262 13.7412 sec/batch\n",
      "Epoch 7/10  Iteration 1197/1780 Training loss: 1.5259 11.3625 sec/batch\n",
      "Epoch 7/10  Iteration 1198/1780 Training loss: 1.5255 11.2221 sec/batch\n",
      "Epoch 7/10  Iteration 1199/1780 Training loss: 1.5250 11.3442 sec/batch\n",
      "Epoch 7/10  Iteration 1200/1780 Training loss: 1.5246 12.2621 sec/batch\n",
      "Validation loss: 1.38859 Saving checkpoint!\n",
      "Epoch 7/10  Iteration 1201/1780 Training loss: 1.5252 12.2767 sec/batch\n",
      "Epoch 7/10  Iteration 1202/1780 Training loss: 1.5251 13.2834 sec/batch\n",
      "Epoch 7/10  Iteration 1203/1780 Training loss: 1.5249 12.1435 sec/batch\n",
      "Epoch 7/10  Iteration 1204/1780 Training loss: 1.5248 12.4146 sec/batch\n",
      "Epoch 7/10  Iteration 1205/1780 Training loss: 1.5249 13.4897 sec/batch\n",
      "Epoch 7/10  Iteration 1206/1780 Training loss: 1.5248 12.1041 sec/batch\n",
      "Epoch 7/10  Iteration 1207/1780 Training loss: 1.5247 14.5327 sec/batch\n",
      "Epoch 7/10  Iteration 1208/1780 Training loss: 1.5245 14.0541 sec/batch\n",
      "Epoch 7/10  Iteration 1209/1780 Training loss: 1.5248 12.6942 sec/batch\n",
      "Epoch 7/10  Iteration 1210/1780 Training loss: 1.5246 12.7475 sec/batch\n",
      "Epoch 7/10  Iteration 1211/1780 Training loss: 1.5245 13.4855 sec/batch\n",
      "Epoch 7/10  Iteration 1212/1780 Training loss: 1.5245 12.6935 sec/batch\n",
      "Epoch 7/10  Iteration 1213/1780 Training loss: 1.5243 12.0395 sec/batch\n",
      "Epoch 7/10  Iteration 1214/1780 Training loss: 1.5243 12.1354 sec/batch\n",
      "Epoch 7/10  Iteration 1215/1780 Training loss: 1.5242 11.7805 sec/batch\n",
      "Epoch 7/10  Iteration 1216/1780 Training loss: 1.5243 11.7597 sec/batch\n",
      "Epoch 7/10  Iteration 1217/1780 Training loss: 1.5242 11.9737 sec/batch\n",
      "Epoch 7/10  Iteration 1218/1780 Training loss: 1.5240 11.7147 sec/batch\n",
      "Epoch 7/10  Iteration 1219/1780 Training loss: 1.5235 11.8147 sec/batch\n",
      "Epoch 7/10  Iteration 1220/1780 Training loss: 1.5234 11.9388 sec/batch\n",
      "Epoch 7/10  Iteration 1221/1780 Training loss: 1.5233 11.7727 sec/batch\n",
      "Epoch 7/10  Iteration 1222/1780 Training loss: 1.5232 12.0115 sec/batch\n",
      "Epoch 7/10  Iteration 1223/1780 Training loss: 1.5232 11.9334 sec/batch\n",
      "Epoch 7/10  Iteration 1224/1780 Training loss: 1.5230 11.5967 sec/batch\n",
      "Epoch 7/10  Iteration 1225/1780 Training loss: 1.5230 11.8069 sec/batch\n",
      "Epoch 7/10  Iteration 1226/1780 Training loss: 1.5228 11.7087 sec/batch\n",
      "Epoch 7/10  Iteration 1227/1780 Training loss: 1.5224 11.8462 sec/batch\n",
      "Epoch 7/10  Iteration 1228/1780 Training loss: 1.5224 11.7846 sec/batch\n",
      "Epoch 7/10  Iteration 1229/1780 Training loss: 1.5225 11.6019 sec/batch\n",
      "Epoch 7/10  Iteration 1230/1780 Training loss: 1.5223 11.9466 sec/batch\n",
      "Epoch 7/10  Iteration 1231/1780 Training loss: 1.5222 11.7062 sec/batch\n",
      "Epoch 7/10  Iteration 1232/1780 Training loss: 1.5221 11.9269 sec/batch\n",
      "Epoch 7/10  Iteration 1233/1780 Training loss: 1.5220 11.9120 sec/batch\n",
      "Epoch 7/10  Iteration 1234/1780 Training loss: 1.5218 15.0909 sec/batch\n",
      "Epoch 7/10  Iteration 1235/1780 Training loss: 1.5219 12.7203 sec/batch\n",
      "Epoch 7/10  Iteration 1236/1780 Training loss: 1.5222 12.1907 sec/batch\n",
      "Epoch 7/10  Iteration 1237/1780 Training loss: 1.5220 12.0619 sec/batch\n",
      "Epoch 7/10  Iteration 1238/1780 Training loss: 1.5219 12.0737 sec/batch\n",
      "Epoch 7/10  Iteration 1239/1780 Training loss: 1.5217 12.5907 sec/batch\n",
      "Epoch 7/10  Iteration 1240/1780 Training loss: 1.5215 12.6023 sec/batch\n",
      "Epoch 7/10  Iteration 1241/1780 Training loss: 1.5215 12.4233 sec/batch\n",
      "Epoch 7/10  Iteration 1242/1780 Training loss: 1.5214 12.1144 sec/batch\n",
      "Epoch 7/10  Iteration 1243/1780 Training loss: 1.5213 11.9013 sec/batch\n",
      "Epoch 7/10  Iteration 1244/1780 Training loss: 1.5212 12.0627 sec/batch\n",
      "Epoch 7/10  Iteration 1245/1780 Training loss: 1.5209 11.5887 sec/batch\n",
      "Epoch 7/10  Iteration 1246/1780 Training loss: 1.5209 11.4036 sec/batch\n",
      "Epoch 8/10  Iteration 1247/1780 Training loss: 1.5960 12.2475 sec/batch\n",
      "Epoch 8/10  Iteration 1248/1780 Training loss: 1.5493 11.9640 sec/batch\n",
      "Epoch 8/10  Iteration 1249/1780 Training loss: 1.5309 11.5884 sec/batch\n",
      "Epoch 8/10  Iteration 1250/1780 Training loss: 1.5233 11.9745 sec/batch\n",
      "Epoch 8/10  Iteration 1251/1780 Training loss: 1.5156 11.9846 sec/batch\n",
      "Epoch 8/10  Iteration 1252/1780 Training loss: 1.5030 11.4235 sec/batch\n",
      "Epoch 8/10  Iteration 1253/1780 Training loss: 1.5035 12.5409 sec/batch\n",
      "Epoch 8/10  Iteration 1254/1780 Training loss: 1.5012 13.2481 sec/batch\n",
      "Epoch 8/10  Iteration 1255/1780 Training loss: 1.5011 12.9479 sec/batch\n",
      "Epoch 8/10  Iteration 1256/1780 Training loss: 1.4995 14.7064 sec/batch\n",
      "Epoch 8/10  Iteration 1257/1780 Training loss: 1.4951 12.4326 sec/batch\n",
      "Epoch 8/10  Iteration 1258/1780 Training loss: 1.4935 13.8750 sec/batch\n",
      "Epoch 8/10  Iteration 1259/1780 Training loss: 1.4933 13.7942 sec/batch\n",
      "Epoch 8/10  Iteration 1260/1780 Training loss: 1.4945 12.3840 sec/batch\n",
      "Epoch 8/10  Iteration 1261/1780 Training loss: 1.4940 12.4409 sec/batch\n",
      "Epoch 8/10  Iteration 1262/1780 Training loss: 1.4926 12.0909 sec/batch\n",
      "Epoch 8/10  Iteration 1263/1780 Training loss: 1.4923 11.9963 sec/batch\n",
      "Epoch 8/10  Iteration 1264/1780 Training loss: 1.4933 12.5256 sec/batch\n",
      "Epoch 8/10  Iteration 1265/1780 Training loss: 1.4933 11.7530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10  Iteration 1266/1780 Training loss: 1.4943 11.4174 sec/batch\n",
      "Epoch 8/10  Iteration 1267/1780 Training loss: 1.4938 11.6123 sec/batch\n",
      "Epoch 8/10  Iteration 1268/1780 Training loss: 1.4944 11.5812 sec/batch\n",
      "Epoch 8/10  Iteration 1269/1780 Training loss: 1.4930 12.7674 sec/batch\n",
      "Epoch 8/10  Iteration 1270/1780 Training loss: 1.4927 18.0746 sec/batch\n",
      "Epoch 8/10  Iteration 1271/1780 Training loss: 1.4925 13.4075 sec/batch\n",
      "Epoch 8/10  Iteration 1272/1780 Training loss: 1.4905 12.6254 sec/batch\n",
      "Epoch 8/10  Iteration 1273/1780 Training loss: 1.4889 12.1290 sec/batch\n",
      "Epoch 8/10  Iteration 1274/1780 Training loss: 1.4890 13.3938 sec/batch\n",
      "Epoch 8/10  Iteration 1275/1780 Training loss: 1.4891 14.5568 sec/batch\n",
      "Epoch 8/10  Iteration 1276/1780 Training loss: 1.4891 16.6197 sec/batch\n",
      "Epoch 8/10  Iteration 1277/1780 Training loss: 1.4887 16.1951 sec/batch\n",
      "Epoch 8/10  Iteration 1278/1780 Training loss: 1.4874 12.8316 sec/batch\n",
      "Epoch 8/10  Iteration 1279/1780 Training loss: 1.4876 12.2721 sec/batch\n",
      "Epoch 8/10  Iteration 1280/1780 Training loss: 1.4879 11.7814 sec/batch\n",
      "Epoch 8/10  Iteration 1281/1780 Training loss: 1.4875 11.8171 sec/batch\n",
      "Epoch 8/10  Iteration 1282/1780 Training loss: 1.4874 12.0945 sec/batch\n",
      "Epoch 8/10  Iteration 1283/1780 Training loss: 1.4866 12.5082 sec/batch\n",
      "Epoch 8/10  Iteration 1284/1780 Training loss: 1.4854 11.9875 sec/batch\n",
      "Epoch 8/10  Iteration 1285/1780 Training loss: 1.4841 11.9591 sec/batch\n",
      "Epoch 8/10  Iteration 1286/1780 Training loss: 1.4838 12.0524 sec/batch\n",
      "Epoch 8/10  Iteration 1287/1780 Training loss: 1.4836 11.8854 sec/batch\n",
      "Epoch 8/10  Iteration 1288/1780 Training loss: 1.4841 12.4704 sec/batch\n",
      "Epoch 8/10  Iteration 1289/1780 Training loss: 1.4835 12.2844 sec/batch\n",
      "Epoch 8/10  Iteration 1290/1780 Training loss: 1.4829 11.8881 sec/batch\n",
      "Epoch 8/10  Iteration 1291/1780 Training loss: 1.4831 11.9366 sec/batch\n",
      "Epoch 8/10  Iteration 1292/1780 Training loss: 1.4820 11.9406 sec/batch\n",
      "Epoch 8/10  Iteration 1293/1780 Training loss: 1.4816 12.1394 sec/batch\n",
      "Epoch 8/10  Iteration 1294/1780 Training loss: 1.4812 11.9763 sec/batch\n",
      "Epoch 8/10  Iteration 1295/1780 Training loss: 1.4812 12.0336 sec/batch\n",
      "Epoch 8/10  Iteration 1296/1780 Training loss: 1.4816 11.9089 sec/batch\n",
      "Epoch 8/10  Iteration 1297/1780 Training loss: 1.4808 11.7801 sec/batch\n",
      "Epoch 8/10  Iteration 1298/1780 Training loss: 1.4817 12.5594 sec/batch\n",
      "Epoch 8/10  Iteration 1299/1780 Training loss: 1.4814 12.2140 sec/batch\n",
      "Epoch 8/10  Iteration 1300/1780 Training loss: 1.4814 11.8928 sec/batch\n",
      "Validation loss: 1.35922 Saving checkpoint!\n",
      "Epoch 8/10  Iteration 1301/1780 Training loss: 1.4827 11.9869 sec/batch\n",
      "Epoch 8/10  Iteration 1302/1780 Training loss: 1.4827 11.8804 sec/batch\n",
      "Epoch 8/10  Iteration 1303/1780 Training loss: 1.4831 12.5407 sec/batch\n",
      "Epoch 8/10  Iteration 1304/1780 Training loss: 1.4825 12.0120 sec/batch\n",
      "Epoch 8/10  Iteration 1305/1780 Training loss: 1.4819 11.9414 sec/batch\n",
      "Epoch 8/10  Iteration 1306/1780 Training loss: 1.4824 12.0422 sec/batch\n",
      "Epoch 8/10  Iteration 1307/1780 Training loss: 1.4822 11.9793 sec/batch\n",
      "Epoch 8/10  Iteration 1308/1780 Training loss: 1.4829 12.1024 sec/batch\n",
      "Epoch 8/10  Iteration 1309/1780 Training loss: 1.4832 12.2093 sec/batch\n",
      "Epoch 8/10  Iteration 1310/1780 Training loss: 1.4833 12.0378 sec/batch\n",
      "Epoch 8/10  Iteration 1311/1780 Training loss: 1.4830 11.8494 sec/batch\n",
      "Epoch 8/10  Iteration 1312/1780 Training loss: 1.4830 12.0964 sec/batch\n",
      "Epoch 8/10  Iteration 1313/1780 Training loss: 1.4830 12.4349 sec/batch\n",
      "Epoch 8/10  Iteration 1314/1780 Training loss: 1.4826 12.3933 sec/batch\n",
      "Epoch 8/10  Iteration 1315/1780 Training loss: 1.4826 11.9158 sec/batch\n",
      "Epoch 8/10  Iteration 1316/1780 Training loss: 1.4824 12.0148 sec/batch\n",
      "Epoch 8/10  Iteration 1317/1780 Training loss: 1.4830 11.9307 sec/batch\n",
      "Epoch 8/10  Iteration 1318/1780 Training loss: 1.4830 12.2148 sec/batch\n",
      "Epoch 8/10  Iteration 1319/1780 Training loss: 1.4834 12.6723 sec/batch\n",
      "Epoch 8/10  Iteration 1320/1780 Training loss: 1.4830 11.9566 sec/batch\n",
      "Epoch 8/10  Iteration 1321/1780 Training loss: 1.4828 12.3016 sec/batch\n",
      "Epoch 8/10  Iteration 1322/1780 Training loss: 1.4829 12.3177 sec/batch\n",
      "Epoch 8/10  Iteration 1323/1780 Training loss: 1.4827 11.9296 sec/batch\n",
      "Epoch 8/10  Iteration 1324/1780 Training loss: 1.4826 12.2559 sec/batch\n",
      "Epoch 8/10  Iteration 1325/1780 Training loss: 1.4819 12.1356 sec/batch\n",
      "Epoch 8/10  Iteration 1326/1780 Training loss: 1.4815 11.9876 sec/batch\n",
      "Epoch 8/10  Iteration 1327/1780 Training loss: 1.4809 11.8054 sec/batch\n",
      "Epoch 8/10  Iteration 1328/1780 Training loss: 1.4808 11.8739 sec/batch\n",
      "Epoch 8/10  Iteration 1329/1780 Training loss: 1.4801 12.6251 sec/batch\n",
      "Epoch 8/10  Iteration 1330/1780 Training loss: 1.4798 12.0517 sec/batch\n",
      "Epoch 8/10  Iteration 1331/1780 Training loss: 1.4794 12.0498 sec/batch\n",
      "Epoch 8/10  Iteration 1332/1780 Training loss: 1.4791 12.0186 sec/batch\n",
      "Epoch 8/10  Iteration 1333/1780 Training loss: 1.4787 11.9231 sec/batch\n",
      "Epoch 8/10  Iteration 1334/1780 Training loss: 1.4783 12.5373 sec/batch\n",
      "Epoch 8/10  Iteration 1335/1780 Training loss: 1.4778 11.8987 sec/batch\n",
      "Epoch 8/10  Iteration 1336/1780 Training loss: 1.4778 11.8321 sec/batch\n",
      "Epoch 8/10  Iteration 1337/1780 Training loss: 1.4774 11.8888 sec/batch\n",
      "Epoch 8/10  Iteration 1338/1780 Training loss: 1.4771 11.7881 sec/batch\n",
      "Epoch 8/10  Iteration 1339/1780 Training loss: 1.4766 12.0941 sec/batch\n",
      "Epoch 8/10  Iteration 1340/1780 Training loss: 1.4763 12.0569 sec/batch\n",
      "Epoch 8/10  Iteration 1341/1780 Training loss: 1.4759 11.8578 sec/batch\n",
      "Epoch 8/10  Iteration 1342/1780 Training loss: 1.4759 11.9709 sec/batch\n",
      "Epoch 8/10  Iteration 1343/1780 Training loss: 1.4757 12.0151 sec/batch\n",
      "Epoch 8/10  Iteration 1344/1780 Training loss: 1.4753 12.4363 sec/batch\n",
      "Epoch 8/10  Iteration 1345/1780 Training loss: 1.4748 12.1447 sec/batch\n",
      "Epoch 8/10  Iteration 1346/1780 Training loss: 1.4743 11.8692 sec/batch\n",
      "Epoch 8/10  Iteration 1347/1780 Training loss: 1.4741 12.0080 sec/batch\n",
      "Epoch 8/10  Iteration 1348/1780 Training loss: 1.4739 12.0059 sec/batch\n",
      "Epoch 8/10  Iteration 1349/1780 Training loss: 1.4736 11.9691 sec/batch\n",
      "Epoch 8/10  Iteration 1350/1780 Training loss: 1.4733 12.4069 sec/batch\n",
      "Epoch 8/10  Iteration 1351/1780 Training loss: 1.4731 12.1102 sec/batch\n",
      "Epoch 8/10  Iteration 1352/1780 Training loss: 1.4729 11.8370 sec/batch\n",
      "Epoch 8/10  Iteration 1353/1780 Training loss: 1.4727 11.8393 sec/batch\n",
      "Epoch 8/10  Iteration 1354/1780 Training loss: 1.4725 11.9096 sec/batch\n",
      "Epoch 8/10  Iteration 1355/1780 Training loss: 1.4723 11.9972 sec/batch\n",
      "Epoch 8/10  Iteration 1356/1780 Training loss: 1.4723 11.9758 sec/batch\n",
      "Epoch 8/10  Iteration 1357/1780 Training loss: 1.4721 11.8726 sec/batch\n",
      "Epoch 8/10  Iteration 1358/1780 Training loss: 1.4718 11.9463 sec/batch\n",
      "Epoch 8/10  Iteration 1359/1780 Training loss: 1.4716 11.7751 sec/batch\n",
      "Epoch 8/10  Iteration 1360/1780 Training loss: 1.4713 12.6976 sec/batch\n",
      "Epoch 8/10  Iteration 1361/1780 Training loss: 1.4709 12.1163 sec/batch\n",
      "Epoch 8/10  Iteration 1362/1780 Training loss: 1.4704 11.8837 sec/batch\n",
      "Epoch 8/10  Iteration 1363/1780 Training loss: 1.4703 11.8970 sec/batch\n",
      "Epoch 8/10  Iteration 1364/1780 Training loss: 1.4702 11.9749 sec/batch\n",
      "Epoch 8/10  Iteration 1365/1780 Training loss: 1.4699 12.6986 sec/batch\n",
      "Epoch 8/10  Iteration 1366/1780 Training loss: 1.4697 11.9923 sec/batch\n",
      "Epoch 8/10  Iteration 1367/1780 Training loss: 1.4695 11.8077 sec/batch\n",
      "Epoch 8/10  Iteration 1368/1780 Training loss: 1.4690 11.8331 sec/batch\n",
      "Epoch 8/10  Iteration 1369/1780 Training loss: 1.4685 12.0299 sec/batch\n",
      "Epoch 8/10  Iteration 1370/1780 Training loss: 1.4685 12.0713 sec/batch\n",
      "Epoch 8/10  Iteration 1371/1780 Training loss: 1.4684 11.9918 sec/batch\n",
      "Epoch 8/10  Iteration 1372/1780 Training loss: 1.4679 11.9799 sec/batch\n",
      "Epoch 8/10  Iteration 1373/1780 Training loss: 1.4679 12.1584 sec/batch\n",
      "Epoch 8/10  Iteration 1374/1780 Training loss: 1.4678 11.8344 sec/batch\n",
      "Epoch 8/10  Iteration 1375/1780 Training loss: 1.4676 12.4768 sec/batch\n",
      "Epoch 8/10  Iteration 1376/1780 Training loss: 1.4673 12.2073 sec/batch\n",
      "Epoch 8/10  Iteration 1377/1780 Training loss: 1.4668 12.1087 sec/batch\n",
      "Epoch 8/10  Iteration 1378/1780 Training loss: 1.4666 12.0313 sec/batch\n",
      "Epoch 8/10  Iteration 1379/1780 Training loss: 1.4666 11.9276 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10  Iteration 1380/1780 Training loss: 1.4665 11.9996 sec/batch\n",
      "Epoch 8/10  Iteration 1381/1780 Training loss: 1.4664 12.7044 sec/batch\n",
      "Epoch 8/10  Iteration 1382/1780 Training loss: 1.4664 11.9522 sec/batch\n",
      "Epoch 8/10  Iteration 1383/1780 Training loss: 1.4664 12.0232 sec/batch\n",
      "Epoch 8/10  Iteration 1384/1780 Training loss: 1.4664 12.1048 sec/batch\n",
      "Epoch 8/10  Iteration 1385/1780 Training loss: 1.4664 11.9824 sec/batch\n",
      "Epoch 8/10  Iteration 1386/1780 Training loss: 1.4662 12.1910 sec/batch\n",
      "Epoch 8/10  Iteration 1387/1780 Training loss: 1.4665 11.9942 sec/batch\n",
      "Epoch 8/10  Iteration 1388/1780 Training loss: 1.4663 11.9496 sec/batch\n",
      "Epoch 8/10  Iteration 1389/1780 Training loss: 1.4662 12.1403 sec/batch\n",
      "Epoch 8/10  Iteration 1390/1780 Training loss: 1.4663 11.9541 sec/batch\n",
      "Epoch 8/10  Iteration 1391/1780 Training loss: 1.4660 12.7419 sec/batch\n",
      "Epoch 8/10  Iteration 1392/1780 Training loss: 1.4660 12.1174 sec/batch\n",
      "Epoch 8/10  Iteration 1393/1780 Training loss: 1.4660 11.8978 sec/batch\n",
      "Epoch 8/10  Iteration 1394/1780 Training loss: 1.4662 11.9789 sec/batch\n",
      "Epoch 8/10  Iteration 1395/1780 Training loss: 1.4662 12.0535 sec/batch\n",
      "Epoch 8/10  Iteration 1396/1780 Training loss: 1.4660 12.5855 sec/batch\n",
      "Epoch 8/10  Iteration 1397/1780 Training loss: 1.4656 12.1115 sec/batch\n",
      "Epoch 8/10  Iteration 1398/1780 Training loss: 1.4653 11.9416 sec/batch\n",
      "Epoch 8/10  Iteration 1399/1780 Training loss: 1.4653 11.9971 sec/batch\n",
      "Epoch 8/10  Iteration 1400/1780 Training loss: 1.4652 12.0922 sec/batch\n",
      "Validation loss: 1.33267 Saving checkpoint!\n",
      "Epoch 8/10  Iteration 1401/1780 Training loss: 1.4656 11.7698 sec/batch\n",
      "Epoch 8/10  Iteration 1402/1780 Training loss: 1.4656 11.7799 sec/batch\n",
      "Epoch 8/10  Iteration 1403/1780 Training loss: 1.4655 11.6782 sec/batch\n",
      "Epoch 8/10  Iteration 1404/1780 Training loss: 1.4653 12.0052 sec/batch\n",
      "Epoch 8/10  Iteration 1405/1780 Training loss: 1.4650 12.1776 sec/batch\n",
      "Epoch 8/10  Iteration 1406/1780 Training loss: 1.4650 11.6735 sec/batch\n",
      "Epoch 8/10  Iteration 1407/1780 Training loss: 1.4651 11.6146 sec/batch\n",
      "Epoch 8/10  Iteration 1408/1780 Training loss: 1.4650 11.7310 sec/batch\n",
      "Epoch 8/10  Iteration 1409/1780 Training loss: 1.4649 11.6568 sec/batch\n",
      "Epoch 8/10  Iteration 1410/1780 Training loss: 1.4648 11.8312 sec/batch\n",
      "Epoch 8/10  Iteration 1411/1780 Training loss: 1.4647 11.6854 sec/batch\n",
      "Epoch 8/10  Iteration 1412/1780 Training loss: 1.4645 11.7680 sec/batch\n",
      "Epoch 8/10  Iteration 1413/1780 Training loss: 1.4646 11.6506 sec/batch\n",
      "Epoch 8/10  Iteration 1414/1780 Training loss: 1.4649 11.7481 sec/batch\n",
      "Epoch 8/10  Iteration 1415/1780 Training loss: 1.4648 12.2556 sec/batch\n",
      "Epoch 8/10  Iteration 1416/1780 Training loss: 1.4647 12.0671 sec/batch\n",
      "Epoch 8/10  Iteration 1417/1780 Training loss: 1.4645 12.6311 sec/batch\n",
      "Epoch 8/10  Iteration 1418/1780 Training loss: 1.4642 11.5655 sec/batch\n",
      "Epoch 8/10  Iteration 1419/1780 Training loss: 1.4642 11.5536 sec/batch\n",
      "Epoch 8/10  Iteration 1420/1780 Training loss: 1.4641 12.3804 sec/batch\n",
      "Epoch 8/10  Iteration 1421/1780 Training loss: 1.4641 11.9901 sec/batch\n",
      "Epoch 8/10  Iteration 1422/1780 Training loss: 1.4639 11.6715 sec/batch\n",
      "Epoch 8/10  Iteration 1423/1780 Training loss: 1.4636 11.6376 sec/batch\n",
      "Epoch 8/10  Iteration 1424/1780 Training loss: 1.4636 11.6967 sec/batch\n",
      "Epoch 9/10  Iteration 1425/1780 Training loss: 1.5368 11.6925 sec/batch\n",
      "Epoch 9/10  Iteration 1426/1780 Training loss: 1.4923 12.1399 sec/batch\n",
      "Epoch 9/10  Iteration 1427/1780 Training loss: 1.4770 11.6602 sec/batch\n",
      "Epoch 9/10  Iteration 1428/1780 Training loss: 1.4698 11.7962 sec/batch\n",
      "Epoch 9/10  Iteration 1429/1780 Training loss: 1.4615 11.5340 sec/batch\n",
      "Epoch 9/10  Iteration 1430/1780 Training loss: 1.4516 11.8043 sec/batch\n",
      "Epoch 9/10  Iteration 1431/1780 Training loss: 1.4523 11.7164 sec/batch\n",
      "Epoch 9/10  Iteration 1432/1780 Training loss: 1.4472 11.8269 sec/batch\n",
      "Epoch 9/10  Iteration 1433/1780 Training loss: 1.4467 11.5792 sec/batch\n",
      "Epoch 9/10  Iteration 1434/1780 Training loss: 1.4453 11.5972 sec/batch\n",
      "Epoch 9/10  Iteration 1435/1780 Training loss: 1.4418 11.7345 sec/batch\n",
      "Epoch 9/10  Iteration 1436/1780 Training loss: 1.4405 11.9801 sec/batch\n",
      "Epoch 9/10  Iteration 1437/1780 Training loss: 1.4409 12.1098 sec/batch\n",
      "Epoch 9/10  Iteration 1438/1780 Training loss: 1.4422 11.6343 sec/batch\n",
      "Epoch 9/10  Iteration 1439/1780 Training loss: 1.4416 11.6029 sec/batch\n",
      "Epoch 9/10  Iteration 1440/1780 Training loss: 1.4393 11.5831 sec/batch\n",
      "Epoch 9/10  Iteration 1441/1780 Training loss: 1.4400 11.6814 sec/batch\n",
      "Epoch 9/10  Iteration 1442/1780 Training loss: 1.4412 12.4233 sec/batch\n",
      "Epoch 9/10  Iteration 1443/1780 Training loss: 1.4410 11.7517 sec/batch\n",
      "Epoch 9/10  Iteration 1444/1780 Training loss: 1.4419 11.5136 sec/batch\n",
      "Epoch 9/10  Iteration 1445/1780 Training loss: 1.4409 11.5714 sec/batch\n",
      "Epoch 9/10  Iteration 1446/1780 Training loss: 1.4413 11.6036 sec/batch\n",
      "Epoch 9/10  Iteration 1447/1780 Training loss: 1.4400 11.7594 sec/batch\n",
      "Epoch 9/10  Iteration 1448/1780 Training loss: 1.4399 11.7499 sec/batch\n",
      "Epoch 9/10  Iteration 1449/1780 Training loss: 1.4397 11.5448 sec/batch\n",
      "Epoch 9/10  Iteration 1450/1780 Training loss: 1.4379 11.4973 sec/batch\n",
      "Epoch 9/10  Iteration 1451/1780 Training loss: 1.4364 11.6263 sec/batch\n",
      "Epoch 9/10  Iteration 1452/1780 Training loss: 1.4367 11.6136 sec/batch\n",
      "Epoch 9/10  Iteration 1453/1780 Training loss: 1.4369 12.5120 sec/batch\n",
      "Epoch 9/10  Iteration 1454/1780 Training loss: 1.4370 11.6642 sec/batch\n",
      "Epoch 9/10  Iteration 1455/1780 Training loss: 1.4368 11.5553 sec/batch\n",
      "Epoch 9/10  Iteration 1456/1780 Training loss: 1.4359 11.5251 sec/batch\n",
      "Epoch 9/10  Iteration 1457/1780 Training loss: 1.4357 11.5985 sec/batch\n",
      "Epoch 9/10  Iteration 1458/1780 Training loss: 1.4358 11.8907 sec/batch\n",
      "Epoch 9/10  Iteration 1459/1780 Training loss: 1.4356 12.0242 sec/batch\n",
      "Epoch 9/10  Iteration 1460/1780 Training loss: 1.4355 11.6002 sec/batch\n",
      "Epoch 9/10  Iteration 1461/1780 Training loss: 1.4346 11.5347 sec/batch\n",
      "Epoch 9/10  Iteration 1462/1780 Training loss: 1.4332 11.5357 sec/batch\n",
      "Epoch 9/10  Iteration 1463/1780 Training loss: 1.4317 11.5336 sec/batch\n",
      "Epoch 9/10  Iteration 1464/1780 Training loss: 1.4311 11.7865 sec/batch\n",
      "Epoch 9/10  Iteration 1465/1780 Training loss: 1.4306 11.9819 sec/batch\n",
      "Epoch 9/10  Iteration 1466/1780 Training loss: 1.4309 11.5372 sec/batch\n",
      "Epoch 9/10  Iteration 1467/1780 Training loss: 1.4305 11.5115 sec/batch\n",
      "Epoch 9/10  Iteration 1468/1780 Training loss: 1.4296 11.6092 sec/batch\n",
      "Epoch 9/10  Iteration 1469/1780 Training loss: 1.4298 12.3709 sec/batch\n",
      "Epoch 9/10  Iteration 1470/1780 Training loss: 1.4288 11.8741 sec/batch\n",
      "Epoch 9/10  Iteration 1471/1780 Training loss: 1.4283 11.7059 sec/batch\n",
      "Epoch 9/10  Iteration 1472/1780 Training loss: 1.4280 11.5467 sec/batch\n",
      "Epoch 9/10  Iteration 1473/1780 Training loss: 1.4276 11.6549 sec/batch\n",
      "Epoch 9/10  Iteration 1474/1780 Training loss: 1.4279 11.8087 sec/batch\n",
      "Epoch 9/10  Iteration 1475/1780 Training loss: 1.4273 12.3467 sec/batch\n",
      "Epoch 9/10  Iteration 1476/1780 Training loss: 1.4280 11.8947 sec/batch\n",
      "Epoch 9/10  Iteration 1477/1780 Training loss: 1.4278 11.6437 sec/batch\n",
      "Epoch 9/10  Iteration 1478/1780 Training loss: 1.4279 11.6702 sec/batch\n",
      "Epoch 9/10  Iteration 1479/1780 Training loss: 1.4275 11.7221 sec/batch\n",
      "Epoch 9/10  Iteration 1480/1780 Training loss: 1.4275 11.8374 sec/batch\n",
      "Epoch 9/10  Iteration 1481/1780 Training loss: 1.4279 11.7689 sec/batch\n",
      "Epoch 9/10  Iteration 1482/1780 Training loss: 1.4275 11.6108 sec/batch\n",
      "Epoch 9/10  Iteration 1483/1780 Training loss: 1.4268 11.8053 sec/batch\n",
      "Epoch 9/10  Iteration 1484/1780 Training loss: 1.4272 11.5745 sec/batch\n",
      "Epoch 9/10  Iteration 1485/1780 Training loss: 1.4270 12.3547 sec/batch\n",
      "Epoch 9/10  Iteration 1486/1780 Training loss: 1.4278 12.1133 sec/batch\n",
      "Epoch 9/10  Iteration 1487/1780 Training loss: 1.4280 11.6001 sec/batch\n",
      "Epoch 9/10  Iteration 1488/1780 Training loss: 1.4280 11.5072 sec/batch\n",
      "Epoch 9/10  Iteration 1489/1780 Training loss: 1.4277 11.5681 sec/batch\n",
      "Epoch 9/10  Iteration 1490/1780 Training loss: 1.4278 11.5614 sec/batch\n",
      "Epoch 9/10  Iteration 1491/1780 Training loss: 1.4279 12.0799 sec/batch\n",
      "Epoch 9/10  Iteration 1492/1780 Training loss: 1.4275 11.5775 sec/batch\n",
      "Epoch 9/10  Iteration 1493/1780 Training loss: 1.4274 11.6312 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10  Iteration 1494/1780 Training loss: 1.4271 11.5230 sec/batch\n",
      "Epoch 9/10  Iteration 1495/1780 Training loss: 1.4277 11.4836 sec/batch\n",
      "Epoch 9/10  Iteration 1496/1780 Training loss: 1.4279 11.7889 sec/batch\n",
      "Epoch 9/10  Iteration 1497/1780 Training loss: 1.4284 11.4643 sec/batch\n",
      "Epoch 9/10  Iteration 1498/1780 Training loss: 1.4279 11.5346 sec/batch\n",
      "Epoch 9/10  Iteration 1499/1780 Training loss: 1.4279 11.5558 sec/batch\n",
      "Epoch 9/10  Iteration 1500/1780 Training loss: 1.4279 11.4174 sec/batch\n",
      "Validation loss: 1.31137 Saving checkpoint!\n",
      "Epoch 9/10  Iteration 1501/1780 Training loss: 1.4290 10.9291 sec/batch\n",
      "Epoch 9/10  Iteration 1502/1780 Training loss: 1.4289 10.8391 sec/batch\n",
      "Epoch 9/10  Iteration 1503/1780 Training loss: 1.4283 11.7488 sec/batch\n",
      "Epoch 9/10  Iteration 1504/1780 Training loss: 1.4281 12.1697 sec/batch\n",
      "Epoch 9/10  Iteration 1505/1780 Training loss: 1.4276 11.9085 sec/batch\n",
      "Epoch 9/10  Iteration 1506/1780 Training loss: 1.4275 11.6099 sec/batch\n",
      "Epoch 9/10  Iteration 1507/1780 Training loss: 1.4269 11.0457 sec/batch\n",
      "Epoch 9/10  Iteration 1508/1780 Training loss: 1.4269 10.9509 sec/batch\n",
      "Epoch 9/10  Iteration 1509/1780 Training loss: 1.4264 11.0322 sec/batch\n",
      "Epoch 9/10  Iteration 1510/1780 Training loss: 1.4263 11.3304 sec/batch\n",
      "Epoch 9/10  Iteration 1511/1780 Training loss: 1.4260 11.1438 sec/batch\n",
      "Epoch 9/10  Iteration 1512/1780 Training loss: 1.4255 13.4491 sec/batch\n",
      "Epoch 9/10  Iteration 1513/1780 Training loss: 1.4250 13.3596 sec/batch\n",
      "Epoch 9/10  Iteration 1514/1780 Training loss: 1.4250 13.0378 sec/batch\n",
      "Epoch 9/10  Iteration 1515/1780 Training loss: 1.4246 13.0706 sec/batch\n",
      "Epoch 9/10  Iteration 1516/1780 Training loss: 1.4244 13.1165 sec/batch\n",
      "Epoch 9/10  Iteration 1517/1780 Training loss: 1.4239 13.6922 sec/batch\n",
      "Epoch 9/10  Iteration 1518/1780 Training loss: 1.4236 13.4506 sec/batch\n",
      "Epoch 9/10  Iteration 1519/1780 Training loss: 1.4232 13.3358 sec/batch\n",
      "Epoch 9/10  Iteration 1520/1780 Training loss: 1.4232 13.4586 sec/batch\n",
      "Epoch 9/10  Iteration 1521/1780 Training loss: 1.4231 13.5978 sec/batch\n",
      "Epoch 9/10  Iteration 1522/1780 Training loss: 1.4226 13.1454 sec/batch\n",
      "Epoch 9/10  Iteration 1523/1780 Training loss: 1.4220 13.1766 sec/batch\n",
      "Epoch 9/10  Iteration 1524/1780 Training loss: 1.4215 13.1268 sec/batch\n",
      "Epoch 9/10  Iteration 1525/1780 Training loss: 1.4214 13.1238 sec/batch\n",
      "Epoch 9/10  Iteration 1526/1780 Training loss: 1.4211 13.2596 sec/batch\n",
      "Epoch 9/10  Iteration 1527/1780 Training loss: 1.4208 13.1604 sec/batch\n",
      "Epoch 9/10  Iteration 1528/1780 Training loss: 1.4206 13.3942 sec/batch\n",
      "Epoch 9/10  Iteration 1529/1780 Training loss: 1.4203 13.2197 sec/batch\n",
      "Epoch 9/10  Iteration 1530/1780 Training loss: 1.4201 13.3988 sec/batch\n",
      "Epoch 9/10  Iteration 1531/1780 Training loss: 1.4199 14.2347 sec/batch\n",
      "Epoch 9/10  Iteration 1532/1780 Training loss: 1.4197 13.3111 sec/batch\n",
      "Epoch 9/10  Iteration 1533/1780 Training loss: 1.4194 13.3372 sec/batch\n",
      "Epoch 9/10  Iteration 1534/1780 Training loss: 1.4192 13.8504 sec/batch\n",
      "Epoch 9/10  Iteration 1535/1780 Training loss: 1.4190 13.7484 sec/batch\n",
      "Epoch 9/10  Iteration 1536/1780 Training loss: 1.4189 12.2153 sec/batch\n",
      "Epoch 9/10  Iteration 1537/1780 Training loss: 1.4187 11.0587 sec/batch\n",
      "Epoch 9/10  Iteration 1538/1780 Training loss: 1.4184 10.8344 sec/batch\n",
      "Epoch 9/10  Iteration 1539/1780 Training loss: 1.4180 10.8327 sec/batch\n",
      "Epoch 9/10  Iteration 1540/1780 Training loss: 1.4175 10.9683 sec/batch\n",
      "Epoch 9/10  Iteration 1541/1780 Training loss: 1.4172 11.0430 sec/batch\n",
      "Epoch 9/10  Iteration 1542/1780 Training loss: 1.4171 10.8778 sec/batch\n",
      "Epoch 9/10  Iteration 1543/1780 Training loss: 1.4168 10.9868 sec/batch\n",
      "Epoch 9/10  Iteration 1544/1780 Training loss: 1.4166 10.8955 sec/batch\n",
      "Epoch 9/10  Iteration 1545/1780 Training loss: 1.4164 11.1291 sec/batch\n",
      "Epoch 9/10  Iteration 1546/1780 Training loss: 1.4159 10.9718 sec/batch\n",
      "Epoch 9/10  Iteration 1547/1780 Training loss: 1.4155 11.0996 sec/batch\n",
      "Epoch 9/10  Iteration 1548/1780 Training loss: 1.4155 11.0780 sec/batch\n",
      "Epoch 9/10  Iteration 1549/1780 Training loss: 1.4153 10.9328 sec/batch\n",
      "Epoch 9/10  Iteration 1550/1780 Training loss: 1.4147 11.1572 sec/batch\n",
      "Epoch 9/10  Iteration 1551/1780 Training loss: 1.4147 10.9427 sec/batch\n",
      "Epoch 9/10  Iteration 1552/1780 Training loss: 1.4146 11.0758 sec/batch\n",
      "Epoch 9/10  Iteration 1553/1780 Training loss: 1.4142 10.8684 sec/batch\n",
      "Epoch 9/10  Iteration 1554/1780 Training loss: 1.4138 10.9215 sec/batch\n",
      "Epoch 9/10  Iteration 1555/1780 Training loss: 1.4133 10.9960 sec/batch\n",
      "Epoch 9/10  Iteration 1556/1780 Training loss: 1.4130 11.0706 sec/batch\n",
      "Epoch 9/10  Iteration 1557/1780 Training loss: 1.4129 10.9806 sec/batch\n",
      "Epoch 9/10  Iteration 1558/1780 Training loss: 1.4128 11.1499 sec/batch\n",
      "Epoch 9/10  Iteration 1559/1780 Training loss: 1.4128 11.0198 sec/batch\n",
      "Epoch 9/10  Iteration 1560/1780 Training loss: 1.4128 11.0443 sec/batch\n",
      "Epoch 9/10  Iteration 1561/1780 Training loss: 1.4129 10.9312 sec/batch\n",
      "Epoch 9/10  Iteration 1562/1780 Training loss: 1.4128 10.9792 sec/batch\n",
      "Epoch 9/10  Iteration 1563/1780 Training loss: 1.4128 11.0142 sec/batch\n",
      "Epoch 9/10  Iteration 1564/1780 Training loss: 1.4127 10.8960 sec/batch\n",
      "Epoch 9/10  Iteration 1565/1780 Training loss: 1.4129 11.0788 sec/batch\n",
      "Epoch 9/10  Iteration 1566/1780 Training loss: 1.4128 11.0626 sec/batch\n",
      "Epoch 9/10  Iteration 1567/1780 Training loss: 1.4126 10.8375 sec/batch\n",
      "Epoch 9/10  Iteration 1568/1780 Training loss: 1.4127 11.9164 sec/batch\n",
      "Epoch 9/10  Iteration 1569/1780 Training loss: 1.4124 10.9896 sec/batch\n",
      "Epoch 9/10  Iteration 1570/1780 Training loss: 1.4125 10.8329 sec/batch\n",
      "Epoch 9/10  Iteration 1571/1780 Training loss: 1.4125 10.8230 sec/batch\n",
      "Epoch 9/10  Iteration 1572/1780 Training loss: 1.4126 11.0658 sec/batch\n",
      "Epoch 9/10  Iteration 1573/1780 Training loss: 1.4126 11.0550 sec/batch\n",
      "Epoch 9/10  Iteration 1574/1780 Training loss: 1.4124 11.0983 sec/batch\n",
      "Epoch 9/10  Iteration 1575/1780 Training loss: 1.4120 10.8630 sec/batch\n",
      "Epoch 9/10  Iteration 1576/1780 Training loss: 1.4118 11.0355 sec/batch\n",
      "Epoch 9/10  Iteration 1577/1780 Training loss: 1.4117 10.9367 sec/batch\n",
      "Epoch 9/10  Iteration 1578/1780 Training loss: 1.4115 10.9431 sec/batch\n",
      "Epoch 9/10  Iteration 1579/1780 Training loss: 1.4114 10.8607 sec/batch\n",
      "Epoch 9/10  Iteration 1580/1780 Training loss: 1.4113 10.8918 sec/batch\n",
      "Epoch 9/10  Iteration 1581/1780 Training loss: 1.4112 10.8471 sec/batch\n",
      "Epoch 9/10  Iteration 1582/1780 Training loss: 1.4111 11.0279 sec/batch\n",
      "Epoch 9/10  Iteration 1583/1780 Training loss: 1.4107 10.8434 sec/batch\n",
      "Epoch 9/10  Iteration 1584/1780 Training loss: 1.4107 11.0269 sec/batch\n",
      "Epoch 9/10  Iteration 1585/1780 Training loss: 1.4108 10.8700 sec/batch\n",
      "Epoch 9/10  Iteration 1586/1780 Training loss: 1.4108 10.9572 sec/batch\n",
      "Epoch 9/10  Iteration 1587/1780 Training loss: 1.4107 10.9162 sec/batch\n",
      "Epoch 9/10  Iteration 1588/1780 Training loss: 1.4106 10.9882 sec/batch\n",
      "Epoch 9/10  Iteration 1589/1780 Training loss: 1.4105 10.9206 sec/batch\n",
      "Epoch 9/10  Iteration 1590/1780 Training loss: 1.4104 10.9103 sec/batch\n",
      "Epoch 9/10  Iteration 1591/1780 Training loss: 1.4105 11.3066 sec/batch\n",
      "Epoch 9/10  Iteration 1592/1780 Training loss: 1.4109 10.9347 sec/batch\n",
      "Epoch 9/10  Iteration 1593/1780 Training loss: 1.4108 10.9664 sec/batch\n",
      "Epoch 9/10  Iteration 1594/1780 Training loss: 1.4107 10.9939 sec/batch\n",
      "Epoch 9/10  Iteration 1595/1780 Training loss: 1.4106 10.9393 sec/batch\n",
      "Epoch 9/10  Iteration 1596/1780 Training loss: 1.4104 11.0093 sec/batch\n",
      "Epoch 9/10  Iteration 1597/1780 Training loss: 1.4104 11.0574 sec/batch\n",
      "Epoch 9/10  Iteration 1598/1780 Training loss: 1.4104 10.9097 sec/batch\n",
      "Epoch 9/10  Iteration 1599/1780 Training loss: 1.4104 10.9209 sec/batch\n",
      "Epoch 9/10  Iteration 1600/1780 Training loss: 1.4102 10.9957 sec/batch\n",
      "Validation loss: 1.28205 Saving checkpoint!\n",
      "Epoch 9/10  Iteration 1601/1780 Training loss: 1.4106 10.7866 sec/batch\n",
      "Epoch 9/10  Iteration 1602/1780 Training loss: 1.4108 10.8534 sec/batch\n",
      "Epoch 10/10  Iteration 1603/1780 Training loss: 1.4963 10.8269 sec/batch\n",
      "Epoch 10/10  Iteration 1604/1780 Training loss: 1.4529 10.9945 sec/batch\n",
      "Epoch 10/10  Iteration 1605/1780 Training loss: 1.4348 10.7974 sec/batch\n",
      "Epoch 10/10  Iteration 1606/1780 Training loss: 1.4260 10.8343 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10  Iteration 1607/1780 Training loss: 1.4154 10.9156 sec/batch\n",
      "Epoch 10/10  Iteration 1608/1780 Training loss: 1.4038 10.8536 sec/batch\n",
      "Epoch 10/10  Iteration 1609/1780 Training loss: 1.4040 10.8155 sec/batch\n",
      "Epoch 10/10  Iteration 1610/1780 Training loss: 1.4007 10.8245 sec/batch\n",
      "Epoch 10/10  Iteration 1611/1780 Training loss: 1.3998 10.8677 sec/batch\n",
      "Epoch 10/10  Iteration 1612/1780 Training loss: 1.3986 10.9325 sec/batch\n",
      "Epoch 10/10  Iteration 1613/1780 Training loss: 1.3943 10.9843 sec/batch\n",
      "Epoch 10/10  Iteration 1614/1780 Training loss: 1.3933 10.9180 sec/batch\n",
      "Epoch 10/10  Iteration 1615/1780 Training loss: 1.3927 10.8571 sec/batch\n",
      "Epoch 10/10  Iteration 1616/1780 Training loss: 1.3939 10.8295 sec/batch\n",
      "Epoch 10/10  Iteration 1617/1780 Training loss: 1.3922 10.9348 sec/batch\n",
      "Epoch 10/10  Iteration 1618/1780 Training loss: 1.3900 10.7568 sec/batch\n",
      "Epoch 10/10  Iteration 1619/1780 Training loss: 1.3903 10.8105 sec/batch\n",
      "Epoch 10/10  Iteration 1620/1780 Training loss: 1.3916 10.8778 sec/batch\n",
      "Epoch 10/10  Iteration 1621/1780 Training loss: 1.3914 11.0147 sec/batch\n",
      "Epoch 10/10  Iteration 1622/1780 Training loss: 1.3928 10.8350 sec/batch\n",
      "Epoch 10/10  Iteration 1623/1780 Training loss: 1.3920 10.9588 sec/batch\n",
      "Epoch 10/10  Iteration 1624/1780 Training loss: 1.3927 10.9306 sec/batch\n",
      "Epoch 10/10  Iteration 1625/1780 Training loss: 1.3914 10.7899 sec/batch\n",
      "Epoch 10/10  Iteration 1626/1780 Training loss: 1.3913 10.9526 sec/batch\n",
      "Epoch 10/10  Iteration 1627/1780 Training loss: 1.3911 10.8024 sec/batch\n",
      "Epoch 10/10  Iteration 1628/1780 Training loss: 1.3893 10.7767 sec/batch\n",
      "Epoch 10/10  Iteration 1629/1780 Training loss: 1.3877 10.8729 sec/batch\n",
      "Epoch 10/10  Iteration 1630/1780 Training loss: 1.3881 10.8758 sec/batch\n",
      "Epoch 10/10  Iteration 1631/1780 Training loss: 1.3880 10.9949 sec/batch\n",
      "Epoch 10/10  Iteration 1632/1780 Training loss: 1.3880 10.9184 sec/batch\n",
      "Epoch 10/10  Iteration 1633/1780 Training loss: 1.3876 10.7832 sec/batch\n",
      "Epoch 10/10  Iteration 1634/1780 Training loss: 1.3864 10.9249 sec/batch\n",
      "Epoch 10/10  Iteration 1635/1780 Training loss: 1.3865 10.8922 sec/batch\n",
      "Epoch 10/10  Iteration 1636/1780 Training loss: 1.3867 10.8721 sec/batch\n",
      "Epoch 10/10  Iteration 1637/1780 Training loss: 1.3864 10.7545 sec/batch\n",
      "Epoch 10/10  Iteration 1638/1780 Training loss: 1.3859 10.7393 sec/batch\n",
      "Epoch 10/10  Iteration 1639/1780 Training loss: 1.3854 11.0486 sec/batch\n",
      "Epoch 10/10  Iteration 1640/1780 Training loss: 1.3842 10.9851 sec/batch\n",
      "Epoch 10/10  Iteration 1641/1780 Training loss: 1.3827 10.8903 sec/batch\n",
      "Epoch 10/10  Iteration 1642/1780 Training loss: 1.3822 10.8757 sec/batch\n",
      "Epoch 10/10  Iteration 1643/1780 Training loss: 1.3816 10.9912 sec/batch\n",
      "Epoch 10/10  Iteration 1644/1780 Training loss: 1.3823 10.9580 sec/batch\n",
      "Epoch 10/10  Iteration 1645/1780 Training loss: 1.3819 10.8789 sec/batch\n",
      "Epoch 10/10  Iteration 1646/1780 Training loss: 1.3813 10.9836 sec/batch\n",
      "Epoch 10/10  Iteration 1647/1780 Training loss: 1.3815 10.8653 sec/batch\n",
      "Epoch 10/10  Iteration 1648/1780 Training loss: 1.3805 10.9912 sec/batch\n",
      "Epoch 10/10  Iteration 1649/1780 Training loss: 1.3804 10.9574 sec/batch\n",
      "Epoch 10/10  Iteration 1650/1780 Training loss: 1.3798 10.7916 sec/batch\n",
      "Epoch 10/10  Iteration 1651/1780 Training loss: 1.3797 10.9274 sec/batch\n",
      "Epoch 10/10  Iteration 1652/1780 Training loss: 1.3801 10.7974 sec/batch\n",
      "Epoch 10/10  Iteration 1653/1780 Training loss: 1.3795 11.0250 sec/batch\n",
      "Epoch 10/10  Iteration 1654/1780 Training loss: 1.3803 10.7987 sec/batch\n",
      "Epoch 10/10  Iteration 1655/1780 Training loss: 1.3803 10.7019 sec/batch\n",
      "Epoch 10/10  Iteration 1656/1780 Training loss: 1.3803 10.8616 sec/batch\n",
      "Epoch 10/10  Iteration 1657/1780 Training loss: 1.3801 10.8161 sec/batch\n",
      "Epoch 10/10  Iteration 1658/1780 Training loss: 1.3801 10.8708 sec/batch\n",
      "Epoch 10/10  Iteration 1659/1780 Training loss: 1.3805 10.8402 sec/batch\n",
      "Epoch 10/10  Iteration 1660/1780 Training loss: 1.3802 10.9163 sec/batch\n",
      "Epoch 10/10  Iteration 1661/1780 Training loss: 1.3797 10.8860 sec/batch\n",
      "Epoch 10/10  Iteration 1662/1780 Training loss: 1.3802 10.9811 sec/batch\n",
      "Epoch 10/10  Iteration 1663/1780 Training loss: 1.3802 10.9172 sec/batch\n",
      "Epoch 10/10  Iteration 1664/1780 Training loss: 1.3810 10.7449 sec/batch\n",
      "Epoch 10/10  Iteration 1665/1780 Training loss: 1.3813 10.7127 sec/batch\n",
      "Epoch 10/10  Iteration 1666/1780 Training loss: 1.3815 10.7956 sec/batch\n",
      "Epoch 10/10  Iteration 1667/1780 Training loss: 1.3813 10.8642 sec/batch\n",
      "Epoch 10/10  Iteration 1668/1780 Training loss: 1.3815 10.8874 sec/batch\n",
      "Epoch 10/10  Iteration 1669/1780 Training loss: 1.3816 10.7894 sec/batch\n",
      "Epoch 10/10  Iteration 1670/1780 Training loss: 1.3812 10.7914 sec/batch\n",
      "Epoch 10/10  Iteration 1671/1780 Training loss: 1.3813 10.8497 sec/batch\n",
      "Epoch 10/10  Iteration 1672/1780 Training loss: 1.3810 10.9333 sec/batch\n",
      "Epoch 10/10  Iteration 1673/1780 Training loss: 1.3815 10.9402 sec/batch\n",
      "Epoch 10/10  Iteration 1674/1780 Training loss: 1.3817 10.9648 sec/batch\n",
      "Epoch 10/10  Iteration 1675/1780 Training loss: 1.3820 10.9384 sec/batch\n",
      "Epoch 10/10  Iteration 1676/1780 Training loss: 1.3817 10.9559 sec/batch\n",
      "Epoch 10/10  Iteration 1677/1780 Training loss: 1.3816 10.9011 sec/batch\n",
      "Epoch 10/10  Iteration 1678/1780 Training loss: 1.3816 10.9196 sec/batch\n",
      "Epoch 10/10  Iteration 1679/1780 Training loss: 1.3814 10.7988 sec/batch\n",
      "Epoch 10/10  Iteration 1680/1780 Training loss: 1.3813 10.8194 sec/batch\n",
      "Epoch 10/10  Iteration 1681/1780 Training loss: 1.3806 10.7703 sec/batch\n",
      "Epoch 10/10  Iteration 1682/1780 Training loss: 1.3804 10.8396 sec/batch\n",
      "Epoch 10/10  Iteration 1683/1780 Training loss: 1.3798 10.7486 sec/batch\n",
      "Epoch 10/10  Iteration 1684/1780 Training loss: 1.3798 10.7862 sec/batch\n",
      "Epoch 10/10  Iteration 1685/1780 Training loss: 1.3792 10.8702 sec/batch\n",
      "Epoch 10/10  Iteration 1686/1780 Training loss: 1.3792 10.7472 sec/batch\n",
      "Epoch 10/10  Iteration 1687/1780 Training loss: 1.3789 10.8125 sec/batch\n",
      "Epoch 10/10  Iteration 1688/1780 Training loss: 1.3786 10.8676 sec/batch\n",
      "Epoch 10/10  Iteration 1689/1780 Training loss: 1.3782 10.8855 sec/batch\n",
      "Epoch 10/10  Iteration 1690/1780 Training loss: 1.3780 10.8022 sec/batch\n",
      "Epoch 10/10  Iteration 1691/1780 Training loss: 1.3775 10.9117 sec/batch\n",
      "Epoch 10/10  Iteration 1692/1780 Training loss: 1.3777 10.9383 sec/batch\n",
      "Epoch 10/10  Iteration 1693/1780 Training loss: 1.3774 10.7835 sec/batch\n",
      "Epoch 10/10  Iteration 1694/1780 Training loss: 1.3772 10.7663 sec/batch\n",
      "Epoch 10/10  Iteration 1695/1780 Training loss: 1.3766 11.0463 sec/batch\n",
      "Epoch 10/10  Iteration 1696/1780 Training loss: 1.3763 10.7449 sec/batch\n",
      "Epoch 10/10  Iteration 1697/1780 Training loss: 1.3759 10.8803 sec/batch\n",
      "Epoch 10/10  Iteration 1698/1780 Training loss: 1.3759 10.7378 sec/batch\n",
      "Epoch 10/10  Iteration 1699/1780 Training loss: 1.3758 10.8912 sec/batch\n",
      "Epoch 10/10  Iteration 1700/1780 Training loss: 1.3754 10.7509 sec/batch\n",
      "Validation loss: 1.26161 Saving checkpoint!\n",
      "Epoch 10/10  Iteration 1701/1780 Training loss: 1.3764 10.6871 sec/batch\n",
      "Epoch 10/10  Iteration 1702/1780 Training loss: 1.3761 10.8376 sec/batch\n",
      "Epoch 10/10  Iteration 1703/1780 Training loss: 1.3760 10.7993 sec/batch\n",
      "Epoch 10/10  Iteration 1704/1780 Training loss: 1.3759 10.7188 sec/batch\n",
      "Epoch 10/10  Iteration 1705/1780 Training loss: 1.3757 10.8153 sec/batch\n",
      "Epoch 10/10  Iteration 1706/1780 Training loss: 1.3755 10.7070 sec/batch\n",
      "Epoch 10/10  Iteration 1707/1780 Training loss: 1.3753 10.7686 sec/batch\n",
      "Epoch 10/10  Iteration 1708/1780 Training loss: 1.3752 10.6666 sec/batch\n",
      "Epoch 10/10  Iteration 1709/1780 Training loss: 1.3750 10.7608 sec/batch\n",
      "Epoch 10/10  Iteration 1710/1780 Training loss: 1.3750 10.7548 sec/batch\n",
      "Epoch 10/10  Iteration 1711/1780 Training loss: 1.3749 10.9059 sec/batch\n",
      "Epoch 10/10  Iteration 1712/1780 Training loss: 1.3748 10.7016 sec/batch\n",
      "Epoch 10/10  Iteration 1713/1780 Training loss: 1.3746 10.8059 sec/batch\n",
      "Epoch 10/10  Iteration 1714/1780 Training loss: 1.3745 10.8913 sec/batch\n",
      "Epoch 10/10  Iteration 1715/1780 Training loss: 1.3743 10.9045 sec/batch\n",
      "Epoch 10/10  Iteration 1716/1780 Training loss: 1.3741 10.9056 sec/batch\n",
      "Epoch 10/10  Iteration 1717/1780 Training loss: 1.3738 10.9053 sec/batch\n",
      "Epoch 10/10  Iteration 1718/1780 Training loss: 1.3734 10.6318 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10  Iteration 1719/1780 Training loss: 1.3733 10.7932 sec/batch\n",
      "Epoch 10/10  Iteration 1720/1780 Training loss: 1.3732 10.8362 sec/batch\n",
      "Epoch 10/10  Iteration 1721/1780 Training loss: 1.3729 10.9534 sec/batch\n",
      "Epoch 10/10  Iteration 1722/1780 Training loss: 1.3729 10.6819 sec/batch\n",
      "Epoch 10/10  Iteration 1723/1780 Training loss: 1.3727 10.6743 sec/batch\n",
      "Epoch 10/10  Iteration 1724/1780 Training loss: 1.3722 10.9389 sec/batch\n",
      "Epoch 10/10  Iteration 1725/1780 Training loss: 1.3718 11.0053 sec/batch\n",
      "Epoch 10/10  Iteration 1726/1780 Training loss: 1.3718 10.8767 sec/batch\n",
      "Epoch 10/10  Iteration 1727/1780 Training loss: 1.3716 10.7155 sec/batch\n",
      "Epoch 10/10  Iteration 1728/1780 Training loss: 1.3712 11.5818 sec/batch\n",
      "Epoch 10/10  Iteration 1729/1780 Training loss: 1.3712 10.8739 sec/batch\n",
      "Epoch 10/10  Iteration 1730/1780 Training loss: 1.3711 10.8728 sec/batch\n",
      "Epoch 10/10  Iteration 1731/1780 Training loss: 1.3709 10.7315 sec/batch\n",
      "Epoch 10/10  Iteration 1732/1780 Training loss: 1.3706 10.8736 sec/batch\n",
      "Epoch 10/10  Iteration 1733/1780 Training loss: 1.3701 10.7861 sec/batch\n",
      "Epoch 10/10  Iteration 1734/1780 Training loss: 1.3699 10.8531 sec/batch\n",
      "Epoch 10/10  Iteration 1735/1780 Training loss: 1.3699 10.8817 sec/batch\n",
      "Epoch 10/10  Iteration 1736/1780 Training loss: 1.3699 10.9067 sec/batch\n",
      "Epoch 10/10  Iteration 1737/1780 Training loss: 1.3698 10.9745 sec/batch\n",
      "Epoch 10/10  Iteration 1738/1780 Training loss: 1.3698 10.8733 sec/batch\n",
      "Epoch 10/10  Iteration 1739/1780 Training loss: 1.3699 10.9481 sec/batch\n",
      "Epoch 10/10  Iteration 1740/1780 Training loss: 1.3700 10.9558 sec/batch\n",
      "Epoch 10/10  Iteration 1741/1780 Training loss: 1.3699 10.9416 sec/batch\n",
      "Epoch 10/10  Iteration 1742/1780 Training loss: 1.3698 10.9858 sec/batch\n",
      "Epoch 10/10  Iteration 1743/1780 Training loss: 1.3701 10.9105 sec/batch\n",
      "Epoch 10/10  Iteration 1744/1780 Training loss: 1.3701 10.8811 sec/batch\n",
      "Epoch 10/10  Iteration 1745/1780 Training loss: 1.3699 10.8481 sec/batch\n",
      "Epoch 10/10  Iteration 1746/1780 Training loss: 1.3700 10.8070 sec/batch\n",
      "Epoch 10/10  Iteration 1747/1780 Training loss: 1.3699 10.7497 sec/batch\n",
      "Epoch 10/10  Iteration 1748/1780 Training loss: 1.3700 10.8592 sec/batch\n",
      "Epoch 10/10  Iteration 1749/1780 Training loss: 1.3700 10.7118 sec/batch\n",
      "Epoch 10/10  Iteration 1750/1780 Training loss: 1.3702 10.7447 sec/batch\n",
      "Epoch 10/10  Iteration 1751/1780 Training loss: 1.3702 10.7880 sec/batch\n",
      "Epoch 10/10  Iteration 1752/1780 Training loss: 1.3700 10.7830 sec/batch\n",
      "Epoch 10/10  Iteration 1753/1780 Training loss: 1.3696 10.9235 sec/batch\n",
      "Epoch 10/10  Iteration 1754/1780 Training loss: 1.3693 10.8585 sec/batch\n",
      "Epoch 10/10  Iteration 1755/1780 Training loss: 1.3693 11.0039 sec/batch\n",
      "Epoch 10/10  Iteration 1756/1780 Training loss: 1.3693 10.7765 sec/batch\n",
      "Epoch 10/10  Iteration 1757/1780 Training loss: 1.3692 10.7870 sec/batch\n",
      "Epoch 10/10  Iteration 1758/1780 Training loss: 1.3692 11.0234 sec/batch\n",
      "Epoch 10/10  Iteration 1759/1780 Training loss: 1.3692 10.7892 sec/batch\n",
      "Epoch 10/10  Iteration 1760/1780 Training loss: 1.3691 10.9723 sec/batch\n",
      "Epoch 10/10  Iteration 1761/1780 Training loss: 1.3688 10.9852 sec/batch\n",
      "Epoch 10/10  Iteration 1762/1780 Training loss: 1.3689 10.9775 sec/batch\n",
      "Epoch 10/10  Iteration 1763/1780 Training loss: 1.3690 10.9709 sec/batch\n",
      "Epoch 10/10  Iteration 1764/1780 Training loss: 1.3690 11.0195 sec/batch\n",
      "Epoch 10/10  Iteration 1765/1780 Training loss: 1.3689 11.0301 sec/batch\n",
      "Epoch 10/10  Iteration 1766/1780 Training loss: 1.3688 10.9283 sec/batch\n",
      "Epoch 10/10  Iteration 1767/1780 Training loss: 1.3687 10.9036 sec/batch\n",
      "Epoch 10/10  Iteration 1768/1780 Training loss: 1.3687 11.0605 sec/batch\n",
      "Epoch 10/10  Iteration 1769/1780 Training loss: 1.3689 10.8461 sec/batch\n",
      "Epoch 10/10  Iteration 1770/1780 Training loss: 1.3693 10.9471 sec/batch\n",
      "Epoch 10/10  Iteration 1771/1780 Training loss: 1.3692 10.7926 sec/batch\n",
      "Epoch 10/10  Iteration 1772/1780 Training loss: 1.3691 10.8629 sec/batch\n",
      "Epoch 10/10  Iteration 1773/1780 Training loss: 1.3690 10.7524 sec/batch\n",
      "Epoch 10/10  Iteration 1774/1780 Training loss: 1.3688 10.8416 sec/batch\n",
      "Epoch 10/10  Iteration 1775/1780 Training loss: 1.3689 10.6630 sec/batch\n",
      "Epoch 10/10  Iteration 1776/1780 Training loss: 1.3690 10.9061 sec/batch\n",
      "Epoch 10/10  Iteration 1777/1780 Training loss: 1.3690 10.6959 sec/batch\n",
      "Epoch 10/10  Iteration 1778/1780 Training loss: 1.3688 10.5840 sec/batch\n",
      "Epoch 10/10  Iteration 1779/1780 Training loss: 1.3687 10.7072 sec/batch\n",
      "Epoch 10/10  Iteration 1780/1780 Training loss: 1.3688 10.7073 sec/batch\n",
      "Validation loss: 1.25206 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "save_every_n = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                #saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i178_l512_2.580.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i178_l512_2.580.ckpt\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlathit that if had so\n",
      "like it that it were. He could not trouble to his wife, and there was\n",
      "anything in them of the side of his weaky in the creature at his forteren\n",
      "to him.\n",
      "\n",
      "\"What is it? I can't bread to those,\" said Stepan Arkadyevitch. \"It's not\n",
      "my children, and there is an almost this arm, true it mays already,\n",
      "and tell you what I have say to you, and was not looking at the peasant,\n",
      "why is, I don't know him out, and she doesn't speak to me immediately, as\n",
      "you would say the countess and the more frest an angelembre, and time and\n",
      "things's silent, but I was not in my stand that is in my head. But if he\n",
      "say, and was so feeling with his soul. A child--in his soul of his\n",
      "soul of his soul. He should not see that any of that sense of. Here he\n",
      "had not been so composed and to speak for as in a whole picture, but\n",
      "all the setting and her excellent and society, who had been delighted\n",
      "and see to anywing had been being troed to thousand words on them,\n",
      "we liked him.\n",
      "\n",
      "That set in her money at the table, he came into the party. The capable\n",
      "of his she could not be as an old composure.\n",
      "\n",
      "\"That's all something there will be down becime by throe is\n",
      "such a silent, as in a countess, I should state it out and divorct.\n",
      "The discussion is not for me. I was that something was simply they are\n",
      "all three manshess of a sensitions of mind it all.\"\n",
      "\n",
      "\"No,\" he thought, shouted and lifting his soul. \"While it might see your\n",
      "honser and she, I could burst. And I had been a midelity. And I had a\n",
      "marnief are through the countess,\" he said, looking at him, a chosing\n",
      "which they had been carried out and still solied, and there was a sen that\n",
      "was to be completely, and that this matter of all the seconds of it, and\n",
      "a concipation were to her husband, who came up and conscaously, that he\n",
      "was not the station. All his fourse she was always at the country,,\n",
      "to speak oft, and though they were to hear the delightful throom and\n",
      "whether they came towards the morning, and his living and a coller and\n",
      "hold--the children. \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farnt him oste wha sorind thans tout thint asd an sesand an hires on thime sind thit aled, ban thand and out hore as the ter hos ton ho te that, was tis tart al the hand sostint him sore an tit an son thes, win he se ther san ther hher tas tarereng,.\n",
      "\n",
      "Anl at an ades in ond hesiln, ad hhe torers teans, wast tar arering tho this sos alten sorer has hhas an siton ther him he had sin he ard ate te anling the sosin her ans and\n",
      "arins asd and ther ale te tot an tand tanginge wath and ho ald, so sot th asend sat hare sother horesinnd, he hesense wing ante her so tith tir sherinn, anded and to the toul anderin he sorit he torsith she se atere an ting ot hand and thit hhe so the te wile har\n",
      "ens ont in the sersise, and we he seres tar aterer, to ato tat or has he he wan ton here won and sen heren he sosering, to to theer oo adent har herere the wosh oute, was serild ward tous hed astend..\n",
      "\n",
      "I's sint on alt in har tor tit her asd hade shithans ored he talereng an soredendere tim tot hees. Tise sor and \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard as astice her said he celatice of to seress in the raice, and to be the some and sere allats to that said to that the sark and a cast a the wither ald the pacinesse of her had astition, he said to the sount as she west at hissele. Af the cond it he was a fact onthis astisarianing.\n",
      "\n",
      "\n",
      "\"Or a ton to to be that's a more at aspestale as the sont of anstiring as\n",
      "thours and trey.\n",
      "\n",
      "The same wo dangring the\n",
      "raterst, who sore and somethy had ast out an of his book. \"We had's beane were that, and a morted a thay he had to tere. Then to\n",
      "her homent andertersed his his ancouted to the pirsted, the soution for of the pirsice inthirgest and stenciol, with the hard and and\n",
      "a colrice of to be oneres,\n",
      "the song to this anderssad.\n",
      "The could ounterss the said to serom of\n",
      "soment a carsed of sheres of she\n",
      "torded\n",
      "har and want in their of hould, but\n",
      "her told in that in he tad a the same to her. Serghing an her has and with the seed, and the camt ont his about of the\n",
      "sail, the her then all houg ant or to hus to \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrat, his felt has at it.\n",
      "\n",
      "\"When the pose ther hor exceed\n",
      "to his sheant was,\" weat a sime of his sounsed. The coment and the facily that which had began terede a marilicaly whice whether the pose of his hand, at she was alligated herself the same on she had to\n",
      "taiking to his forthing and streath how to hand\n",
      "began in a lang at some at it, this he cholded not set all her. \"Wo love that is setthing. Him anstering as seen that.\"\n",
      "\n",
      "\"Yes in the man that say the mare a crances is it?\" said Sergazy Ivancatching. \"You doon think were somether is ifficult of a mone of\n",
      "though the most at the countes that the\n",
      "mean on the come to say the most, to\n",
      "his feesing of\n",
      "a man she, whilo he\n",
      "sained and well, that he would still at to said. He wind at his for the sore in the most\n",
      "of hoss and almoved to see him. They have betine the sumper into at he his stire, and what he was that at the so steate of the\n",
      "sound, and shin should have a geest of shall feet on the conderation to she had been at that imporsing the dre\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
