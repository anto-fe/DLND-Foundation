{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  7, 67, 44, 41, 60, 76, 71,  2, 75, 75, 75, 66, 67, 44, 44, 59,\n",
       "       71, 62, 67, 61, 49,  0, 49, 60, 64, 71, 67, 76, 60, 71, 67,  0,  0,\n",
       "       71, 67,  0, 49, 72, 60, 18, 71, 60, 53, 60, 76, 59, 71, 45, 77,  7,\n",
       "       67, 44, 44, 59, 71, 62, 67, 61, 49,  0, 59, 71, 49, 64, 71, 45, 77,\n",
       "        7, 67, 44, 44, 59, 71, 49, 77, 71, 49, 41, 64, 71, 30,  1, 77, 75,\n",
       "        1, 67, 59, 55, 75, 75, 73, 53, 60, 76, 59, 41,  7, 49, 77], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7, 67, 44, 41, 60, 76, 71,  2, 75],\n",
       "       [10, 77, 33, 71,  7, 60, 71, 61, 30, 53],\n",
       "       [71, 31, 67, 41, 31,  7, 49, 77, 56, 71],\n",
       "       [30, 41,  7, 60, 76, 71,  1, 30, 45,  0],\n",
       "       [71, 41,  7, 60, 71,  0, 67, 77, 33, 70],\n",
       "       [71, 47,  7, 76, 30, 45, 56,  7, 71,  0],\n",
       "       [41, 71, 41, 30, 75, 33, 30, 55, 75, 75],\n",
       "       [30, 71,  7, 60, 76, 64, 60,  0, 62, 34],\n",
       "       [ 7, 67, 41, 71, 49, 64, 71, 41,  7, 60],\n",
       "       [60, 76, 64, 60,  0, 62, 71, 67, 77, 33]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "\n",
    "\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "#     cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(lstm_size),\n",
    "                    output_keep_prob=keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    \n",
    "    seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                           name='softmax_w')\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab),\n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/178 Training loss: 4.4189 10.8157 sec/batch\n",
      "Epoch 1/1  Iteration 2/178 Training loss: 4.3715 9.8484 sec/batch\n",
      "Epoch 1/1  Iteration 3/178 Training loss: 4.1782 10.0583 sec/batch\n",
      "Epoch 1/1  Iteration 4/178 Training loss: 4.3518 9.2887 sec/batch\n",
      "Epoch 1/1  Iteration 5/178 Training loss: 4.2870 9.1797 sec/batch\n",
      "Epoch 1/1  Iteration 6/178 Training loss: 4.2227 9.7229 sec/batch\n",
      "Epoch 1/1  Iteration 7/178 Training loss: 4.1538 9.9416 sec/batch\n",
      "Epoch 1/1  Iteration 8/178 Training loss: 4.0843 13.0961 sec/batch\n",
      "Epoch 1/1  Iteration 9/178 Training loss: 4.0176 12.7225 sec/batch\n",
      "Epoch 1/1  Iteration 10/178 Training loss: 3.9602 12.4976 sec/batch\n",
      "Epoch 1/1  Iteration 11/178 Training loss: 3.9099 13.2518 sec/batch\n",
      "Epoch 1/1  Iteration 12/178 Training loss: 3.8697 11.9898 sec/batch\n",
      "Epoch 1/1  Iteration 13/178 Training loss: 3.8315 10.4181 sec/batch\n",
      "Epoch 1/1  Iteration 14/178 Training loss: 3.7990 10.8632 sec/batch\n",
      "Epoch 1/1  Iteration 15/178 Training loss: 3.7686 11.5410 sec/batch\n",
      "Epoch 1/1  Iteration 16/178 Training loss: 3.7405 12.9124 sec/batch\n",
      "Epoch 1/1  Iteration 17/178 Training loss: 3.7148 13.7143 sec/batch\n",
      "Epoch 1/1  Iteration 18/178 Training loss: 3.6926 11.5060 sec/batch\n",
      "Epoch 1/1  Iteration 19/178 Training loss: 3.6722 10.9697 sec/batch\n",
      "Epoch 1/1  Iteration 20/178 Training loss: 3.6518 10.8638 sec/batch\n",
      "Epoch 1/1  Iteration 21/178 Training loss: 3.6339 10.9808 sec/batch\n",
      "Epoch 1/1  Iteration 22/178 Training loss: 3.6173 11.1920 sec/batch\n",
      "Epoch 1/1  Iteration 23/178 Training loss: 3.6013 14.3772 sec/batch\n",
      "Epoch 1/1  Iteration 24/178 Training loss: 3.5870 13.2764 sec/batch\n",
      "Epoch 1/1  Iteration 25/178 Training loss: 3.5729 12.2039 sec/batch\n",
      "Epoch 1/1  Iteration 26/178 Training loss: 3.5604 15.4396 sec/batch\n",
      "Epoch 1/1  Iteration 27/178 Training loss: 3.5491 14.6019 sec/batch\n",
      "Epoch 1/1  Iteration 28/178 Training loss: 3.5373 15.5432 sec/batch\n",
      "Epoch 1/1  Iteration 29/178 Training loss: 3.5267 17.8368 sec/batch\n",
      "Epoch 1/1  Iteration 30/178 Training loss: 3.5168 16.2225 sec/batch\n",
      "Epoch 1/1  Iteration 31/178 Training loss: 3.5079 17.4785 sec/batch\n",
      "Epoch 1/1  Iteration 32/178 Training loss: 3.4988 14.0284 sec/batch\n",
      "Epoch 1/1  Iteration 33/178 Training loss: 3.4898 13.7941 sec/batch\n",
      "Epoch 1/1  Iteration 34/178 Training loss: 3.4819 13.8476 sec/batch\n",
      "Epoch 1/1  Iteration 35/178 Training loss: 3.4739 15.2622 sec/batch\n",
      "Epoch 1/1  Iteration 36/178 Training loss: 3.4665 13.6458 sec/batch\n",
      "Epoch 1/1  Iteration 37/178 Training loss: 3.4588 12.2060 sec/batch\n",
      "Epoch 1/1  Iteration 38/178 Training loss: 3.4516 13.0167 sec/batch\n",
      "Epoch 1/1  Iteration 39/178 Training loss: 3.4444 12.6226 sec/batch\n",
      "Epoch 1/1  Iteration 40/178 Training loss: 3.4379 12.1694 sec/batch\n",
      "Epoch 1/1  Iteration 41/178 Training loss: 3.4315 12.4012 sec/batch\n",
      "Epoch 1/1  Iteration 42/178 Training loss: 3.4254 11.8676 sec/batch\n",
      "Epoch 1/1  Iteration 43/178 Training loss: 3.4194 14.0928 sec/batch\n",
      "Epoch 1/1  Iteration 44/178 Training loss: 3.4137 11.6002 sec/batch\n",
      "Epoch 1/1  Iteration 45/178 Training loss: 3.4080 13.4099 sec/batch\n",
      "Epoch 1/1  Iteration 46/178 Training loss: 3.4030 13.8464 sec/batch\n",
      "Epoch 1/1  Iteration 47/178 Training loss: 3.3981 11.8330 sec/batch\n",
      "Epoch 1/1  Iteration 48/178 Training loss: 3.3936 12.6044 sec/batch\n",
      "Epoch 1/1  Iteration 49/178 Training loss: 3.3891 13.6297 sec/batch\n",
      "Epoch 1/1  Iteration 50/178 Training loss: 3.3848 13.0094 sec/batch\n",
      "Epoch 1/1  Iteration 51/178 Training loss: 3.3805 12.4467 sec/batch\n",
      "Epoch 1/1  Iteration 52/178 Training loss: 3.3762 15.0828 sec/batch\n",
      "Epoch 1/1  Iteration 53/178 Training loss: 3.3722 13.6563 sec/batch\n",
      "Epoch 1/1  Iteration 54/178 Training loss: 3.3681 11.7784 sec/batch\n",
      "Epoch 1/1  Iteration 55/178 Training loss: 3.3644 12.6449 sec/batch\n",
      "Epoch 1/1  Iteration 56/178 Training loss: 3.3605 12.3642 sec/batch\n",
      "Epoch 1/1  Iteration 57/178 Training loss: 3.3568 12.1160 sec/batch\n",
      "Epoch 1/1  Iteration 58/178 Training loss: 3.3533 15.1168 sec/batch\n",
      "Epoch 1/1  Iteration 59/178 Training loss: 3.3496 13.8217 sec/batch\n",
      "Epoch 1/1  Iteration 60/178 Training loss: 3.3464 12.6885 sec/batch\n",
      "Epoch 1/1  Iteration 61/178 Training loss: 3.3432 12.7768 sec/batch\n",
      "Epoch 1/1  Iteration 62/178 Training loss: 3.3404 14.5760 sec/batch\n",
      "Epoch 1/1  Iteration 63/178 Training loss: 3.3377 14.8867 sec/batch\n",
      "Epoch 1/1  Iteration 64/178 Training loss: 3.3344 16.0324 sec/batch\n",
      "Epoch 1/1  Iteration 65/178 Training loss: 3.3314 13.4097 sec/batch\n",
      "Epoch 1/1  Iteration 66/178 Training loss: 3.3289 13.0667 sec/batch\n",
      "Epoch 1/1  Iteration 67/178 Training loss: 3.3264 14.5732 sec/batch\n",
      "Epoch 1/1  Iteration 68/178 Training loss: 3.3232 12.5169 sec/batch\n",
      "Epoch 1/1  Iteration 69/178 Training loss: 3.3204 11.9932 sec/batch\n",
      "Epoch 1/1  Iteration 70/178 Training loss: 3.3182 14.0224 sec/batch\n",
      "Epoch 1/1  Iteration 71/178 Training loss: 3.3156 12.8789 sec/batch\n",
      "Epoch 1/1  Iteration 72/178 Training loss: 3.3135 11.8711 sec/batch\n",
      "Epoch 1/1  Iteration 73/178 Training loss: 3.3111 12.7427 sec/batch\n",
      "Epoch 1/1  Iteration 74/178 Training loss: 3.3088 11.6792 sec/batch\n",
      "Epoch 1/1  Iteration 75/178 Training loss: 3.3067 11.2320 sec/batch\n",
      "Epoch 1/1  Iteration 76/178 Training loss: 3.3046 12.7443 sec/batch\n",
      "Epoch 1/1  Iteration 77/178 Training loss: 3.3024 11.6183 sec/batch\n",
      "Epoch 1/1  Iteration 78/178 Training loss: 3.3003 12.9337 sec/batch\n",
      "Epoch 1/1  Iteration 79/178 Training loss: 3.2982 16.1279 sec/batch\n",
      "Epoch 1/1  Iteration 80/178 Training loss: 3.2959 13.8326 sec/batch\n",
      "Epoch 1/1  Iteration 81/178 Training loss: 3.2938 13.8071 sec/batch\n",
      "Epoch 1/1  Iteration 82/178 Training loss: 3.2919 12.6815 sec/batch\n",
      "Epoch 1/1  Iteration 83/178 Training loss: 3.2901 13.0348 sec/batch\n",
      "Epoch 1/1  Iteration 84/178 Training loss: 3.2882 13.0235 sec/batch\n",
      "Epoch 1/1  Iteration 85/178 Training loss: 3.2861 13.1696 sec/batch\n",
      "Epoch 1/1  Iteration 86/178 Training loss: 3.2841 14.1319 sec/batch\n",
      "Epoch 1/1  Iteration 87/178 Training loss: 3.2821 16.5979 sec/batch\n",
      "Epoch 1/1  Iteration 88/178 Training loss: 3.2801 12.2956 sec/batch\n",
      "Epoch 1/1  Iteration 89/178 Training loss: 3.2784 12.9844 sec/batch\n",
      "Epoch 1/1  Iteration 90/178 Training loss: 3.2766 15.2680 sec/batch\n",
      "Epoch 1/1  Iteration 91/178 Training loss: 3.2749 13.8561 sec/batch\n",
      "Epoch 1/1  Iteration 92/178 Training loss: 3.2731 14.4254 sec/batch\n",
      "Epoch 1/1  Iteration 93/178 Training loss: 3.2714 14.1132 sec/batch\n",
      "Epoch 1/1  Iteration 94/178 Training loss: 3.2697 12.7675 sec/batch\n",
      "Epoch 1/1  Iteration 95/178 Training loss: 3.2679 15.4036 sec/batch\n",
      "Epoch 1/1  Iteration 96/178 Training loss: 3.2662 16.2784 sec/batch\n",
      "Epoch 1/1  Iteration 97/178 Training loss: 3.2646 15.8474 sec/batch\n",
      "Epoch 1/1  Iteration 98/178 Training loss: 3.2630 13.1804 sec/batch\n",
      "Epoch 1/1  Iteration 99/178 Training loss: 3.2613 13.0785 sec/batch\n",
      "Epoch 1/1  Iteration 100/178 Training loss: 3.2595 13.1368 sec/batch\n",
      "Epoch 1/1  Iteration 101/178 Training loss: 3.2579 13.2934 sec/batch\n",
      "Epoch 1/1  Iteration 102/178 Training loss: 3.2562 13.2906 sec/batch\n",
      "Epoch 1/1  Iteration 103/178 Training loss: 3.2546 13.4627 sec/batch\n",
      "Epoch 1/1  Iteration 104/178 Training loss: 3.2529 13.1432 sec/batch\n",
      "Epoch 1/1  Iteration 105/178 Training loss: 3.2521 13.2553 sec/batch\n",
      "Epoch 1/1  Iteration 106/178 Training loss: 3.2597 13.4423 sec/batch\n",
      "Epoch 1/1  Iteration 107/178 Training loss: 3.2662 16.3830 sec/batch\n",
      "Epoch 1/1  Iteration 108/178 Training loss: 3.2710 14.9286 sec/batch\n",
      "Epoch 1/1  Iteration 109/178 Training loss: 3.2744 12.4205 sec/batch\n",
      "Epoch 1/1  Iteration 110/178 Training loss: 3.2760 13.6092 sec/batch\n",
      "Epoch 1/1  Iteration 111/178 Training loss: 3.2762 14.8491 sec/batch\n",
      "Epoch 1/1  Iteration 112/178 Training loss: 3.2747 12.1398 sec/batch\n",
      "Epoch 1/1  Iteration 113/178 Training loss: 3.2730 12.0883 sec/batch\n",
      "Epoch 1/1  Iteration 114/178 Training loss: 3.2714 12.2680 sec/batch\n",
      "Epoch 1/1  Iteration 115/178 Training loss: 3.2697 13.3633 sec/batch\n",
      "Epoch 1/1  Iteration 116/178 Training loss: 3.2682 15.1310 sec/batch\n",
      "Epoch 1/1  Iteration 117/178 Training loss: 3.2667 13.6705 sec/batch\n",
      "Epoch 1/1  Iteration 118/178 Training loss: 3.2653 15.3767 sec/batch\n",
      "Epoch 1/1  Iteration 119/178 Training loss: 3.2638 12.4758 sec/batch\n",
      "Epoch 1/1  Iteration 120/178 Training loss: 3.2622 13.6614 sec/batch\n",
      "Epoch 1/1  Iteration 121/178 Training loss: 3.2607 12.7465 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 122/178 Training loss: 3.2592 13.6229 sec/batch\n",
      "Epoch 1/1  Iteration 123/178 Training loss: 3.2577 14.8792 sec/batch\n",
      "Epoch 1/1  Iteration 124/178 Training loss: 3.2562 13.2870 sec/batch\n",
      "Epoch 1/1  Iteration 125/178 Training loss: 3.2546 15.4006 sec/batch\n",
      "Epoch 1/1  Iteration 126/178 Training loss: 3.2528 15.3401 sec/batch\n",
      "Epoch 1/1  Iteration 127/178 Training loss: 3.2512 12.6792 sec/batch\n",
      "Epoch 1/1  Iteration 128/178 Training loss: 3.2495 12.8088 sec/batch\n",
      "Epoch 1/1  Iteration 129/178 Training loss: 3.2478 14.0751 sec/batch\n",
      "Epoch 1/1  Iteration 130/178 Training loss: 3.2461 16.2950 sec/batch\n",
      "Epoch 1/1  Iteration 131/178 Training loss: 3.2446 12.4314 sec/batch\n",
      "Epoch 1/1  Iteration 132/178 Training loss: 3.2429 14.5914 sec/batch\n",
      "Epoch 1/1  Iteration 133/178 Training loss: 3.2412 14.4486 sec/batch\n",
      "Epoch 1/1  Iteration 134/178 Training loss: 3.2396 15.8534 sec/batch\n",
      "Epoch 1/1  Iteration 135/178 Training loss: 3.2376 15.5156 sec/batch\n",
      "Epoch 1/1  Iteration 136/178 Training loss: 3.2357 15.2370 sec/batch\n",
      "Epoch 1/1  Iteration 137/178 Training loss: 3.2339 13.2874 sec/batch\n",
      "Epoch 1/1  Iteration 138/178 Training loss: 3.2321 14.2226 sec/batch\n",
      "Epoch 1/1  Iteration 139/178 Training loss: 3.2304 14.6620 sec/batch\n",
      "Epoch 1/1  Iteration 140/178 Training loss: 3.2285 14.2076 sec/batch\n",
      "Epoch 1/1  Iteration 141/178 Training loss: 3.2267 11.9276 sec/batch\n",
      "Epoch 1/1  Iteration 142/178 Training loss: 3.2246 11.5682 sec/batch\n",
      "Epoch 1/1  Iteration 143/178 Training loss: 3.2226 12.6875 sec/batch\n",
      "Epoch 1/1  Iteration 144/178 Training loss: 3.2205 14.0574 sec/batch\n",
      "Epoch 1/1  Iteration 145/178 Training loss: 3.2186 11.9395 sec/batch\n",
      "Epoch 1/1  Iteration 146/178 Training loss: 3.2165 11.8206 sec/batch\n",
      "Epoch 1/1  Iteration 147/178 Training loss: 3.2145 12.2106 sec/batch\n",
      "Epoch 1/1  Iteration 148/178 Training loss: 3.2126 12.1545 sec/batch\n",
      "Epoch 1/1  Iteration 149/178 Training loss: 3.2103 13.1189 sec/batch\n",
      "Epoch 1/1  Iteration 150/178 Training loss: 3.2082 16.2824 sec/batch\n",
      "Epoch 1/1  Iteration 151/178 Training loss: 3.2062 15.4183 sec/batch\n",
      "Epoch 1/1  Iteration 152/178 Training loss: 3.2043 15.6726 sec/batch\n",
      "Epoch 1/1  Iteration 153/178 Training loss: 3.2022 15.7296 sec/batch\n",
      "Epoch 1/1  Iteration 154/178 Training loss: 3.2000 17.5272 sec/batch\n",
      "Epoch 1/1  Iteration 155/178 Training loss: 3.1976 13.6917 sec/batch\n",
      "Epoch 1/1  Iteration 156/178 Training loss: 3.1952 15.4155 sec/batch\n",
      "Epoch 1/1  Iteration 157/178 Training loss: 3.1928 15.7453 sec/batch\n",
      "Epoch 1/1  Iteration 158/178 Training loss: 3.1904 16.8024 sec/batch\n",
      "Epoch 1/1  Iteration 159/178 Training loss: 3.1879 17.6345 sec/batch\n",
      "Epoch 1/1  Iteration 160/178 Training loss: 3.1859 17.1110 sec/batch\n",
      "Epoch 1/1  Iteration 161/178 Training loss: 3.1836 20.0367 sec/batch\n",
      "Epoch 1/1  Iteration 162/178 Training loss: 3.1812 19.2045 sec/batch\n",
      "Epoch 1/1  Iteration 163/178 Training loss: 3.1787 17.3487 sec/batch\n",
      "Epoch 1/1  Iteration 164/178 Training loss: 3.1763 19.4329 sec/batch\n",
      "Epoch 1/1  Iteration 165/178 Training loss: 3.1739 18.3399 sec/batch\n",
      "Epoch 1/1  Iteration 166/178 Training loss: 3.1714 18.2452 sec/batch\n",
      "Epoch 1/1  Iteration 167/178 Training loss: 3.1689 22.1600 sec/batch\n",
      "Epoch 1/1  Iteration 168/178 Training loss: 3.1664 17.5131 sec/batch\n",
      "Epoch 1/1  Iteration 169/178 Training loss: 3.1639 16.4543 sec/batch\n",
      "Epoch 1/1  Iteration 170/178 Training loss: 3.1613 17.6538 sec/batch\n",
      "Epoch 1/1  Iteration 171/178 Training loss: 3.1588 14.8455 sec/batch\n",
      "Epoch 1/1  Iteration 172/178 Training loss: 3.1564 15.4337 sec/batch\n",
      "Epoch 1/1  Iteration 173/178 Training loss: 3.1540 16.2645 sec/batch\n",
      "Epoch 1/1  Iteration 174/178 Training loss: 3.1516 15.5772 sec/batch\n",
      "Epoch 1/1  Iteration 175/178 Training loss: 3.1492 14.9909 sec/batch\n",
      "Epoch 1/1  Iteration 176/178 Training loss: 3.1466 15.9098 sec/batch\n",
      "Epoch 1/1  Iteration 177/178 Training loss: 3.1439 15.1893 sec/batch\n",
      "Epoch 1/1  Iteration 178/178 Training loss: 3.1412 14.2425 sec/batch\n",
      "Validation loss: 2.58 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i178_l512_2.580.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i178_l512_2.580.ckpt\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farlathit that if had so\n",
      "like it that it were. He could not trouble to his wife, and there was\n",
      "anything in them of the side of his weaky in the creature at his forteren\n",
      "to him.\n",
      "\n",
      "\"What is it? I can't bread to those,\" said Stepan Arkadyevitch. \"It's not\n",
      "my children, and there is an almost this arm, true it mays already,\n",
      "and tell you what I have say to you, and was not looking at the peasant,\n",
      "why is, I don't know him out, and she doesn't speak to me immediately, as\n",
      "you would say the countess and the more frest an angelembre, and time and\n",
      "things's silent, but I was not in my stand that is in my head. But if he\n",
      "say, and was so feeling with his soul. A child--in his soul of his\n",
      "soul of his soul. He should not see that any of that sense of. Here he\n",
      "had not been so composed and to speak for as in a whole picture, but\n",
      "all the setting and her excellent and society, who had been delighted\n",
      "and see to anywing had been being troed to thousand words on them,\n",
      "we liked him.\n",
      "\n",
      "That set in her money at the table, he came into the party. The capable\n",
      "of his she could not be as an old composure.\n",
      "\n",
      "\"That's all something there will be down becime by throe is\n",
      "such a silent, as in a countess, I should state it out and divorct.\n",
      "The discussion is not for me. I was that something was simply they are\n",
      "all three manshess of a sensitions of mind it all.\"\n",
      "\n",
      "\"No,\" he thought, shouted and lifting his soul. \"While it might see your\n",
      "honser and she, I could burst. And I had been a midelity. And I had a\n",
      "marnief are through the countess,\" he said, looking at him, a chosing\n",
      "which they had been carried out and still solied, and there was a sen that\n",
      "was to be completely, and that this matter of all the seconds of it, and\n",
      "a concipation were to her husband, who came up and conscaously, that he\n",
      "was not the station. All his fourse she was always at the country,,\n",
      "to speak oft, and though they were to hear the delightful throom and\n",
      "whether they came towards the morning, and his living and a coller and\n",
      "hold--the children. \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard as astice her said he celatice of to seress in the raice, and to be the some and sere allats to that said to that the sark and a cast a the wither ald the pacinesse of her had astition, he said to the sount as she west at hissele. Af the cond it he was a fact onthis astisarianing.\n",
      "\n",
      "\n",
      "\"Or a ton to to be that's a more at aspestale as the sont of anstiring as\n",
      "thours and trey.\n",
      "\n",
      "The same wo dangring the\n",
      "raterst, who sore and somethy had ast out an of his book. \"We had's beane were that, and a morted a thay he had to tere. Then to\n",
      "her homent andertersed his his ancouted to the pirsted, the soution for of the pirsice inthirgest and stenciol, with the hard and and\n",
      "a colrice of to be oneres,\n",
      "the song to this anderssad.\n",
      "The could ounterss the said to serom of\n",
      "soment a carsed of sheres of she\n",
      "torded\n",
      "har and want in their of hould, but\n",
      "her told in that in he tad a the same to her. Serghing an her has and with the seed, and the camt ont his about of the\n",
      "sail, the her then all houg ant or to hus to \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrat, his felt has at it.\n",
      "\n",
      "\"When the pose ther hor exceed\n",
      "to his sheant was,\" weat a sime of his sounsed. The coment and the facily that which had began terede a marilicaly whice whether the pose of his hand, at she was alligated herself the same on she had to\n",
      "taiking to his forthing and streath how to hand\n",
      "began in a lang at some at it, this he cholded not set all her. \"Wo love that is setthing. Him anstering as seen that.\"\n",
      "\n",
      "\"Yes in the man that say the mare a crances is it?\" said Sergazy Ivancatching. \"You doon think were somether is ifficult of a mone of\n",
      "though the most at the countes that the\n",
      "mean on the come to say the most, to\n",
      "his feesing of\n",
      "a man she, whilo he\n",
      "sained and well, that he would still at to said. He wind at his for the sore in the most\n",
      "of hoss and almoved to see him. They have betine the sumper into at he his stire, and what he was that at the so steate of the\n",
      "sound, and shin should have a geest of shall feet on the conderation to she had been at that imporsing the dre\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
